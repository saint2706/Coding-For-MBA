{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fa1e39",
   "metadata": {},
   "source": [
    "# Day 59 – Generative Models\n",
    "\n",
    "Generative models synthesise data, compress signals, and enable controllable creativity. In this lesson you will:\n",
    "\n",
    "- Contrast autoencoders, variational autoencoders, GANs, and diffusion models across objectives and sampling procedures.\n",
    "- Optimise lightweight autoencoders and VAEs on synthetic data to observe reconstruction loss curves.\n",
    "- Understand GAN training dynamics with simplified generator–discriminator updates and stability heuristics.\n",
    "- Explore diffusion process fundamentals: forward noising, denoising score matching, and scheduler design.\n",
    "\n",
    "Execute `python Day_59_Generative_Models/solutions.py` to run miniature training loops that log decreasing reconstruction losses and summarise practical tuning tips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b25b0d",
   "metadata": {},
   "source": [
    "Synthetic generative modelling routines for Day 59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingLog:\n",
    "    \"\"\"Container for training statistics collected per iteration.\"\"\"\n",
    "\n",
    "    losses: List[float]\n",
    "    reconstructions: np.ndarray\n",
    "\n",
    "\n",
    "def generate_swiss_roll(n_samples: int = 128, random_state: int = 59) -> np.ndarray:\n",
    "    \"\"\"Return a 2D swiss-roll style dataset for reconstruction demos.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    theta = rng.uniform(0, 3 * np.pi, size=n_samples)\n",
    "    height = rng.uniform(-1.0, 1.0, size=n_samples)\n",
    "    x = theta * np.cos(theta)\n",
    "    y = theta * np.sin(theta)\n",
    "    data = np.column_stack((x, y))\n",
    "    data -= data.mean(axis=0, keepdims=True)\n",
    "    data /= np.abs(data).max()\n",
    "    data += 0.05 * height[:, None]\n",
    "    return data.astype(np.float64)\n",
    "\n",
    "\n",
    "def _tanh(x: np.ndarray) -> np.ndarray:\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def _tanh_grad(x: np.ndarray) -> np.ndarray:\n",
    "    t = np.tanh(x)\n",
    "    return 1.0 - t**2\n",
    "\n",
    "\n",
    "def train_autoencoder_synthetic(\n",
    "    data: np.ndarray | None = None,\n",
    "    hidden_dim: int = 3,\n",
    "    epochs: int = 200,\n",
    "    lr: float = 0.05,\n",
    "    random_state: int = 59,\n",
    ") -> TrainingLog:\n",
    "    \"\"\"Train a deterministic autoencoder on synthetic data.\"\"\"\n",
    "\n",
    "    X = data if data is not None else generate_swiss_roll(random_state=random_state)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n_features = X.shape[1]\n",
    "    W1 = rng.normal(0.0, 0.2, size=(n_features, hidden_dim))\n",
    "    b1 = np.zeros(hidden_dim)\n",
    "    W2 = rng.normal(0.0, 0.2, size=(hidden_dim, n_features))\n",
    "    b2 = np.zeros(n_features)\n",
    "\n",
    "    losses: List[float] = []\n",
    "    for _ in range(epochs):\n",
    "        z_lin = X @ W1 + b1\n",
    "        z = _tanh(z_lin)\n",
    "        recon = z @ W2 + b2\n",
    "        diff = recon - X\n",
    "        loss = float(np.mean(diff**2))\n",
    "        losses.append(loss)\n",
    "\n",
    "        grad_recon = (2.0 / X.shape[0]) * diff\n",
    "        grad_W2 = z.T @ grad_recon\n",
    "        grad_b2 = grad_recon.sum(axis=0)\n",
    "        grad_hidden = (grad_recon @ W2.T) * _tanh_grad(z_lin)\n",
    "        grad_W1 = X.T @ grad_hidden\n",
    "        grad_b1 = grad_hidden.sum(axis=0)\n",
    "\n",
    "        W2 -= lr * grad_W2\n",
    "        b2 -= lr * grad_b2\n",
    "        W1 -= lr * grad_W1\n",
    "        b1 -= lr * grad_b1\n",
    "\n",
    "    final_recon = _tanh(X @ W1 + b1) @ W2 + b2\n",
    "    return TrainingLog(losses=losses, reconstructions=final_recon)\n",
    "\n",
    "\n",
    "def train_variational_autoencoder_synthetic(\n",
    "    data: np.ndarray | None = None,\n",
    "    latent_dim: int = 2,\n",
    "    epochs: int = 200,\n",
    "    lr: float = 0.05,\n",
    "    kl_weight: float = 0.01,\n",
    "    random_state: int = 59,\n",
    ") -> TrainingLog:\n",
    "    \"\"\"Run a minimal VAE with reparameterisation on synthetic data.\"\"\"\n",
    "\n",
    "    X = data if data is not None else generate_swiss_roll(random_state=random_state + 1)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n_features = X.shape[1]\n",
    "    W_mu = rng.normal(0.0, 0.2, size=(n_features, latent_dim))\n",
    "    b_mu = np.zeros(latent_dim)\n",
    "    W_logvar = rng.normal(0.0, 0.2, size=(n_features, latent_dim))\n",
    "    b_logvar = np.zeros(latent_dim)\n",
    "    W_dec = rng.normal(0.0, 0.2, size=(latent_dim, n_features))\n",
    "    b_dec = np.zeros(n_features)\n",
    "\n",
    "    losses: List[float] = []\n",
    "    for _ in range(epochs):\n",
    "        mu = X @ W_mu + b_mu\n",
    "        logvar = X @ W_logvar + b_logvar\n",
    "        std = np.exp(0.5 * logvar)\n",
    "        eps = rng.normal(0.0, 1.0, size=mu.shape)\n",
    "        z = mu + eps * std\n",
    "        recon = _tanh(z) @ W_dec + b_dec\n",
    "        diff = recon - X\n",
    "        recon_loss = np.mean(diff**2)\n",
    "        kl_div = -0.5 * np.mean(1 + logvar - mu**2 - np.exp(logvar))\n",
    "        loss = float(recon_loss + kl_weight * kl_div)\n",
    "        losses.append(loss)\n",
    "\n",
    "        grad_recon = (2.0 / X.shape[0]) * diff\n",
    "        grad_W_dec = (_tanh(z)).T @ grad_recon\n",
    "        grad_b_dec = grad_recon.sum(axis=0)\n",
    "        grad_hidden = (grad_recon @ W_dec.T) * (1 - np.tanh(z) ** 2)\n",
    "\n",
    "        grad_mu = grad_hidden + kl_weight * (mu / X.shape[0])\n",
    "        grad_logvar = (\n",
    "            grad_hidden * eps * std * 0.5\n",
    "            + kl_weight * 0.5 * (np.exp(logvar) - 1) / X.shape[0]\n",
    "        )\n",
    "\n",
    "        grad_W_mu = X.T @ grad_mu\n",
    "        grad_b_mu = grad_mu.sum(axis=0)\n",
    "        grad_W_logvar = X.T @ grad_logvar\n",
    "        grad_b_logvar = grad_logvar.sum(axis=0)\n",
    "\n",
    "        W_dec -= lr * grad_W_dec\n",
    "        b_dec -= lr * grad_b_dec\n",
    "        W_mu -= lr * grad_W_mu\n",
    "        b_mu -= lr * grad_b_mu\n",
    "        W_logvar -= lr * grad_W_logvar\n",
    "        b_logvar -= lr * grad_b_logvar\n",
    "\n",
    "    final_z = X @ W_mu + b_mu\n",
    "    final_recon = _tanh(final_z) @ W_dec + b_dec\n",
    "    return TrainingLog(losses=losses, reconstructions=final_recon)\n",
    "\n",
    "\n",
    "def train_diffusion_denoiser(\n",
    "    data: np.ndarray | None = None,\n",
    "    timesteps: int = 10,\n",
    "    epochs: int = 200,\n",
    "    lr: float = 0.05,\n",
    "    random_state: int = 59,\n",
    ") -> TrainingLog:\n",
    "    \"\"\"Train a denoiser to recover clean data from a simple diffusion step.\"\"\"\n",
    "\n",
    "    X = data if data is not None else generate_swiss_roll(random_state=random_state + 2)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n_features = X.shape[1]\n",
    "    W = rng.normal(0.0, 0.2, size=(n_features, n_features))\n",
    "    b = np.zeros(n_features)\n",
    "    losses: List[float] = []\n",
    "\n",
    "    betas = np.linspace(1e-3, 5e-2, timesteps)\n",
    "    alphas = 1.0 - betas\n",
    "    alpha_bar = np.cumprod(alphas)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        t = rng.integers(0, timesteps)\n",
    "        noise = rng.normal(0.0, 1.0, size=X.shape)\n",
    "        noisy = np.sqrt(alpha_bar[t]) * X + np.sqrt(1 - alpha_bar[t]) * noise\n",
    "        pred_noise = noisy @ W + b\n",
    "        diff = pred_noise - noise\n",
    "        loss = float(np.mean(diff**2))\n",
    "        losses.append(loss)\n",
    "\n",
    "        grad = (2.0 / X.shape[0]) * diff\n",
    "        grad_W = noisy.T @ grad\n",
    "        grad_b = grad.sum(axis=0)\n",
    "        W -= lr * grad_W\n",
    "        b -= lr * grad_b\n",
    "\n",
    "    final_noise = X @ W + b\n",
    "    return TrainingLog(losses=losses, reconstructions=final_noise)\n",
    "\n",
    "\n",
    "def gan_training_summary(\n",
    "    steps: int = 100, random_state: int = 59\n",
    ") -> List[Dict[str, float]]:\n",
    "    \"\"\"Simulate GAN training metrics to illustrate convergence heuristics.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    real_mean = 1.5\n",
    "    gen_mean = rng.normal(-1.0, 0.1)\n",
    "    log: List[Dict[str, float]] = []\n",
    "    for step in range(steps):\n",
    "        gen_mean += 0.03 * (real_mean - gen_mean)\n",
    "        discriminator_loss = float(np.exp(-abs(real_mean - gen_mean)))\n",
    "        generator_loss = float(abs(real_mean - gen_mean))\n",
    "        log.append(\n",
    "            {\n",
    "                \"step\": step,\n",
    "                \"gen_loss\": generator_loss,\n",
    "                \"disc_loss\": discriminator_loss,\n",
    "                \"gen_mean\": gen_mean,\n",
    "            }\n",
    "        )\n",
    "    return log\n",
    "\n",
    "\n",
    "def summarise_generative_objectives() -> Dict[str, str]:\n",
    "    \"\"\"Return cheat-sheet style descriptions of key generative objectives.\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"autoencoder\": \"Minimise reconstruction error with deterministic encoder/decoder.\",\n",
    "        \"vae\": \"Optimise ELBO = reconstruction + KL divergence to prior.\",\n",
    "        \"gan\": \"Adversarial min-max between generator and discriminator losses.\",\n",
    "        \"diffusion\": \"Score matching/denoising losses across noisy timesteps.\",\n",
    "    }\n",
    "\n",
    "\n",
    "def run_all_demos(random_state: int = 59) -> Dict[str, object]:\n",
    "    \"\"\"Convenience entrypoint mirroring the CLI behaviour.\"\"\"\n",
    "\n",
    "    data = generate_swiss_roll(random_state=random_state)\n",
    "    ae = train_autoencoder_synthetic(data=data, random_state=random_state)\n",
    "    vae = train_variational_autoencoder_synthetic(data=data, random_state=random_state)\n",
    "    diffusion = train_diffusion_denoiser(data=data, random_state=random_state)\n",
    "    gan_log = gan_training_summary(random_state=random_state)\n",
    "    return {\n",
    "        \"autoencoder\": ae,\n",
    "        \"vae\": vae,\n",
    "        \"diffusion\": diffusion,\n",
    "        \"gan\": gan_log,\n",
    "        \"objectives\": summarise_generative_objectives(),\n",
    "    }\n",
    "\n",
    "\n",
    "def _demo() -> None:\n",
    "    stats = run_all_demos()\n",
    "    print(\n",
    "        f\"Autoencoder start/end loss: {stats['autoencoder'].losses[0]:.4f} -> {stats['autoencoder'].losses[-1]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"VAE start/end loss: {stats['vae'].losses[0]:.4f} -> {stats['vae'].losses[-1]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Diffusion start/end loss: {stats['diffusion'].losses[0]:.4f} -> {stats['diffusion'].losses[-1]:.4f}\"\n",
    "    )\n",
    "    print(f\"GAN terminal generator mean: {stats['gan'][-1]['gen_mean']:.3f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
