{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06f7574",
   "metadata": {},
   "source": [
    "# Day 55 â€“ Advanced Unsupervised Learning\n",
    "\n",
    "Density-based clustering, hierarchical approaches, and modern embeddings unlock structure within messy\n",
    "unlabelled datasets. Use the resources in this folder to:\n",
    "\n",
    "- Compare DBSCAN and agglomerative clustering on reproducible customer segmentation datasets.\n",
    "- Generate t-SNE and UMAP-style embeddings for storytelling-ready visualisations of high-dimensional data.\n",
    "- Train compact autoencoders that reconstruct core signal while surfacing anomalies through reconstruction error.\n",
    "- Combine reconstruction-based methods with classic isolation forests for a practical anomaly detection workflow.\n",
    "\n",
    "Run `python Day_55_Advanced_Unsupervised_Learning/solutions.py` to reproduce the cluster assignments,\n",
    "embeddings, and anomaly scores featured in the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0c63e",
   "metadata": {},
   "source": [
    "Advanced unsupervised learning helpers for Day 55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ff0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClusteringResult:\n",
    "    \"\"\"Simple container for clustering outputs.\"\"\"\n",
    "\n",
    "    labels: NDArray[np.int_]\n",
    "    model: object\n",
    "\n",
    "\n",
    "def generate_clustering_data(\n",
    "    n_samples: int = 450,\n",
    "    random_state: int = 55,\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.int_]]:\n",
    "    \"\"\"Return a reproducible blob dataset for clustering experiments.\"\"\"\n",
    "\n",
    "    X, y = make_blobs(\n",
    "        n_samples=n_samples,\n",
    "        centers=[[0, 0], [4, 4], [-4, 4]],\n",
    "        cluster_std=[0.55, 0.6, 0.7],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def run_dbscan(\n",
    "    X: ArrayLike,\n",
    "    eps: float = 0.55,\n",
    "    min_samples: int = 8,\n",
    "    scale: bool = True,\n",
    ") -> ClusteringResult:\n",
    "    \"\"\"Cluster the dataset with DBSCAN and optional feature scaling.\"\"\"\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = model.fit_predict(X)\n",
    "    return ClusteringResult(labels=labels, model=model)\n",
    "\n",
    "\n",
    "def run_agglomerative(\n",
    "    X: ArrayLike,\n",
    "    n_clusters: int = 3,\n",
    "    linkage: str = \"ward\",\n",
    ") -> ClusteringResult:\n",
    "    \"\"\"Perform agglomerative clustering and return labels and the fitted estimator.\"\"\"\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "    labels = model.fit_predict(X)\n",
    "    return ClusteringResult(labels=labels, model=model)\n",
    "\n",
    "\n",
    "def compute_tsne_embedding(\n",
    "    X: ArrayLike,\n",
    "    n_components: int = 2,\n",
    "    perplexity: float = 30.0,\n",
    "    random_state: int = 55,\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"Return a deterministic t-SNE embedding for visualisation.\"\"\"\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    tsne = TSNE(\n",
    "        n_components=n_components,\n",
    "        perplexity=perplexity,\n",
    "        random_state=random_state,\n",
    "        init=\"pca\",\n",
    "        learning_rate=\"auto\",\n",
    "        max_iter=1500,\n",
    "    )\n",
    "    return tsne.fit_transform(X)\n",
    "\n",
    "\n",
    "def build_autoencoder(\n",
    "    input_dim: int,\n",
    "    encoding_dim: int = 2,\n",
    "    random_state: int = 55,\n",
    ") -> keras.Model:\n",
    "    \"\"\"Create a compact dense autoencoder.\"\"\"\n",
    "\n",
    "    tf.keras.utils.set_random_seed(random_state)\n",
    "    encoder_inputs = keras.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(encoding_dim, activation=\"relu\")(encoder_inputs)\n",
    "    decoded = layers.Dense(input_dim, activation=\"linear\")(encoded)\n",
    "    autoencoder = keras.Model(inputs=encoder_inputs, outputs=decoded)\n",
    "    autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=\"mse\")\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "def train_autoencoder(\n",
    "    model: keras.Model,\n",
    "    X_train: ArrayLike,\n",
    "    epochs: int = 80,\n",
    "    batch_size: int = 32,\n",
    ") -> keras.callbacks.History:\n",
    "    \"\"\"Train the autoencoder on the provided dataset.\"\"\"\n",
    "\n",
    "    X_train = np.asarray(X_train, dtype=float)\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        X_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def reconstruction_errors(model: keras.Model, X: ArrayLike) -> NDArray[np.float64]:\n",
    "    \"\"\"Return mean squared reconstruction error per sample.\"\"\"\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    reconstructions = model.predict(X, verbose=0)\n",
    "    return np.mean((X - reconstructions) ** 2, axis=1)\n",
    "\n",
    "\n",
    "def autoencoder_anomaly_threshold(errors: ArrayLike, quantile: float = 0.95) -> float:\n",
    "    \"\"\"Return an empirical anomaly threshold from reconstruction errors.\"\"\"\n",
    "\n",
    "    errors = np.asarray(errors, dtype=float)\n",
    "    return float(np.quantile(errors, quantile))\n",
    "\n",
    "\n",
    "def detect_anomalies_with_autoencoder(\n",
    "    model: keras.Model,\n",
    "    X: ArrayLike,\n",
    "    threshold: float,\n",
    ") -> NDArray[np.int_]:\n",
    "    \"\"\"Return a binary mask where 1 indicates an anomaly.\"\"\"\n",
    "\n",
    "    errors = reconstruction_errors(model, X)\n",
    "    return (errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "def isolation_forest_scores(\n",
    "    X: ArrayLike,\n",
    "    contamination: float = 0.05,\n",
    "    random_state: int = 55,\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"Return anomaly scores from an isolation forest.\"\"\"\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    model = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=random_state,\n",
    "        n_estimators=200,\n",
    "    )\n",
    "    model.fit(X)\n",
    "    return -model.decision_function(X)\n",
    "\n",
    "\n",
    "def lof_anomaly_scores(\n",
    "    X: ArrayLike, n_neighbors: int = 20\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.int_]]:\n",
    "    \"\"\"Return Local Outlier Factor scores (larger implies more anomalous).\"\"\"\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=False)\n",
    "    labels = lof.fit_predict(X)\n",
    "    return -lof.negative_outlier_factor_, labels\n",
    "\n",
    "\n",
    "def tsne_distance_preservation(X: ArrayLike, embedding: ArrayLike) -> float:\n",
    "    \"\"\"Compute Spearman correlation between pairwise distances before and after embedding.\"\"\"\n",
    "\n",
    "    original = pairwise_distances(np.asarray(X, dtype=float))\n",
    "    embedded = pairwise_distances(np.asarray(embedding, dtype=float))\n",
    "    original_flat = original[np.triu_indices_from(original, k=1)]\n",
    "    embedded_flat = embedded[np.triu_indices_from(embedded, k=1)]\n",
    "    if original_flat.size == 0:\n",
    "        return 1.0\n",
    "    return float(np.corrcoef(original_flat, embedded_flat)[0, 1])\n",
    "\n",
    "\n",
    "def demo_unsupervised_pipeline(random_state: int = 55) -> Dict[str, float]:\n",
    "    \"\"\"Run the featured unsupervised workflow and return summary statistics.\"\"\"\n",
    "\n",
    "    X, _ = generate_clustering_data(random_state=random_state)\n",
    "    dbscan_result = run_dbscan(X)\n",
    "    agg_result = run_agglomerative(X)\n",
    "    embedding = compute_tsne_embedding(X, random_state=random_state)\n",
    "    distance_corr = tsne_distance_preservation(X, embedding)\n",
    "\n",
    "    auto = build_autoencoder(input_dim=X.shape[1], random_state=random_state)\n",
    "    train_autoencoder(auto, X, epochs=60)\n",
    "    errors = reconstruction_errors(auto, X)\n",
    "    threshold = autoencoder_anomaly_threshold(errors, 0.9)\n",
    "    anomaly_rate = float(np.mean(errors > threshold))\n",
    "\n",
    "    if_scores = isolation_forest_scores(X)\n",
    "\n",
    "    return {\n",
    "        \"dbscan_clusters\": float(\n",
    "            len(set(dbscan_result.labels)) - (1 if -1 in dbscan_result.labels else 0)\n",
    "        ),\n",
    "        \"agglomerative_clusters\": float(len(np.unique(agg_result.labels))),\n",
    "        \"tsne_distance_corr\": distance_corr,\n",
    "        \"autoencoder_threshold\": threshold,\n",
    "        \"autoencoder_anomaly_rate\": anomaly_rate,\n",
    "        \"isolation_forest_score_mean\": float(np.mean(if_scores)),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    summary = demo_unsupervised_pipeline()\n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key}: {value:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
