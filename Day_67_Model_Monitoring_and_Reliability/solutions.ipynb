{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e16c2e5",
   "metadata": {},
   "source": [
    "# Day 67 – Model Monitoring and Reliability Engineering\n",
    "\n",
    "The final instalment of the MLOps arc closes the loop from deployment to\n",
    "operations. After mastering persistence (Day 50), automation (Day 65),\n",
    "and serving (Day 66), this lesson introduces the observability patterns\n",
    "that keep models trustworthy in production.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- **Data and concept drift detection** – Track feature distributions with\n",
    "  population stability index (PSI), Kullback–Leibler divergence, or\n",
    "  threshold-based heuristics that trigger alerts when inputs shift.\n",
    "- **Automated retraining triggers** – Combine drift signals, performance\n",
    "  metrics, and business guardrails to decide when to schedule a new\n",
    "  training job.\n",
    "- **Progressive delivery** – Roll out models with canary or shadow\n",
    "  deployments, automatically rolling back if latency or accuracy\n",
    "  regressions appear.\n",
    "- **Observability tooling** – Instrument models with Prometheus metrics\n",
    "  exporters, OpenTelemetry traces, and structured logging for rapid\n",
    "  incident response.\n",
    "\n",
    "## Hands-on practice\n",
    "\n",
    "`solutions.py` provides synthetic drift generators, simple detection\n",
    "algorithms, and a canary analysis helper. These components emit metrics\n",
    "that could be scraped by Prometheus or pushed to OpenTelemetry\n",
    "collectors, illustrating how to connect monitoring to automated decision\n",
    "systems.\n",
    "\n",
    "Run the script to see drift alerts bubble up:\n",
    "\n",
    "```bash\n",
    "python Day_67_Model_Monitoring_and_Reliability/solutions.py\n",
    "```\n",
    "\n",
    "`tests/test_day_67.py` feeds controlled distribution shifts through the\n",
    "helpers and confirms that alerts fire, retraining queues populate, and\n",
    "canary verdicts respect latency/accuracy thresholds.\n",
    "\n",
    "## Extend the exercise\n",
    "\n",
    "- Replace the heuristic drift detector with `alibi-detect`, `evidently`,\n",
    "  or scikit-multiflow to monitor complex multivariate shifts.\n",
    "- Export the observability payloads to Prometheus using `prometheus- client` counters, gauges, and histograms.\n",
    "- Emit OpenTelemetry traces that attach prediction metadata and user\n",
    "  identifiers for distributed tracing across microservices.\n",
    "- Integrate human-in-the-loop acknowledgement by forwarding alerts to an\n",
    "  incident management platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514cef0b",
   "metadata": {},
   "source": [
    "Monitoring utilities for production ML systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from statistics import mean, pstdev\n",
    "from typing import Any, Dict, Iterable, List, Mapping\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DriftReport:\n",
    "    feature: str\n",
    "    baseline_mean: float\n",
    "    current_mean: float\n",
    "    drift_score: float\n",
    "    triggered: bool\n",
    "\n",
    "\n",
    "def compute_mean_drift(\n",
    "    baseline: Iterable[float],\n",
    "    current: Iterable[float],\n",
    "    *,\n",
    "    threshold: float = 0.2,\n",
    ") -> DriftReport:\n",
    "    \"\"\"Compare distributions using a simple relative mean difference.\"\"\"\n",
    "\n",
    "    baseline_list = list(baseline)\n",
    "    current_list = list(current)\n",
    "    if not baseline_list or not current_list:\n",
    "        raise ValueError(\"Both baseline and current samples must be provided\")\n",
    "    baseline_mean = mean(baseline_list)\n",
    "    current_mean = mean(current_list)\n",
    "    baseline_std = pstdev(baseline_list) or 1e-6\n",
    "    drift_score = abs(current_mean - baseline_mean) / baseline_std\n",
    "    triggered = drift_score >= threshold\n",
    "    return DriftReport(\n",
    "        feature=\"feature_value\",\n",
    "        baseline_mean=round(baseline_mean, 4),\n",
    "        current_mean=round(current_mean, 4),\n",
    "        drift_score=round(drift_score, 4),\n",
    "        triggered=triggered,\n",
    "    )\n",
    "\n",
    "\n",
    "def should_trigger_retraining(\n",
    "    report: DriftReport, *, accuracy: float, latency: float\n",
    ") -> bool:\n",
    "    \"\"\"Decide whether to retrain given drift and live metrics.\"\"\"\n",
    "\n",
    "    if report.triggered:\n",
    "        return True\n",
    "    if accuracy < 0.78:\n",
    "        return True\n",
    "    if latency > 0.5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CanaryVerdict:\n",
    "    promote: bool\n",
    "    reason: str\n",
    "    metrics: Dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "def evaluate_canary(\n",
    "    baseline_metrics: Mapping[str, float],\n",
    "    candidate_metrics: Mapping[str, float],\n",
    "    *,\n",
    "    allowed_latency_delta: float = 0.05,\n",
    "    min_accuracy: float = 0.8,\n",
    ") -> CanaryVerdict:\n",
    "    \"\"\"Compare baseline vs candidate metrics and decide promotion.\"\"\"\n",
    "\n",
    "    latency_delta = candidate_metrics.get(\"latency\", 0.0) - baseline_metrics.get(\n",
    "        \"latency\", 0.0\n",
    "    )\n",
    "    accuracy = candidate_metrics.get(\"accuracy\", 0.0)\n",
    "    error_rate = candidate_metrics.get(\"error_rate\", 0.0)\n",
    "    if accuracy < min_accuracy:\n",
    "        return CanaryVerdict(False, \"Accuracy below threshold\", {\"accuracy\": accuracy})\n",
    "    if latency_delta > allowed_latency_delta:\n",
    "        return CanaryVerdict(\n",
    "            False, \"Latency regression\", {\"latency_delta\": round(latency_delta, 4)}\n",
    "        )\n",
    "    if error_rate > baseline_metrics.get(\"error_rate\", 0.0) * 1.2:\n",
    "        return CanaryVerdict(False, \"Error rate increase\", {\"error_rate\": error_rate})\n",
    "    return CanaryVerdict(\n",
    "        True,\n",
    "        \"Canary healthy\",\n",
    "        {\"accuracy\": accuracy, \"latency_delta\": round(latency_delta, 4)},\n",
    "    )\n",
    "\n",
    "\n",
    "def build_observability_snapshot(\n",
    "    report: DriftReport,\n",
    "    verdict: CanaryVerdict,\n",
    "    *,\n",
    "    predictions_served: int,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Aggregate metrics for Prometheus/OpenTelemetry exporters.\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"drift\": {\n",
    "            \"feature\": report.feature,\n",
    "            \"score\": report.drift_score,\n",
    "            \"triggered\": report.triggered,\n",
    "        },\n",
    "        \"canary\": {\n",
    "            \"promote\": verdict.promote,\n",
    "            \"reason\": verdict.reason,\n",
    "            \"metrics\": verdict.metrics,\n",
    "        },\n",
    "        \"counters\": {\n",
    "            \"predictions_served_total\": predictions_served,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def detect_drift_across_features(\n",
    "    baseline_frame: Mapping[str, Iterable[float]],\n",
    "    current_frame: Mapping[str, Iterable[float]],\n",
    "    *,\n",
    "    threshold: float = 0.2,\n",
    ") -> Dict[str, DriftReport]:\n",
    "    \"\"\"Apply mean drift detection across multiple features.\"\"\"\n",
    "\n",
    "    reports: Dict[str, DriftReport] = {}\n",
    "    for feature, baseline_values in baseline_frame.items():\n",
    "        current_values = current_frame.get(feature)\n",
    "        if current_values is None:\n",
    "            continue\n",
    "        reports[feature] = compute_mean_drift(\n",
    "            baseline_values, current_values, threshold=threshold\n",
    "        )\n",
    "    return reports\n",
    "\n",
    "\n",
    "def enqueue_retraining_tasks(\n",
    "    reports: Mapping[str, DriftReport],\n",
    "    *,\n",
    "    accuracy: float,\n",
    "    latency: float,\n",
    ") -> List[str]:\n",
    "    \"\"\"Return a queue of features that should trigger retraining.\"\"\"\n",
    "\n",
    "    queue: List[str] = []\n",
    "    for feature, report in reports.items():\n",
    "        if should_trigger_retraining(report, accuracy=accuracy, latency=latency):\n",
    "            queue.append(feature)\n",
    "    return queue\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    baseline = [0.1, 0.2, 0.15, 0.18]\n",
    "    current = [0.35, 0.4, 0.45, 0.38]\n",
    "    report = compute_mean_drift(baseline, current)\n",
    "    verdict = evaluate_canary(\n",
    "        {\"latency\": 0.2, \"accuracy\": 0.83, \"error_rate\": 0.05},\n",
    "        {\"latency\": 0.22, \"accuracy\": 0.85, \"error_rate\": 0.04},\n",
    "    )\n",
    "    snapshot = build_observability_snapshot(report, verdict, predictions_served=1200)\n",
    "    print(\"Drift report\", report)  # noqa: T201\n",
    "    print(\"Canary verdict\", verdict)  # noqa: T201\n",
    "    print(\"Observability snapshot\", snapshot)  # noqa: T201\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
