{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671e281e",
   "metadata": {},
   "source": [
    "Utility functions for building and training a small LSTM on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19dd359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int = DEFAULT_SEED) -> None:\n",
    "    \"\"\"Synchronise NumPy and TensorFlow RNGs for deterministic runs.\"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "\n",
    "def prepare_imdb_data(\n",
    "    *, vocab_size: int = 10_000, max_length: int = 256\n",
    ") -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Load the IMDB sentiment dataset and pad sequences to a uniform length.\"\"\"\n",
    "\n",
    "    (train_data, train_labels), (test_data, test_labels) = datasets.imdb.load_data(\n",
    "        num_words=vocab_size\n",
    "    )\n",
    "\n",
    "    train_data_padded = pad_sequences(train_data, maxlen=max_length, padding=\"post\")\n",
    "    test_data_padded = pad_sequences(test_data, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "    return (train_data_padded, np.array(train_labels)), (\n",
    "        test_data_padded,\n",
    "        np.array(test_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "def build_rnn_model(\n",
    "    *,\n",
    "    vocab_size: int = 10_000,\n",
    "    embedding_dim: int = 16,\n",
    "    max_length: int = 256,\n",
    "    lstm_units: int = 64,\n",
    "    dense_units: int = 64,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"Create an LSTM-based classifier mirroring the tutorial architecture.\"\"\"\n",
    "\n",
    "    model = models.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(max_length,)),\n",
    "            layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "            layers.LSTM(lstm_units),\n",
    "            layers.Dense(dense_units, activation=\"relu\"),\n",
    "            layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_rnn_model(\n",
    "    model: tf.keras.Model,\n",
    "    train_data: np.ndarray,\n",
    "    train_labels: np.ndarray,\n",
    "    *,\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 128,\n",
    "    validation_split: float = 0.2,\n",
    "    verbose: int = 1,\n",
    "    shuffle: bool = True,\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \"\"\"Fit the RNN and return the training history.\"\"\"\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=verbose,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_rnn_model(\n",
    "    model: tf.keras.Model,\n",
    "    test_data: np.ndarray,\n",
    "    test_labels: np.ndarray,\n",
    "    *,\n",
    "    verbose: int = 2,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate the trained RNN on the held-out test data.\"\"\"\n",
    "\n",
    "    return model.evaluate(test_data, test_labels, verbose=verbose, return_dict=True)\n",
    "\n",
    "\n",
    "def run_full_workflow(\n",
    "    *,\n",
    "    vocab_size: int = 10_000,\n",
    "    max_length: int = 256,\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 128,\n",
    "    verbose: int = 1,\n",
    "    seed: int = DEFAULT_SEED,\n",
    ") -> Tuple[tf.keras.callbacks.History, Dict[str, float], tf.keras.Model]:\n",
    "    \"\"\"Train and evaluate the IMDB LSTM classifier end-to-end.\"\"\"\n",
    "\n",
    "    set_global_seed(seed)\n",
    "    (train_data, train_labels), (test_data, test_labels) = prepare_imdb_data(\n",
    "        vocab_size=vocab_size, max_length=max_length\n",
    "    )\n",
    "    model = build_rnn_model(\n",
    "        vocab_size=vocab_size, embedding_dim=16, max_length=max_length\n",
    "    )\n",
    "    history = train_rnn_model(\n",
    "        model,\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    metrics = evaluate_rnn_model(model, test_data, test_labels, verbose=verbose)\n",
    "    return history, metrics, model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    history, metrics, model = run_full_workflow()\n",
    "\n",
    "    print(\"--- RNN (LSTM) for IMDB Sentiment Classification ---\")\n",
    "    model.summary()\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Final training accuracy:\", history.history[\"accuracy\"][-1])\n",
    "    print(\"Test metrics:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
