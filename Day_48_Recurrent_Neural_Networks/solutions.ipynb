{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908d76c5",
   "metadata": {},
   "source": [
    "# Day 48: Recurrent Neural Networks (RNNs) for Sequence Data\n",
    "\n",
    "Welcome to Day 48! Today, we explore **Recurrent Neural Networks (RNNs)**, a class of neural networks designed specifically for handling **sequential data**, such as time series, text, or audio.\n",
    "\n",
    "> **Prerequisites:** Install TensorFlow with `pip install tensorflow` so you can build and train the RNN examples (CPU build by default; follow TensorFlow's GPU instructions if you have compatible hardware). Need to brush up on `pip` usage? Revisit the Day 20 Python Package Manager lesson.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What are RNNs?\n",
    "\n",
    "Unlike feedforward networks (like ANNs and CNNs), which process inputs independently, RNNs have **loops** in them, allowing information to persist. This \"memory\" lets them use information from prior inputs to influence the current input and output.\n",
    "\n",
    "- **The Loop:** An RNN processes a sequence one element at a time. At each step, the output from the previous step is fed back as an input to the current step. This creates a hidden state that acts as a memory of the sequence seen so far.\n",
    "\n",
    "### The Vanishing Gradient Problem\n",
    "\n",
    "Simple RNNs struggle to learn long-range dependencies (patterns over long sequences). This is due to the **vanishing gradient problem**, where the gradients used to update the network's weights become very small during backpropagation, effectively stopping the learning process for earlier time steps.\n",
    "\n",
    "### Advanced RNN Architectures\n",
    "\n",
    "To solve this problem, more sophisticated RNN variants were developed:\n",
    "\n",
    "1. **Long Short-Term Memory (LSTM)**\n",
    "\n",
    "   - LSTMs are a special kind of RNN that are explicitly designed to avoid the long-term dependency problem.\n",
    "   - They have a more complex internal structure called a **cell**, which includes three **gates** (forget, input, and output gates). These gates regulate the flow of information, allowing the network to remember or forget information over long periods.\n",
    "\n",
    "1. **Gated Recurrent Unit (GRU)**\n",
    "\n",
    "   - GRUs are a simplified version of LSTMs. They combine the forget and input gates into a single \"update gate\" and have fewer parameters.\n",
    "   - They often perform similarly to LSTMs but are computationally more efficient.\n",
    "\n",
    "### Typical RNN Architecture for Classification\n",
    "\n",
    "1. **Input / Embedding Layer:** For text data, an `Embedding` layer is often used first to convert integer indices (representing words) into dense vectors.\n",
    "1. **Recurrent Layer (LSTM or GRU):** This layer processes the sequence of vectors.\n",
    "1. **Dense Layer:** A standard fully connected layer for classification.\n",
    "1. **Output Layer:** Produces the final prediction.\n",
    "\n",
    "______________________________________________________________________\n",
    "\n",
    "## Practice Exercise\n",
    "\n",
    "- The refactored `solutions.py` module exposes building blocks such as `prepare_imdb_data`, `build_rnn_model`, and `train_rnn_model`. Import what you need for notebooks or experiments; nothing trains automatically when the module is imported.\n",
    "- The goal remains to classify IMDB reviews as `positive` or `negative`, and the workflow now mirrors the modular structure used for other deep-learning lessons.\n",
    "- The code covers:\n",
    "  1. Loading and preprocessing the IMDB text data. (Reviews are pre-processed into sequences of integers).\n",
    "  1. Padding the sequences to ensure they all have the same length.\n",
    "  1. Building a sequential model with an `Embedding` layer, an `LSTM` layer, and `Dense` layers.\n",
    "  1. Compiling and training the RNN.\n",
    "  1. Evaluating its performance on the test set.\n",
    "\n",
    "### Running the example and tests\n",
    "\n",
    "- Launch the full script with `python Day_48_Recurrent_Neural_Networks/solutions.py` to download IMDB, train the LSTM, and print evaluation metrics.\n",
    "- For a very fast check, use `pytest tests/test_day_48.py`. The test stubs out the dataset loader with a handful of synthetic sequences and runs a single epoch so it finishes quickly.\n",
    "- LSTM models benefit significantly from GPU acceleration. If you have CUDA/cuDNN configured, TensorFlow will pick it up automatically; otherwise the CPU execution path will still work (just slower)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50f552",
   "metadata": {},
   "source": [
    "Utility functions for building and training a small LSTM on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int = DEFAULT_SEED) -> None:\n",
    "    \"\"\"Synchronise NumPy and TensorFlow RNGs for deterministic runs.\"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "\n",
    "def prepare_imdb_data(\n",
    "    *, vocab_size: int = 10_000, max_length: int = 256\n",
    ") -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Load the IMDB sentiment dataset and pad sequences to a uniform length.\"\"\"\n",
    "\n",
    "    (train_data, train_labels), (test_data, test_labels) = datasets.imdb.load_data(\n",
    "        num_words=vocab_size\n",
    "    )\n",
    "\n",
    "    train_data_padded = pad_sequences(train_data, maxlen=max_length, padding=\"post\")\n",
    "    test_data_padded = pad_sequences(test_data, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "    return (train_data_padded, np.array(train_labels)), (\n",
    "        test_data_padded,\n",
    "        np.array(test_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "def build_rnn_model(\n",
    "    *,\n",
    "    vocab_size: int = 10_000,\n",
    "    embedding_dim: int = 16,\n",
    "    max_length: int = 256,\n",
    "    lstm_units: int = 64,\n",
    "    dense_units: int = 64,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"Create an LSTM-based classifier mirroring the tutorial architecture.\"\"\"\n",
    "\n",
    "    model = models.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(max_length,)),\n",
    "            layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "            layers.LSTM(lstm_units),\n",
    "            layers.Dense(dense_units, activation=\"relu\"),\n",
    "            layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_rnn_model(\n",
    "    model: tf.keras.Model,\n",
    "    train_data: np.ndarray,\n",
    "    train_labels: np.ndarray,\n",
    "    *,\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 128,\n",
    "    validation_split: float = 0.2,\n",
    "    verbose: int = 1,\n",
    "    shuffle: bool = True,\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \"\"\"Fit the RNN and return the training history.\"\"\"\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=verbose,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_rnn_model(\n",
    "    model: tf.keras.Model,\n",
    "    test_data: np.ndarray,\n",
    "    test_labels: np.ndarray,\n",
    "    *,\n",
    "    verbose: int = 2,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate the trained RNN on the held-out test data.\"\"\"\n",
    "\n",
    "    return model.evaluate(test_data, test_labels, verbose=verbose, return_dict=True)\n",
    "\n",
    "\n",
    "def run_full_workflow(\n",
    "    *,\n",
    "    vocab_size: int = 10_000,\n",
    "    max_length: int = 256,\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 128,\n",
    "    verbose: int = 1,\n",
    "    seed: int = DEFAULT_SEED,\n",
    ") -> Tuple[tf.keras.callbacks.History, Dict[str, float], tf.keras.Model]:\n",
    "    \"\"\"Train and evaluate the IMDB LSTM classifier end-to-end.\"\"\"\n",
    "\n",
    "    set_global_seed(seed)\n",
    "    (train_data, train_labels), (test_data, test_labels) = prepare_imdb_data(\n",
    "        vocab_size=vocab_size, max_length=max_length\n",
    "    )\n",
    "    model = build_rnn_model(\n",
    "        vocab_size=vocab_size, embedding_dim=16, max_length=max_length\n",
    "    )\n",
    "    history = train_rnn_model(\n",
    "        model,\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    metrics = evaluate_rnn_model(model, test_data, test_labels, verbose=verbose)\n",
    "    return history, metrics, model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    history, metrics, model = run_full_workflow()\n",
    "\n",
    "    print(\"--- RNN (LSTM) for IMDB Sentiment Classification ---\")\n",
    "    model.summary()\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Final training accuracy:\", history.history[\"accuracy\"][-1])\n",
    "    print(\"Test metrics:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
