Reinforcement learning (RL) balances exploration and exploitation while offline evaluation keeps policies safe. After this lesson you can:

- Compare value-based, policy-based, and actorâ€“critic methods across episodic control problems.
- Simulate contextual bandits and conservative offline policy evaluation with replay buffers and importance sampling.
- Analyse stability tricks: entropy bonuses, target networks, batch-constrained Q-learning, and doubly robust estimators.
- Reproduce seeded experiments that converge to expected reward thresholds for regression tests.

Execute `python Day_61_Reinforcement_and_Offline_Learning/solutions.py` to walk through deterministic policy optimisation, offline evaluation diagnostics, and bandit baselines.

## Additional Materials

- [solutions.py](https://github.com/saint2706/Coding-For-MBA/blob/main/Day_61_Reinforcement_and_Offline_Learning/solutions.py)
