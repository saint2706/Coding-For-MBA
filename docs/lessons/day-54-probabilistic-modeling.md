Gaussian mixtures, Bayesian classifiers, expectation-maximisation, and hidden Markov models power
probabilistic reasoning for ambiguous business signals. Use the notebook or `solutions.py` helpers to:

- Simulate multi-modal customer cohorts and recover their structure with Gaussian mixtures.
- Train Gaussian Naive Bayes classifiers that expose posterior log-probabilities for fast decision rules.
- Run expectation-maximisation loops to maximise mixture log-likelihoods on noisy, partially labelled data.
- Implement a numerically stable hidden Markov forward pass to evaluate sequence likelihoods under state
  transitions and Gaussian emissions.

Execute `python Day_54_Probabilistic_Modeling/solutions.py` to print representative log-likelihood outputs
for the reproducible toy datasets.

## Additional Materials

- [solutions.py](https://github.com/saint2706/Coding-For-MBA/blob/main/Day_54_Probabilistic_Modeling/solutions.py)
