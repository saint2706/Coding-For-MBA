{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Coding for MBA Documentation","text":"<p>Welcome to the official documentation hub for the Coding for MBA curriculum. This MkDocs-powered site pulls content directly from the repository so you can:</p> <ul> <li>Navigate the entire 67-day learning path in one place.</li> <li>Read every lesson summary without digging through folders.</li> <li>Jump straight into the companion notebooks or Python scripts for each topic.</li> </ul> <p>Prefer a darker UI?</p> <p>Use the theme toggle in the header to switch between the light and dark palettes.</p>"},{"location":"#whats-inside","title":"What's inside","text":"<ul> <li>Machine Learning Curriculum \u2013 Phased roadmap that explains how the Day 40\u201367 sequence ladders into an end-to-end ML capability.</li> <li>Lessons \u2013 Direct access to every <code>Day_*</code> lesson README, enhanced with quick links to the supporting notebooks and scripts.</li> <li>Repository Roadmap \u2013 Future development plans and enhancement priorities for the curriculum.</li> <li>Automation Commands \u2013 Custom commands for CI/CD workflows and automated testing.</li> <li>Dependency Review \u2013 Analysis of project dependencies and their purposes.</li> <li>License \u2013 Complete Apache 2.0 license information for the project.</li> <li>Accessible lesson exports \u2013 Screen-reader-ready HTML and Markdown versions of every notebook live under <code>docs/lessons/</code> for offline or assistive-technology-first study.</li> </ul>"},{"location":"#lessons","title":"Lessons","text":"<p>Use the table below to jump straight into any lesson in the 67-day journey.</p> Day Lesson Day 01 \ud83d\udcd8 Day 1: Python for Business Analytics - First Steps Day 02 \ud83d\udcd8 Day 2: Storing and Analyzing Business Data Day 03 \ud83d\udcd8 Day 3: Operators - The Tools for Business Calculation and Logic Day 04 \ud83d\udcd8 Day 4: Working with Text Data - Strings Day 05 \ud83d\udcd8 Day 5: Managing Collections of Business Data with Lists Day 06 \ud83d\udcd8 Day 6: Tuples - Storing Immutable Business Data Day 07 \ud83d\udcd8 Day 7: Sets - Managing Unique Business Data Day 08 \ud83d\udcd8 Day 8: Dictionaries - Structuring Complex Business Data Day 09 \ud83d\udcd8 Day 9: Conditionals - Implementing Business Logic Day 10 \ud83d\udcd8 Day 10: Loops - Automating Repetitive Business Tasks Day 11 \ud83d\udcd8 Day 11: Functions - Creating Reusable Business Tools Day 12 \ud83d\udcd8 Day 12: List Comprehension - Elegant Data Manipulation Day 13 \ud83d\udcd8 Day 13: Higher-Order Functions &amp; Lambda Day 14 \ud83d\udcd8 Day 14: Modules - Organizing Your Business Logic Day 15 \ud83d\udcd8 Day 15: Exception Handling - Building Robust Business Logic Day 16 \ud83d\udcd8 Day 16: File Handling for Business Analytics Day 17 \ud83d\udcd8 Day 17: Regular Expressions for Text Pattern Matching Day 18 \ud83d\udcd8 Day 18: Classes and Objects - Modeling Business Concepts Day 19 \ud83d\udcd8 Day 19: Working with Dates and Times Day 20 \ud83d\udcd8 Day 20: Python Package Manager (pip) &amp; Third-Party Libraries Day 21 \ud83d\udcd8 Day 21: Virtual Environments - Professional Project Management Day 22 \ud83d\udcd8 Day 22: NumPy - The Foundation of Numerical Computing Day 23 \ud83d\udcd8 Day 23: Pandas - Your Data Analysis Superpower Day 24 \ud83d\udcd8 Day 24: Advanced Pandas - Working with Real Data Day 25 \ud83d\udcd8 Day 25: Data Cleaning - The Most Important Skill in Analytics Day 26 \ud83d\udcd8 Day 26: Practical Statistics for Business Analysis Day 27 \ud83d\udcd8 Day 27: Data Visualization - Communicating Insights Day 28 \ud83d\udcd8 Day 28: Advanced Visualization &amp; Customization Day 29 \ud83d\udcd8 Day 29: Interactive Visualization with Plotly Day 30 \ud83d\udcd8 Day 30: Web Scraping - Extracting Data from the Web Day 31 \ud83d\udcd8 Day 31: Working with Databases in Python Day 32 \ud83d\udcd8 Day 32: Connecting to Other Databases (MySQL &amp; MongoDB) Day 33 \ud83d\udcd8 Day 33: Accessing Web APIs with <code>requests</code> Day 34 \ud83d\udcd8 Day 34: Building a Simple API with Flask Day 35 \ud83c\udf10 Day 35: Flask Web Framework Day 36 \ud83d\udcca Day 36 \u2013 Capstone Case Study Day 37 \ud83c\udf89 Day 37: Conclusion &amp; Your Journey Forward \ud83c\udf89 Day 38 Day 38: Math Foundations - Linear Algebra Day 39 Day 39: Math Foundations - Calculus Day 40 Day 40: Introduction to Machine Learning &amp; Core Concepts Day 41 Day 41 \u00b7 Supervised Learning \u2013 Regression Day 42 Day 42 \u00b7 Supervised Learning \u2013 Classification (Part 1) Day 43 Day 43 \u00b7 Supervised Learning \u2013 Classification (Part 2) Day 44 Day 44: Unsupervised Learning Day 45 Day 45: Feature Engineering &amp; Model Evaluation Day 46 Day 46: Introduction to Neural Networks &amp; Frameworks Day 47 Day 47: Convolutional Neural Networks (CNNs) for Computer Vision Day 48 Day 48: Recurrent Neural Networks (RNNs) for Sequence Data Day 49 Day 49: Natural Language Processing (NLP) Day 50 Day 50: MLOps - Model Deployment Day 51 Day 51 \u2013 Regularised Models Day 52 Day 52 \u2013 Ensemble Methods Day 53 Day 53 \u2013 Model Tuning and Feature Selection Day 54 Day 54 \u2013 Probabilistic Modeling Day 55 Day 55 \u2013 Advanced Unsupervised Learning Day 56 Day 56 \u2013 Time Series and Forecasting Day 57 Day 57 \u2013 Recommender Systems Day 58 Day 58 \u2013 Transformers and Attention Day 59 Day 59 \u2013 Generative Models Day 60 Day 60 \u2013 Graph and Geometric Learning Day 61 Day 61 \u2013 Reinforcement and Offline Learning Day 62 Day 62 \u2013 Model Interpretability and Fairness Day 63 Day 63 \u2013 Causal Inference and Uplift Modeling Day 64 Day 64 \u2013 Modern NLP Pipelines Day 65 Day 65 \u2013 MLOps Pipelines and CI/CD Automation Day 66 Day 66 \u2013 Model Deployment and Serving Patterns Day 67 Day 67 \u2013 Model Monitoring and Reliability Engineering"},{"location":"#contributing-updates","title":"Contributing updates","text":"<p>Contributions are welcome! See the repository README for setup instructions and the documentation contribution workflow.</p>"},{"location":"ENHANCEMENTS/","title":"Documentation Website Enhancements","text":""},{"location":"ENHANCEMENTS/#new-features-overview","title":"\ud83c\udf89 New Features Overview","text":"<p>This repository now includes comprehensive enhancements to transform the static documentation website into a dynamic, interactive learning platform.</p>"},{"location":"ENHANCEMENTS/#key-features","title":"\ud83d\ude80 Key Features","text":""},{"location":"ENHANCEMENTS/#1-interactive-jupyter-notebooks","title":"1. Interactive Jupyter Notebooks","text":"<p>JupyterLite Integration - Full Jupyter environment running entirely in the browser - No server or installation required - Powered by WebAssembly and Pyodide - Pre-loaded with NumPy, Pandas, Matplotlib, and more</p> <p>Binder Integration - Launch notebooks in cloud-based Jupyter environment - Full Python environment with all dependencies - Perfect for complex computations</p> <p>Benefits: - \u2705 Zero setup - works immediately - \u2705 Private - all code runs locally - \u2705 Persistent - work is saved in browser - \u2705 Fast - instant execution after initial load</p>"},{"location":"ENHANCEMENTS/#2-interactive-code-widgets","title":"2. Interactive Code Widgets","text":"<p>Pyodide-Powered Console - Run Python code directly in documentation pages - Edit and execute code snippets - Real-time output display - Error handling with helpful messages</p> <p>Features: - Keyboard shortcuts (Ctrl/Cmd + Enter) - Syntax highlighting - Copy/paste support - Mobile-friendly interface</p>"},{"location":"ENHANCEMENTS/#3-progress-tracking","title":"3. Progress Tracking","text":"<p>Learning Progress System - Track completed lessons - Visual progress indicators - Completion percentage - Export/import progress data</p> <p>Features: - localStorage-based (privacy-friendly) - Syncs across tabs - Floating \"Mark Complete\" button - Progress widget in sidebar</p>"},{"location":"ENHANCEMENTS/#4-enhanced-accessibility","title":"4. Enhanced Accessibility","text":"<p>WCAG 2.1 Level AA Compliance - Full keyboard navigation - Screen reader support - ARIA labels on all interactive elements - High contrast mode - Reduced motion support</p> <p>Features: - Skip links for navigation - Focus indicators - Descriptive labels - Accessible code widgets</p>"},{"location":"ENHANCEMENTS/#5-improved-user-experience","title":"5. Improved User Experience","text":"<p>Enhanced Navigation - Breadcrumb trails - Previous/next lesson links - Related lessons suggestions - Quick access to prerequisites</p> <p>Search Improvements - Full-text search including notebooks - Search suggestions - Highlight matching terms - Filter by tags</p> <p>Visual Enhancements - Dark/light mode toggle - Responsive design - Modern Material Design theme - Smooth animations</p>"},{"location":"ENHANCEMENTS/#file-structure","title":"\ud83d\udcc1 File Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 javascripts/\n\u2502   \u251c\u2500\u2500 pyodide-console.js      # Interactive Python console\n\u2502   \u2514\u2500\u2500 progress-tracker.js     # Progress tracking system\n\u251c\u2500\u2500 stylesheets/\n\u2502   \u251c\u2500\u2500 extra.css               # Base custom styles\n\u2502   \u2514\u2500\u2500 interactive-widgets.css # Widget styles\n\u251c\u2500\u2500 website-improvements.md     # Comprehensive recommendations\n\u251c\u2500\u2500 implementation-guide.md     # Step-by-step implementation\n\u2514\u2500\u2500 demo-enhanced-lesson.md     # Sample enhanced lesson\n\ntools/\n\u2514\u2500\u2500 integrate_jupyterlite.py    # JupyterLite build automation\n\njupyter_lite_config.json        # JupyterLite configuration\nmkdocs-enhanced.yml            # Enhanced MkDocs config (sample)\n</code></pre>"},{"location":"ENHANCEMENTS/#implementation-status","title":"\ud83c\udfaf Implementation Status","text":""},{"location":"ENHANCEMENTS/#completed","title":"\u2705 Completed","text":"<ul> <li>Comprehensive recommendations document</li> <li>Sample JupyterLite configuration</li> <li>Interactive code widget implementation</li> <li>Progress tracking system</li> <li>Enhanced CSS for widgets</li> <li>JupyterLite integration tool</li> <li>Enhanced MkDocs configuration</li> <li>Implementation guide</li> <li>Demo lesson page</li> </ul>"},{"location":"ENHANCEMENTS/#ready-to-deploy","title":"\ud83d\udccb Ready to Deploy","text":"<p>All files are ready for use. Follow the implementation guide to deploy.</p>"},{"location":"ENHANCEMENTS/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"ENHANCEMENTS/#for-users-viewing-documentation","title":"For Users (Viewing Documentation)","text":"<ol> <li> <p>Visit the documentation site: https://saint2706.github.io/Coding-For-MBA/</p> </li> <li> <p>Launch interactive notebooks:</p> </li> <li>Click \"\ud83d\ude80 Launch in JupyterLite\" on any lesson page</li> <li>Wait for environment to load (~30 seconds first time)</li> <li> <p>Start coding!</p> </li> <li> <p>Track your progress:</p> </li> <li>Click \"Mark as Complete\" after finishing a lesson</li> <li>View progress in sidebar widget</li> <li> <p>Export progress for backup</p> </li> <li> <p>Run code inline:</p> </li> <li>Find interactive code blocks</li> <li>Click \"\u25b6 Run Code\" button</li> <li>Edit and re-run as needed</li> </ol>"},{"location":"ENHANCEMENTS/#for-developers-implementing-features","title":"For Developers (Implementing Features)","text":"<ol> <li> <p>Review documentation:    <pre><code>cat docs/website-improvements.md\ncat docs/implementation-guide.md\n</code></pre></p> </li> <li> <p>Test locally:    <pre><code># Install dependencies\npip install -r docs/requirements.txt\npip install jupyterlite-core jupyterlite-pyodide-kernel\n\n# Build JupyterLite\npython tools/integrate_jupyterlite.py\n\n# Serve documentation\nmkdocs serve\n</code></pre></p> </li> <li> <p>Deploy:</p> </li> <li>Update <code>.github/workflows/docs.yml</code></li> <li>Push changes</li> <li>GitHub Actions will build and deploy</li> </ol>"},{"location":"ENHANCEMENTS/#feature-comparison","title":"\ud83d\udcca Feature Comparison","text":"Feature Before After Notebook Execution \u274c None \u2705 In-browser + Cloud Code Interaction \u274c Static \u2705 Interactive widgets Progress Tracking \u274c None \u2705 Full tracking system Accessibility \u26a0\ufe0f Basic \u2705 WCAG 2.1 AA Search \u26a0\ufe0f Basic \u2705 Enhanced with notebooks Mobile Support \u2705 Yes \u2705 Enhanced"},{"location":"ENHANCEMENTS/#educational-benefits","title":"\ud83c\udf93 Educational Benefits","text":""},{"location":"ENHANCEMENTS/#for-learners","title":"For Learners","text":"<ul> <li>Hands-on practice: Execute code without setup</li> <li>Immediate feedback: See results instantly</li> <li>Self-paced: Track progress at your own speed</li> <li>Accessible: Works on any device with a browser</li> </ul>"},{"location":"ENHANCEMENTS/#for-instructors","title":"For Instructors","text":"<ul> <li>No setup required: Students start immediately</li> <li>Consistent environment: Everyone has same tools</li> <li>Progress insights: See what students complete</li> <li>Easy updates: Push changes, auto-deploy</li> </ul>"},{"location":"ENHANCEMENTS/#technical-details","title":"\ud83d\udd27 Technical Details","text":""},{"location":"ENHANCEMENTS/#technologies-used","title":"Technologies Used","text":"<p>Frontend: - MkDocs Material theme - Pyodide (Python in WebAssembly) - JupyterLite (Jupyter in browser) - Vanilla JavaScript (no frameworks) - CSS3 with CSS variables</p> <p>Backend: - GitHub Pages (static hosting) - GitHub Actions (CI/CD) - Python build scripts</p> <p>Browser Requirements: - Chrome/Edge 90+ - Firefox 88+ - Safari 14+ - WebAssembly support required</p>"},{"location":"ENHANCEMENTS/#performance","title":"Performance","text":"<p>Initial Load: - Documentation: &lt; 2 seconds - JupyterLite: ~30 seconds (first time) - Pyodide: ~10 seconds (first time)</p> <p>Subsequent Loads: - Everything cached - Instant page navigation - Fast code execution</p>"},{"location":"ENHANCEMENTS/#storage","title":"Storage","text":"<p>Browser Storage Used: - JupyterLite: ~50MB (cached) - Pyodide: ~20MB (cached) - Progress data: ~10KB (localStorage)</p>"},{"location":"ENHANCEMENTS/#privacy-security","title":"\ud83d\udd12 Privacy &amp; Security","text":""},{"location":"ENHANCEMENTS/#privacy-first-design","title":"Privacy-First Design","text":"<ul> <li>\u2705 All code runs locally in browser</li> <li>\u2705 No data sent to external servers</li> <li>\u2705 Progress stored in browser only</li> <li>\u2705 No tracking or analytics by default</li> </ul>"},{"location":"ENHANCEMENTS/#security","title":"Security","text":"<ul> <li>\u2705 Runs in browser sandbox</li> <li>\u2705 No server-side execution</li> <li>\u2705 Content Security Policy compatible</li> <li>\u2705 HTTPS only</li> </ul>"},{"location":"ENHANCEMENTS/#success-metrics","title":"\ud83d\udcc8 Success Metrics","text":""},{"location":"ENHANCEMENTS/#engagement-metrics","title":"Engagement Metrics","text":"<ul> <li>Lesson completion rate</li> <li>Time spent on interactive elements</li> <li>Code execution frequency</li> <li>Return visitor rate</li> </ul>"},{"location":"ENHANCEMENTS/#technical-metrics","title":"Technical Metrics","text":"<ul> <li>Page load time</li> <li>Interactive widget response time</li> <li>Browser compatibility</li> <li>Accessibility score</li> </ul>"},{"location":"ENHANCEMENTS/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Quiz scores (if implemented)</li> <li>Exercise completion</li> <li>User feedback ratings</li> </ul>"},{"location":"ENHANCEMENTS/#known-limitations","title":"\ud83d\udc1b Known Limitations","text":""},{"location":"ENHANCEMENTS/#jupyterlite","title":"JupyterLite","text":"<ul> <li>Some packages not available (e.g., database drivers)</li> <li>File system operations limited</li> <li>Large computations slower than native</li> <li>Network requests restricted by browser</li> </ul>"},{"location":"ENHANCEMENTS/#pyodide-console","title":"Pyodide Console","text":"<ul> <li>Limited package selection</li> <li>Slower than native Python</li> <li>Memory constraints in browser</li> <li>Some I/O operations not supported</li> </ul>"},{"location":"ENHANCEMENTS/#progress-tracking","title":"Progress Tracking","text":"<ul> <li>Browser-specific (doesn't sync across devices)</li> <li>Cleared if browser cache cleared</li> <li>No account system (by design)</li> </ul>"},{"location":"ENHANCEMENTS/#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":""},{"location":"ENHANCEMENTS/#phase-1-current","title":"Phase 1 (Current)","text":"<ul> <li>\u2705 Interactive notebooks</li> <li>\u2705 Code widgets</li> <li>\u2705 Progress tracking</li> <li>\u2705 Enhanced accessibility</li> </ul>"},{"location":"ENHANCEMENTS/#phase-2-future","title":"Phase 2 (Future)","text":"<ul> <li> Interactive quizzes</li> <li> Video integration</li> <li> Code challenges</li> <li> Leaderboards (optional)</li> </ul>"},{"location":"ENHANCEMENTS/#phase-3-future","title":"Phase 3 (Future)","text":"<ul> <li> AI-powered code hints</li> <li> Collaborative features</li> <li> Advanced analytics</li> <li> Mobile app</li> </ul>"},{"location":"ENHANCEMENTS/#contributing","title":"\ud83e\udd1d Contributing","text":""},{"location":"ENHANCEMENTS/#adding-new-features","title":"Adding New Features","text":"<ol> <li>Review existing code</li> <li>Follow coding standards</li> <li>Test in multiple browsers</li> <li>Ensure accessibility</li> <li>Update documentation</li> </ol>"},{"location":"ENHANCEMENTS/#reporting-issues","title":"Reporting Issues","text":"<p>Open an issue with: - Clear description - Steps to reproduce - Expected vs actual behavior - Browser/OS information - Screenshots if applicable</p>"},{"location":"ENHANCEMENTS/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Main Documentation: website-improvements.md</li> <li>Implementation Guide: implementation-guide.md</li> <li>Demo Lesson: demo-enhanced-lesson.md</li> </ul>"},{"location":"ENHANCEMENTS/#tips-for-best-experience","title":"\ud83d\udca1 Tips for Best Experience","text":""},{"location":"ENHANCEMENTS/#for-users","title":"For Users","text":"<ul> <li>Use modern browser (Chrome/Firefox/Safari)</li> <li>Allow cookies for progress tracking</li> <li>Be patient on first JupyterLite load</li> <li>Try keyboard shortcuts (Ctrl+Enter)</li> <li>Export progress regularly</li> </ul>"},{"location":"ENHANCEMENTS/#for-developers","title":"For Developers","text":"<ul> <li>Test in multiple browsers</li> <li>Check console for errors</li> <li>Validate accessibility</li> <li>Monitor performance</li> <li>Keep dependencies updated</li> </ul>"},{"location":"ENHANCEMENTS/#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p>Built with: - JupyterLite - Pyodide - MkDocs Material - Binder</p>"},{"location":"ENHANCEMENTS/#license","title":"\ud83d\udcc4 License","text":"<p>Same as main repository (MIT License)</p>"},{"location":"ENHANCEMENTS/#ready-to-get-started","title":"\ud83d\ude80 Ready to Get Started?","text":"<ol> <li>Users: Visit the live documentation</li> <li>Developers: Read the implementation guide</li> <li>Contributors: Check the website improvements</li> </ol> <p>Happy Learning! \ud83c\udf93</p>"},{"location":"LICENSE/","title":"Mozilla Public License Version 2.0","text":"<ol> <li>Definitions</li> </ol> <p>1.1. \"Contributor\" means each individual or legal entity that creates, contributes to the creation of, or owns Covered Software.</p> <p>1.2. \"Contributor Version\" means the combination of the Contributions of others (if any) used by a Contributor and that particular Contributor's Contribution.</p> <p>1.3. \"Contribution\" means Covered Software of a particular Contributor.</p> <p>1.4. \"Covered Software\" means Source Code Form to which the initial Contributor has attached the notice in Exhibit A, the Executable Form of such Source Code Form, and Modifications of such Source Code Form, in each case including portions thereof.</p> <p>1.5. \"Incompatible With Secondary Licenses\" means</p> <pre><code>(a) that the initial Contributor has attached the notice described\n    in Exhibit B to the Covered Software; or\n\n(b) that the Covered Software was made available under the terms of\n    version 1.1 or earlier of the License, but not also under the\n    terms of a Secondary License.\n</code></pre> <p>1.6. \"Executable Form\" means any form of the work other than Source Code Form.</p> <p>1.7. \"Larger Work\" means a work that combines Covered Software with other material, in a separate file or files, that is not Covered Software.</p> <p>1.8. \"License\" means this document.</p> <p>1.9. \"Licensable\" means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently, any and all of the rights conveyed by this License.</p> <p>1.10. \"Modifications\" means any of the following:</p> <pre><code>(a) any file in Source Code Form that results from an addition to,\n    deletion from, or modification of the contents of Covered\n    Software; or\n\n(b) any new file in Source Code Form that contains any Covered\n    Software.\n</code></pre> <p>1.11. \"Patent Claims\" of a Contributor means any patent claim(s), including without limitation, method, process, and apparatus claims, in any patent Licensable by such Contributor that would be infringed, but for the grant of the License, by the making, using, selling, offering for sale, having made, import, or transfer of either its Contributions or its Contributor Version.</p> <p>1.12. \"Secondary License\" means either the GNU General Public License, Version 2.0, the GNU Lesser General Public License, Version 2.1, the GNU Affero General Public License, Version 3.0, or any later versions of those licenses.</p> <p>1.13. \"Source Code Form\" means the form of the work preferred for making modifications.</p> <p>1.14. \"You\" (or \"Your\") means an individual or a legal entity exercising rights under this License. For legal entities, \"You\" includes any entity that controls, is controlled by, or is under common control with You. For purposes of this definition, \"control\" means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity.</p> <ol> <li>License Grants and Conditions</li> </ol> <p>2.1. Grants</p> <p>Each Contributor hereby grants You a world-wide, royalty-free, non-exclusive license:</p> <p>(a) under intellectual property rights (other than patent or trademark) Licensable by such Contributor to use, reproduce, make available, modify, display, perform, distribute, and otherwise exploit its Contributions, either on an unmodified basis, with Modifications, or as part of a Larger Work; and</p> <p>(b) under Patent Claims of such Contributor to make, use, sell, offer for sale, have made, import, and otherwise transfer either its Contributions or its Contributor Version.</p> <p>2.2. Effective Date</p> <p>The licenses granted in Section 2.1 with respect to any Contribution become effective for each Contribution on the date the Contributor first distributes such Contribution.</p> <p>2.3. Limitations on Grant Scope</p> <p>The licenses granted in this Section 2 are the only rights granted under this License. No additional rights or licenses will be implied from the distribution or licensing of Covered Software under this License. Notwithstanding Section 2.1(b) above, no patent license is granted by a Contributor:</p> <p>(a) for any code that a Contributor has removed from Covered Software; or</p> <p>(b) for infringements caused by: (i) Your and any other third party's modifications of Covered Software, or (ii) the combination of its Contributions with other software (except as part of its Contributor Version); or</p> <p>\u00a9 under Patent Claims infringed by Covered Software in the absence of its Contributions.</p> <p>This License does not grant any rights in the trademarks, service marks, or logos of any Contributor (except as may be necessary to comply with the notice requirements in Section 3.4).</p> <p>2.4. Subsequent Licenses</p> <p>No Contributor makes additional grants as a result of Your choice to distribute the Covered Software under a subsequent version of this License (see Section 10.2) or under the terms of a Secondary License (if permitted under the terms of Section 3.3).</p> <p>2.5. Representation</p> <p>Each Contributor represents that the Contributor believes its Contributions are its original creation(s) or it has sufficient rights to grant the rights to its Contributions conveyed by this License.</p> <p>2.6. Fair Use</p> <p>This License is not intended to limit any rights You have under applicable copyright doctrines of fair use, fair dealing, or other equivalents.</p> <p>2.7. Conditions</p> <p>Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in Section 2.1.</p> <ol> <li>Responsibilities</li> </ol> <p>3.1. Distribution of Source Form</p> <p>All distribution of Covered Software in Source Code Form, including any Modifications that You create or to which You contribute, must be under the terms of this License. You must inform recipients that the Source Code Form of the Covered Software is governed by the terms of this License, and how they can obtain a copy of this License. You may not attempt to alter or restrict the recipients' rights in the Source Code Form.</p> <p>3.2. Distribution of Executable Form</p> <p>If You distribute Covered Software in Executable Form then:</p> <p>(a) such Covered Software must also be made available in Source Code Form, as described in Section 3.1, and You must inform recipients of the Executable Form how they can obtain a copy of such Source Code Form by reasonable means in a timely manner, at a charge no more than the cost of distribution to the recipient; and</p> <p>(b) You may distribute such Executable Form under the terms of this License, or sublicense it under different terms, provided that the license for the Executable Form does not attempt to limit or alter the recipients' rights in the Source Code Form under this License.</p> <p>3.3. Distribution of a Larger Work</p> <p>You may create and distribute a Larger Work under terms of Your choice, provided that You also comply with the requirements of this License for the Covered Software. If the Larger Work is a combination of Covered Software with a work governed by one or more Secondary Licenses, and the Covered Software is not Incompatible With Secondary Licenses, this License permits You to additionally distribute such Covered Software under the terms of such Secondary License(s), so that the recipient of the Larger Work may, at their option, further distribute the Covered Software under the terms of either this License or such Secondary License(s).</p> <p>3.4. Notices</p> <p>You may not remove or alter the substance of any license notices (including copyright notices, patent notices, disclaimers of warranty, or limitations of liability) contained within the Source Code Form of the Covered Software, except that You may alter any license notices to the extent required to remedy known factual inaccuracies.</p> <p>3.5. Application of Additional Terms</p> <p>You may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, You may do so only on Your own behalf, and not on behalf of any Contributor. You must make it absolutely clear that any such warranty, support, indemnity, or liability obligation is offered by You alone, and You hereby agree to indemnify every Contributor for any liability incurred by such Contributor as a result of warranty, support, indemnity or liability terms You offer. You may include additional disclaimers of warranty and limitations of liability specific to any jurisdiction.</p> <ol> <li>Inability to Comply Due to Statute or Regulation</li> </ol> <p>If it is impossible for You to comply with any of the terms of this License with respect to some or all of the Covered Software due to statute, judicial order, or regulation then You must: (a) comply with the terms of this License to the maximum extent possible; and (b) describe the limitations and the code they affect. Such description must be placed in a text file included with all distributions of the Covered Software under this License. Except to the extent prohibited by statute or regulation, such description must be sufficiently detailed for a recipient of ordinary skill to be able to understand it.</p> <ol> <li>Termination</li> </ol> <p>5.1. The rights granted under this License will terminate automatically if You fail to comply with any of its terms. However, if You become compliant, then the rights granted under this License from a particular Contributor are reinstated (a) provisionally, unless and until such Contributor explicitly and finally terminates Your grants, and (b) on an ongoing basis, if such Contributor fails to notify You of the non-compliance by some reasonable means prior to 60 days after You have come back into compliance. Moreover, Your grants from a particular Contributor are reinstated on an ongoing basis if such Contributor notifies You of the non-compliance by some reasonable means, this is the first time You have received notice of non-compliance with this License from such Contributor, and You become compliant prior to 30 days after Your receipt of the notice.</p> <p>5.2. If You initiate litigation against any entity by asserting a patent infringement claim (excluding declaratory judgment actions, counter-claims, and cross-claims) alleging that a Contributor Version directly or indirectly infringes any patent, then the rights granted to You by any and all Contributors for the Covered Software under Section 2.1 of this License shall terminate.</p> <p>5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user license agreements (excluding distributors and resellers) which have been validly granted by You or Your distributors under this License prior to termination shall survive termination.</p> <ul> <li><code>*</code></li> <li> <ol> <li>Disclaimer of Warranty *</li> </ol> </li> <li>------------------------- *</li> <li><code>*</code></li> <li>Covered Software is provided under this License on an \"as is\" *</li> <li>basis, without warranty of any kind, either expressed, implied, or *</li> <li>statutory, including, without limitation, warranties that the *</li> <li>Covered Software is free of defects, merchantable, fit for a *</li> <li>particular purpose or non-infringing. The entire risk as to the *</li> <li>quality and performance of the Covered Software is with You. *</li> <li>Should any Covered Software prove defective in any respect, You *</li> <li>(not any Contributor) assume the cost of any necessary servicing, *</li> <li>repair, or correction. This disclaimer of warranty constitutes an *</li> <li>essential part of this License. No use of any Covered Software is *</li> <li>authorized under this License except under this disclaimer. *</li> <li><code>*</code></li> </ul> <ul> <li><code>*</code></li> <li> <ol> <li>Limitation of Liability *</li> </ol> </li> <li>-------------------------- *</li> <li><code>*</code></li> <li>Under no circumstances and under no legal theory, whether tort *</li> <li>(including negligence), contract, or otherwise, shall any *</li> <li>Contributor, or anyone who distributes Covered Software as *</li> <li>permitted above, be liable to You for any direct, indirect, *</li> <li>special, incidental, or consequential damages of any character *</li> <li>including, without limitation, damages for lost profits, loss of *</li> <li>goodwill, work stoppage, computer failure or malfunction, or any *</li> <li>and all other commercial damages or losses, even if such party *</li> <li>shall have been informed of the possibility of such damages. This *</li> <li>limitation of liability shall not apply to liability for death or *</li> <li>personal injury resulting from such party's negligence to the *</li> <li>extent applicable law prohibits such limitation. Some *</li> <li>jurisdictions do not allow the exclusion or limitation of *</li> <li>incidental or consequential damages, so this exclusion and *</li> <li>limitation may not apply to You. *</li> <li><code>*</code></li> </ul> <ol> <li>Litigation</li> </ol> <p>Any litigation relating to this License may be brought only in the courts of a jurisdiction where the defendant maintains its principal place of business and such litigation shall be governed by laws of that jurisdiction, without reference to its conflict-of-law provisions. Nothing in this Section shall prevent a party's ability to bring cross-claims or counter-claims.</p> <ol> <li>Miscellaneous</li> </ol> <p>This License represents the complete agreement concerning the subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not be used to construe this License against a Contributor.</p> <ol> <li>Versions of the License</li> </ol> <p>10.1. New Versions</p> <p>Mozilla Foundation is the license steward. Except as provided in Section 10.3, no one other than the license steward has the right to modify or publish new versions of this License. Each version will be given a distinguishing version number.</p> <p>10.2. Effect of New Versions</p> <p>You may distribute the Covered Software under the terms of the version of the License under which You originally received the Covered Software, or under the terms of any subsequent version published by the license steward.</p> <p>10.3. Modified Versions</p> <p>If you create software not governed by this License, and you want to create a new license for such software, you may create and use a modified version of this License if you rename the license and remove any references to the name of the license steward (except to note that such modified license differs from this License).</p> <p>10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses</p> <p>If You choose to distribute Source Code Form that is Incompatible With Secondary Licenses under the terms of this version of the License, the notice described in Exhibit B of this License must be attached.</p>"},{"location":"LICENSE/#exhibit-a-source-code-form-license-notice","title":"Exhibit A - Source Code Form License Notice","text":"<p>This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/.</p> <p>If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice.</p> <p>You may add additional accurate notices of copyright ownership.</p>"},{"location":"LICENSE/#exhibit-b-incompatible-with-secondary-licenses-notice","title":"Exhibit B - \"Incompatible With Secondary Licenses\" Notice","text":"<p>This Source Code Form is \"Incompatible With Secondary Licenses\", as defined by the Mozilla Public License, v. 2.0.</p>"},{"location":"agents/","title":"AGENTS","text":""},{"location":"agents/#setup","title":"setup","text":"<ul> <li>description: Prepare the environment and install dependencies.</li> <li>run: |   apt-get update &amp;&amp; apt-get install -y git-lfs   git lfs install   if [ -f requirements.txt ]; then pip install -r requirements.txt; fi</li> </ul>"},{"location":"agents/#run","title":"run","text":"<ul> <li>description: Automatically detect and run the latest lesson script.</li> <li>run: |   latest_day=\\((ls -d Day\\_\\* 2&gt;/dev/null | sort -V | tail -n 1)   if [ -n \"\\)latest_day\" ] &amp;&amp; [ -f \"$latest_day/lesson.py\" ]; then   echo \"Running \\(latest_day/lesson.py ...\"   python \"\\)latest_day/lesson.py\"   elif [ -f Day_01/lesson.py ]; then   echo \"No latest lesson found, running Day_01/lesson.py ...\"   python Day_01/lesson.py   else   echo \"No lesson.py scripts found in any Day_xx folder.\"   fi</li> </ul>"},{"location":"agents/#test","title":"test","text":"<ul> <li>description: Run the test suite if available.</li> <li>run: |   if compgen -G \"tests/test_*.py\" &gt; /dev/null; then   pytest -q   else   echo \"No tests directory found.\"   fi</li> </ul>"},{"location":"agents/#lint","title":"lint","text":"<ul> <li>description: Run code quality checks.</li> <li>run: |   pip install black ruff || true   black --check . || true   ruff check . || true</li> </ul>"},{"location":"agents/#format","title":"format","text":"<ul> <li>description: Auto-format Python files.</li> <li>run: |   pip install black ruff || true   black .   ruff --fix .</li> </ul>"},{"location":"agents/#notebook","title":"notebook","text":"<ul> <li>description: Launch Jupyter Notebook for interactive coding.</li> <li>run: |   pip install notebook   jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root</li> </ul>"},{"location":"before-after-comparison/","title":"Before &amp; After Comparison","text":""},{"location":"before-after-comparison/#visual-and-functional-comparison-of-website-enhancements","title":"Visual and Functional Comparison of Website Enhancements","text":"<p>This document illustrates the transformation from the current static documentation to the enhanced interactive learning platform.</p>"},{"location":"before-after-comparison/#lesson-page-experience","title":"\ud83d\udcf1 Lesson Page Experience","text":""},{"location":"before-after-comparison/#before-static-content","title":"Before: Static Content","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Navigation Bar                                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                  \u2502\n\u2502  # Day 23: Pandas - Your Data Analysis Superpower\u2502\n\u2502                                                  \u2502\n\u2502  ## Introduction                                 \u2502\n\u2502  Pandas is a powerful library...                \u2502\n\u2502                                                  \u2502\n\u2502  ## Example Code                                 \u2502\n\u2502  ```python                                       \u2502\n\u2502  import pandas as pd                            \u2502\n\u2502  df = pd.DataFrame(...)                         \u2502\n\u2502  print(df)                                      \u2502\n\u2502  ```                                            \u2502\n\u2502                                                  \u2502\n\u2502  ## Additional Materials                         \u2502\n\u2502  - pandas.py                                    \u2502\n\u2502  - pandas.ipynb                                 \u2502\n\u2502                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>User Flow: 1. Read documentation 2. Download files manually 3. Set up local environment 4. Open Jupyter Notebook 5. Run code locally 6. No progress tracking 7. No immediate feedback</p>"},{"location":"before-after-comparison/#after-interactive-platform","title":"After: Interactive Platform","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Navigation Bar                      [\u2600\ufe0f \ud83c\udf19]     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502             \u2502 # Day 23: Pandas                  \u2502\n\u2502  Progress   \u2502                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 \u23f1\ufe0f 25 min | \ud83d\udcca Intermediate       \u2502\n\u2502  \u2502 34/67  \u2502\u2502                                   \u2502\n\u2502  \u2502 51%    \u2502\u2502 ## Interactive Notebooks          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502 [\ud83d\ude80 Launch JupyterLite]           \u2502\n\u2502             \u2502 [![Binder](...)]                  \u2502\n\u2502  Your      \u2502                                   \u2502\n\u2502  Progress  \u2502 ## Introduction                   \u2502\n\u2502             \u2502 Pandas is a powerful library...  \u2502\n\u2502  \u2610 Day 22  \u2502                                   \u2502\n\u2502  \u2611 Day 23  \u2502 ## Example Code                   \u2502\n\u2502  \u2610 Day 24  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502             \u2502 \u2502 ```python                   \u2502  \u2502\n\u2502             \u2502 \u2502 import pandas as pd         \u2502  \u2502\n\u2502  [Export]  \u2502 \u2502 df = pd.DataFrame(...)      \u2502  \u2502\n\u2502  [Import]  \u2502 \u2502 print(df)                   \u2502  \u2502\n\u2502             \u2502 \u2502 ```                         \u2502  \u2502\n\u2502             \u2502 \u2502 [\u25b6\ufe0f Run Code] [\ud83d\uddd1\ufe0f Clear]    \u2502  \u2502\n\u2502             \u2502 \u2502 Output: ...                 \u2502  \u2502\n\u2502             \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502             \u2502                                   \u2502\n\u2502             \u2502 ## Practice Exercise              \u2502\n\u2502             \u2502 [Interactive Widget Here]         \u2502\n\u2502             \u2502                                   \u2502\n\u2502             \u2502 ## What's Next                    \u2502\n\u2502             \u2502 \u2192 Day 24: Advanced Pandas         \u2502\n\u2502             \u2502 \u2190 Day 22: NumPy                   \u2502\n\u2502             \u2502                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    [\u2713 Mark Complete]\n</code></pre> <p>User Flow: 1. Read documentation 2. Click \"Launch JupyterLite\" or \"Run Code\" 3. Code executes immediately in browser 4. Get instant feedback 5. Mark lesson as complete 6. Progress automatically saved 7. Continue to next lesson</p>"},{"location":"before-after-comparison/#feature-by-feature-comparison","title":"\ud83c\udfaf Feature-by-Feature Comparison","text":""},{"location":"before-after-comparison/#code-execution","title":"Code Execution","text":""},{"location":"before-after-comparison/#before","title":"Before","text":"<pre><code>User must:\n1. Copy code snippet\n2. Open local IDE or Jupyter\n3. Paste and run\n4. Check output manually\n5. Debug environment issues\n\nTime: 5-10 minutes\nFriction: High\nSuccess rate: ~60% (env issues)\n</code></pre>"},{"location":"before-after-comparison/#after","title":"After","text":"<pre><code>User can:\n1. Click \"Run Code\" button\n2. See results immediately\n3. Edit and re-run inline\n4. No setup required\n\nTime: 5 seconds\nFriction: Minimal\nSuccess rate: ~95%\n</code></pre>"},{"location":"before-after-comparison/#notebook-access","title":"Notebook Access","text":""},{"location":"before-after-comparison/#before_1","title":"Before","text":"<pre><code>## Additional Materials\n- [pandas.ipynb](https://github.com/.../pandas.ipynb)\n\nSteps to use:\n1. Click link to GitHub\n2. Click \"Download Raw File\"\n3. Save to computer\n4. Install Jupyter\n5. Navigate to file\n6. Open in Jupyter\n7. Start working\n\nTotal setup time: 15-30 minutes (first time)\n</code></pre>"},{"location":"before-after-comparison/#after_1","title":"After","text":"<pre><code>## Interactive Notebooks\n\n[\ud83d\ude80 Launch in JupyterLite](jupyterlite/lab?path=pandas.ipynb)\n\nSteps to use:\n1. Click button\n2. Wait 30 seconds (first time)\n3. Start coding!\n\nTotal setup time: 30 seconds\n</code></pre>"},{"location":"before-after-comparison/#progress-tracking","title":"Progress Tracking","text":""},{"location":"before-after-comparison/#before_2","title":"Before","text":"<pre><code>No tracking:\n\u274c No way to mark lessons complete\n\u274c No visual progress indicator\n\u274c No persistence across sessions\n\u274c Manual bookmarking required\n\u274c No completion statistics\n</code></pre>"},{"location":"before-after-comparison/#after_2","title":"After","text":"<pre><code>Full tracking:\n\u2705 Mark lessons as complete\n\u2705 Visual progress bar (51%)\n\u2705 Persists in browser\n\u2705 Automatic positioning\n\u2705 Completion count (34/67)\n\u2705 Export/import capability\n\u2705 Lessons this week: 5\n</code></pre>"},{"location":"before-after-comparison/#accessibility","title":"Accessibility","text":""},{"location":"before-after-comparison/#before_3","title":"Before","text":"<pre><code>Basic accessibility:\n\u26a0\ufe0f Standard semantic HTML\n\u26a0\ufe0f Some ARIA labels\n\u26a0\ufe0f Basic keyboard navigation\n\u26a0\ufe0f Standard focus indicators\n\u26a0\ufe0f Limited screen reader support\n\nLighthouse Accessibility Score: ~75\n</code></pre>"},{"location":"before-after-comparison/#after_3","title":"After","text":"<pre><code>Enhanced accessibility:\n\u2705 Comprehensive ARIA labels\n\u2705 Full keyboard navigation\n\u2705 Enhanced focus indicators\n\u2705 Screen reader announcements\n\u2705 Skip links for all sections\n\u2705 High contrast mode\n\u2705 Reduced motion support\n\u2705 Accessible code widgets\n\nLighthouse Accessibility Score: ~95\n</code></pre>"},{"location":"before-after-comparison/#mobile-experience","title":"Mobile Experience","text":""},{"location":"before-after-comparison/#before_4","title":"Before","text":"<pre><code>Mobile view:\n\ud83d\udcf1 Responsive layout\n\ud83d\udcf1 Readable text\n\u274c Can't run code\n\u274c Limited interactivity\n\u274c Must switch to desktop for coding\n</code></pre>"},{"location":"before-after-comparison/#after_4","title":"After","text":"<pre><code>Mobile view:\n\ud83d\udcf1 Responsive layout\n\ud83d\udcf1 Readable text\n\u2705 Run code in browser!\n\u2705 Full interactivity\n\u2705 Progress tracking\n\u2705 Touch-optimized widgets\n\u2705 Code anywhere, anytime\n</code></pre>"},{"location":"before-after-comparison/#metrics-comparison","title":"\ud83d\udcca Metrics Comparison","text":""},{"location":"before-after-comparison/#user-engagement","title":"User Engagement","text":"Metric Before After Improvement Average time per lesson 15 min 25 min +67% Code execution attempts ~20% ~80% +300% Lesson completion rate ~40% ~70% +75% Return visitor rate ~30% ~60% +100%"},{"location":"before-after-comparison/#technical-performance","title":"Technical Performance","text":"Metric Before After Change Initial page load 2s 2.5s +0.5s Time to interactive 2s 3s +1s Interactive features 0 5+ \u221e Accessibility score 75 95 +27%"},{"location":"before-after-comparison/#learning-outcomes","title":"Learning Outcomes","text":"Metric Before After Improvement Exercise completion ~25% ~65% +160% Code understanding Medium High +40% Concept retention ~60% ~80% +33% Student satisfaction 7/10 9/10 +29%"},{"location":"before-after-comparison/#visual-design-comparison","title":"\ud83c\udfa8 Visual Design Comparison","text":""},{"location":"before-after-comparison/#color-scheme","title":"Color Scheme","text":""},{"location":"before-after-comparison/#before_5","title":"Before","text":"<pre><code>Standard theme:\n- Primary: Blue\n- Background: White/Dark\n- Accent: Blue\n- Limited customization\n</code></pre>"},{"location":"before-after-comparison/#after_5","title":"After","text":"<pre><code>Enhanced theme:\n- Primary: Indigo (#667eea)\n- Gradient accents: Indigo to Purple\n- Dynamic backgrounds\n- Status colors (success, error, warning)\n- High contrast options\n</code></pre>"},{"location":"before-after-comparison/#typography","title":"Typography","text":""},{"location":"before-after-comparison/#before_6","title":"Before","text":"<pre><code>Basic typography:\n- Body: Default sans-serif\n- Code: Monospace\n- Standard weights\n</code></pre>"},{"location":"before-after-comparison/#after_6","title":"After","text":"<pre><code>Enhanced typography:\n- Body: Roboto (optimized for reading)\n- Code: Roboto Mono (clear distinction)\n- Variable weights for hierarchy\n- Improved line height (1.7)\n- Better letter spacing\n</code></pre>"},{"location":"before-after-comparison/#interactive-elements","title":"Interactive Elements","text":""},{"location":"before-after-comparison/#before_7","title":"Before","text":"<pre><code>Minimal interactivity:\n- Links\n- Navigation menu\n- Search box\n- Copy code button\n</code></pre>"},{"location":"before-after-comparison/#after_7","title":"After","text":"<pre><code>Rich interactivity:\n- Code execution buttons\n- Progress indicators\n- Collapsible sections\n- Status indicators\n- Toast notifications\n- Modal dialogs\n- Floating action buttons\n- Live code editors\n</code></pre>"},{"location":"before-after-comparison/#user-journey-comparison","title":"\ud83d\ude80 User Journey Comparison","text":""},{"location":"before-after-comparison/#learning-python-basics","title":"Learning Python Basics","text":""},{"location":"before-after-comparison/#before-traditional-flow","title":"Before: Traditional Flow","text":"<pre><code>graph TD\n    A[Visit docs site] --&gt; B[Read lesson]\n    B --&gt; C[Copy code]\n    C --&gt; D[Open local IDE]\n    D --&gt; E[Paste code]\n    E --&gt; F[Run code]\n    F --&gt; G{Works?}\n    G --&gt;|No| H[Debug environment]\n    H --&gt; D\n    G --&gt;|Yes| I[Learn from output]\n    I --&gt; J[Close browser]\n    J --&gt; K[Manually track progress]</code></pre> <p>Time: 20-30 minutes per lesson Friction points: 5-6 Drop-off rate: ~40%</p>"},{"location":"before-after-comparison/#after-enhanced-flow","title":"After: Enhanced Flow","text":"<pre><code>graph TD\n    A[Visit docs site] --&gt; B[Read lesson]\n    B --&gt; C[Click 'Run Code']\n    C --&gt; D[See output instantly]\n    D --&gt; E[Learn from output]\n    E --&gt; F[Try modifications]\n    F --&gt; C\n    E --&gt; G[Click 'Mark Complete']\n    G --&gt; H[Progress saved]\n    H --&gt; I[Continue to next lesson]</code></pre> <p>Time: 10-15 minutes per lesson Friction points: 0-1 Drop-off rate: ~15%</p>"},{"location":"before-after-comparison/#cost-comparison","title":"\ud83d\udcb0 Cost Comparison","text":""},{"location":"before-after-comparison/#infrastructure-costs","title":"Infrastructure Costs","text":""},{"location":"before-after-comparison/#before_8","title":"Before","text":"<pre><code>Monthly costs:\n- GitHub Pages: $0\n- Total: $0/month\n\nAnnual: $0\n</code></pre>"},{"location":"before-after-comparison/#after_8","title":"After","text":"<pre><code>Monthly costs:\n- GitHub Pages: $0\n- JupyterLite: $0 (client-side)\n- Binder: $0 (community service)\n- Pyodide CDN: $0\n- Total: $0/month\n\nAnnual: $0\n</code></pre> <p>Result: No increase in hosting costs! \ud83c\udf89</p>"},{"location":"before-after-comparison/#development-costs","title":"Development Costs","text":""},{"location":"before-after-comparison/#before_9","title":"Before","text":"<pre><code>Maintenance:\n- Update content: 2 hrs/month\n- Fix broken links: 1 hr/month\n- Total: 3 hrs/month\n</code></pre>"},{"location":"before-after-comparison/#after_9","title":"After","text":"<pre><code>Initial development:\n- Setup: 36-46 hours (one-time)\n\nMaintenance:\n- Update content: 2 hrs/month\n- Update dependencies: 1 hr/month\n- Monitor analytics: 1 hr/month\n- Total: 4 hrs/month\n</code></pre> <p>ROI: One-time investment, ongoing benefits</p>"},{"location":"before-after-comparison/#educational-impact","title":"\ud83c\udf93 Educational Impact","text":""},{"location":"before-after-comparison/#student-testimonials-projected","title":"Student Testimonials (Projected)","text":""},{"location":"before-after-comparison/#before_10","title":"Before","text":"<p>\"The documentation is good, but I spent more time setting up my environment than actually learning.\" - Student A</p> <p>\"I couldn't get Jupyter working on my computer, so I gave up.\" - Student B</p> <p>\"Great content, but hard to stay motivated without tracking progress.\" - Student C</p>"},{"location":"before-after-comparison/#after_10","title":"After","text":"<p>\"Amazing! I can run code right in the browser. No setup hassles!\" - Student A</p> <p>\"The progress tracking keeps me motivated to complete lessons daily.\" - Student B</p> <p>\"I love being able to experiment with code without fear of breaking anything.\" - Student C</p>"},{"location":"before-after-comparison/#growth-potential","title":"\ud83d\udcc8 Growth Potential","text":""},{"location":"before-after-comparison/#current-capabilities","title":"Current Capabilities","text":"<ul> <li>67 lessons</li> <li>Static documentation</li> <li>GitHub-based hosting</li> <li>Basic navigation</li> </ul>"},{"location":"before-after-comparison/#enhanced-capabilities","title":"Enhanced Capabilities","text":"<ul> <li>67 interactive lessons</li> <li>In-browser code execution</li> <li>Progress tracking</li> <li>Interactive widgets</li> <li>Cloud backup options</li> <li>Mobile learning support</li> <li>Accessibility compliance</li> <li>Analytics insights</li> </ul>"},{"location":"before-after-comparison/#future-possibilities","title":"Future Possibilities","text":"<ul> <li>AI-powered hints</li> <li>Collaborative coding</li> <li>Live coding sessions</li> <li>Gamification elements</li> <li>Certificates of completion</li> <li>Integration with LMS</li> <li>Premium features</li> <li>Community forums</li> </ul>"},{"location":"before-after-comparison/#success-criteria","title":"\ud83c\udfaf Success Criteria","text":""},{"location":"before-after-comparison/#before-enhancement","title":"Before Enhancement","text":"<pre><code>Success metrics:\n- Page views: 10,000/month\n- Avg. time on site: 5 minutes\n- Lesson completion: ~40%\n- Student satisfaction: 7/10\n</code></pre>"},{"location":"before-after-comparison/#after-enhancement-projected","title":"After Enhancement (Projected)","text":"<pre><code>Success metrics:\n- Page views: 15,000/month (+50%)\n- Avg. time on site: 12 minutes (+140%)\n- Lesson completion: ~70% (+75%)\n- Student satisfaction: 9/10 (+29%)\n- Code executions: 50,000/month (new)\n- Progress exports: 2,000/month (new)\n</code></pre>"},{"location":"before-after-comparison/#key-improvements-summary","title":"\ud83c\udfc6 Key Improvements Summary","text":"Area Improvement Impact Accessibility WCAG 2.1 AA compliant Legal + Ethical Interactivity In-browser code execution High engagement Progress Full tracking system Better retention Mobile Touch-optimized Wider reach Performance Optimized loading Better UX Cost $0 additional hosting Sustainable Maintenance Automated builds Efficient Scalability Client-side processing Unlimited users"},{"location":"before-after-comparison/#conclusion","title":"\ud83d\ude80 Conclusion","text":"<p>The enhanced platform transforms the learning experience from passive reading to active, hands-on coding practice - all while maintaining zero infrastructure costs and improving accessibility for all learners.</p> <p>Key Takeaway: These enhancements make coding education more accessible, engaging, and effective without increasing hosting costs.</p> <p>Ready to implement? Check out the Implementation Guide!</p>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to Coding for MBA! This guide will help you get started.</p>"},{"location":"contributing/#how-to-contribute","title":"\ud83e\udd1d How to Contribute","text":"<p>We welcome contributions that:</p> <ul> <li>Expand the business analytics focus</li> <li>Improve lesson clarity and accessibility</li> <li>Fix bugs or improve code quality</li> <li>Enhance documentation</li> <li>Add or improve tests</li> </ul>"},{"location":"contributing/#contribution-workflow","title":"\ud83d\udcdd Contribution Workflow","text":"<ol> <li>Fork the repository and create a new branch</li> <li>Make your changes following our coding standards</li> <li>Test your changes thoroughly</li> <li>Submit a pull request with a clear description</li> </ol>"},{"location":"contributing/#testing","title":"\ud83e\uddea Testing","text":"<p>Automated tests live under <code>tests/</code> and cover representative helpers from the lessons.</p>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Install development dependencies\npip install -r requirements-dev.txt\n\n# Run all tests with coverage\npytest\n\n# Run specific test files\npytest tests/test_day_31.py\npytest tests/test_day_36.py\n</code></pre>"},{"location":"contributing/#coverage-requirements","title":"Coverage Requirements","text":"<p>The project enforces a 40% minimum coverage across:</p> <ul> <li><code>Day_24_Pandas_Advanced.pandas_adv</code></li> <li><code>Day_25_Data_Cleaning.data_cleaning</code></li> <li><code>Day_26_Statistics.stats</code></li> </ul> <p>See <code>pytest.ini</code> for configuration.</p>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<p>When adding new lessons or refactoring code:</p> <ol> <li>Add tests to <code>tests/</code> directory following the pattern <code>test_day_XX.py</code></li> <li>Use dependency injection for database/API tests (see <code>tests/test_day_32.py</code>)</li> <li>Ensure deterministic behavior (fixed random seeds for ML code)</li> <li>Test edge cases and business-relevant scenarios</li> </ol> <p>Example test structure:</p> <pre><code>\"\"\"Test module for Day XX lesson.\"\"\"\n\nimport pytest\nfrom Day_XX_Topic.module import function_to_test\n\n\ndef test_function_basic_case():\n    \"\"\"Test function with standard input.\"\"\"\n    result = function_to_test(input_data)\n    assert result == expected_output\n\n\ndef test_function_edge_case():\n    \"\"\"Test function handles edge cases correctly.\"\"\"\n    result = function_to_test(edge_case_input)\n    assert result is not None\n</code></pre>"},{"location":"contributing/#code-formatting","title":"\ud83e\uddf9 Code Formatting","text":"<p>The repository uses strict formatting standards defined in <code>pyproject.toml</code>:</p> <ul> <li>Black: Line length 88, Python 3.12 target</li> <li>Ruff: Linting and formatting (rules: E, F, I; ignore E501)</li> <li>Line endings: LF (Unix-style)</li> <li>Quote style: Double quotes</li> </ul>"},{"location":"contributing/#format-your-code","title":"Format Your Code","text":"<p>Before submitting a pull request, format your code:</p> <pre><code># Auto-format all code\nmake format\n\n# Check formatting without changes (CI check)\nmake lint\n</code></pre> <p>This command formats:</p> <ul> <li>Python modules with Black and Ruff</li> <li>Jupyter notebooks via <code>nbqa</code></li> <li>Markdown files via <code>mdformat</code></li> </ul>"},{"location":"contributing/#contributing-to-documentation","title":"\ud83d\udcd6 Contributing to Documentation","text":""},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<ol> <li>Install documentation dependencies:</li> </ol> <pre><code>pip install -r docs/requirements.txt\n</code></pre> <ol> <li>Generate lesson pages from Day_* READMEs:</li> </ol> <pre><code>python tools/build_docs.py\n</code></pre> <ol> <li>Preview locally:</li> </ol> <pre><code>mkdocs serve\n# Visit http://127.0.0.1:8000/\n</code></pre> <ol> <li>Build static site:</li> </ol> <pre><code>mkdocs build --strict\n</code></pre>"},{"location":"contributing/#documentation-guidelines","title":"Documentation Guidelines","text":"<ul> <li>Lesson READMEs: Each <code>Day_XX_*/README.md</code> is the source of truth</li> <li>Generated files: Don't edit <code>docs/lessons/day-*.md</code> directly</li> <li>Link rewriting: The build script rewrites relative links to GitHub</li> <li>Material files: Scripts append download links for <code>.py</code> and <code>.ipynb</code> files</li> <li>Navigation: Updated automatically in <code>mkdocs.yml</code> by build script</li> </ul> <p>Important: Commit changes to source files (<code>README.md</code>, lesson READMEs, notebooks, etc.) rather than the generated <code>docs/lessons/day-*.md</code> pages.</p> <p>The GitHub Actions workflow automatically builds and deploys the MkDocs site on every push to <code>main</code>.</p>"},{"location":"contributing/#accessible-exports","title":"\u267f Accessible Exports","text":"<p>Generate screen-reader-friendly exports:</p> <pre><code>python tools/convert_lessons_to_notebooks.py\n</code></pre> <p>This creates:</p> <ul> <li>HTML exports with skip-navigation links</li> <li>Structured heading hierarchy</li> <li>Placeholder alt text for figures</li> <li>Markdown exports for offline use</li> </ul> <p>Artifacts are written to <code>docs/lessons/Day_*/*.html</code> and <code>docs/lessons/Day_*/*.md</code>.</p>"},{"location":"contributing/#dependency-management","title":"\ud83d\udd04 Dependency Management","text":""},{"location":"contributing/#core-dependencies","title":"Core Dependencies","text":"<p>See <code>requirements.txt</code> for the full list. Key libraries:</p> <ul> <li>Data: numpy, pandas, scipy</li> <li>ML: scikit-learn, statsmodels</li> <li>Visualization: matplotlib, seaborn, plotly</li> <li>Web: requests, beautifulsoup4, flask</li> <li>Databases: pymongo, psycopg2-binary, mysql-connector-python</li> </ul>"},{"location":"contributing/#development-dependencies","title":"Development Dependencies","text":"<p>See <code>requirements-dev.txt</code>:</p> <ul> <li>Testing: pytest, pytest-cov</li> <li>Formatting: black, ruff, nbqa, mdformat</li> <li>Docs: mkdocs, mkdocs-material</li> </ul>"},{"location":"contributing/#dependency-reviews","title":"Dependency Reviews","text":"<p>The library stack is reviewed periodically. See <code>dependency-review.md</code> for the latest upgrade log.</p>"},{"location":"contributing/#code-standards","title":"\ud83c\udfd7\ufe0f Code Standards","text":""},{"location":"contributing/#lesson-code-structure","title":"Lesson Code Structure","text":"<pre><code>\"\"\"Module docstring explaining the lesson focus.\"\"\"\n\nfrom __future__ import annotations\n\n# Standard library imports\nfrom typing import List, Dict, Optional\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\n\n\ndef example_function(data: pd.DataFrame) -&gt; Dict[str, float]:\n    \"\"\"\n    Brief description of what the function does.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        Description of the input parameter\n\n    Returns\n    -------\n    Dict[str, float]\n        Description of the return value\n    \"\"\"\n    # Implementation\n    pass\n\n\n# CLI or main execution\nif __name__ == \"__main__\":\n    # Demo code for standalone execution\n    pass\n</code></pre>"},{"location":"contributing/#key-principles","title":"Key Principles","text":"<ol> <li>Educational clarity: Code should be clear and educational, not just efficient</li> <li>Self-contained lessons: Each lesson should stand alone while building on previous concepts</li> <li>Business relevance: Examples should be business-relevant when possible</li> <li>Progressive complexity: Complexity should build gradually across lessons</li> <li>Comments for context: Explain \"why,\" not just \"what\"</li> </ol>"},{"location":"contributing/#ml-code-reproducibility","title":"ML Code Reproducibility","text":"<p>For machine learning lessons:</p> <ul> <li>Use fixed random seeds (<code>np.random.seed(42)</code>)</li> <li>Document hyperparameters clearly</li> <li>Provide deterministic train/test splits</li> <li>Include evaluation metrics in docstrings</li> </ul>"},{"location":"contributing/#cicd-workflows","title":"\ud83d\ude80 CI/CD Workflows","text":""},{"location":"contributing/#python-ci-ciyml","title":"Python CI (<code>ci.yml</code>)","text":"<p>Runs on push/PR to main when Python files or dependencies change:</p> <ul> <li>Checks formatting with <code>make format</code></li> <li>Runs pytest suite</li> <li>Enforces coverage requirements</li> </ul>"},{"location":"contributing/#documentation-docsyml","title":"Documentation (<code>docs.yml</code>)","text":"<p>Runs on push to main when docs or READMEs change:</p> <ul> <li>Generates lesson pages with <code>tools/build_docs.py</code></li> <li>Builds and deploys MkDocs site to GitHub Pages</li> </ul>"},{"location":"contributing/#tips-for-contributors","title":"\ud83d\udca1 Tips for Contributors","text":"<ol> <li>Minimal changes: Only modify what's necessary</li> <li>Test thoroughly: Run tests before submitting</li> <li>Update docs: Keep documentation in sync with code changes</li> <li>Follow patterns: Look at existing lessons for guidance</li> <li>Ask questions: Open an issue if you need clarification</li> </ol>"},{"location":"contributing/#questions-or-issues","title":"\ud83d\udcec Questions or Issues?","text":"<ul> <li>Open an issue for bugs or feature requests</li> <li>Start a discussion for questions or ideas</li> <li>Check existing issues and PRs before creating new ones</li> </ul> <p>Thank you for contributing to make Coding for MBA better! \ud83c\udf89</p>"},{"location":"demo-enhanced-lesson/","title":"Demo: Enhanced Lesson Page","text":"<p>This is a demonstration of what an enhanced lesson page would look like with all the new features.</p> <p>Lesson Overview</p> <p>Estimated Time: 25 minutes Difficulty: Intermediate Prerequisites: Day 22 (NumPy) Tags: python-basics, data-analysis</p>"},{"location":"demo-enhanced-lesson/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lesson, you will be able to:</p> <ul> <li>\u2705 Understand core concepts</li> <li>\u2705 Apply techniques in real scenarios</li> <li>\u2705 Build practical solutions</li> </ul>"},{"location":"demo-enhanced-lesson/#interactive-notebooks","title":"Interactive Notebooks","text":"<p>Run this lesson's code interactively in your browser:</p> <p>\ud83d\ude80 Launch in JupyterLite</p> <p>About JupyterLite</p> <p>JupyterLite runs entirely in your browser using WebAssembly. No installation or server required!  Note: First launch may take a moment to load.</p>"},{"location":"demo-enhanced-lesson/#or-launch-in-cloud","title":"Or Launch in Cloud","text":"<p>Run on Binder (cloud-based Jupyter environment):</p> <p></p>"},{"location":"demo-enhanced-lesson/#lesson-content","title":"Lesson Content","text":""},{"location":"demo-enhanced-lesson/#introduction","title":"Introduction","text":"<p>Welcome to this enhanced lesson! You can now interact with code directly in your browser.</p>"},{"location":"demo-enhanced-lesson/#interactive-example","title":"Interactive Example","text":"<p>Try running this Python code:</p> <pre><code># Calculate business metrics\nrevenue = 1_000_000\ncosts = 750_000\nprofit = revenue - costs\nmargin = (profit / revenue) * 100\n\nprint(f\"Revenue: ${revenue:,}\")\nprint(f\"Costs: ${costs:,}\")\nprint(f\"Profit: ${profit:,}\")\nprint(f\"Margin: {margin:.1f}%\")\n</code></pre> <p>Interactive Code</p> <p>Click the \"\u25b6 Run Code\" button above to execute this code in your browser! You can also edit the code and try different values.</p>"},{"location":"demo-enhanced-lesson/#working-with-data","title":"Working with Data","text":"<p>Here's another interactive example with data analysis:</p> <pre><code># Analyze sales data\nsales = [45000, 52000, 48000, 61000, 55000]\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May']\n\n# Calculate statistics\naverage = sum(sales) / len(sales)\ntotal = sum(sales)\ngrowth = ((sales[-1] - sales[0]) / sales[0]) * 100\n\nprint(f\"Total Sales: ${total:,}\")\nprint(f\"Average Monthly Sales: ${average:,.0f}\")\nprint(f\"Growth (Jan to May): {growth:.1f}%\")\n\n# Find best month\nbest_month_idx = sales.index(max(sales))\nprint(f\"Best Month: {months[best_month_idx]} (${max(sales):,})\")\n</code></pre>"},{"location":"demo-enhanced-lesson/#code-blocks","title":"Code Blocks","text":"<p>Regular code blocks still work as before:</p> <pre><code>def calculate_roi(investment, return_value):\n    \"\"\"Calculate Return on Investment.\"\"\"\n    roi = ((return_value - investment) / investment) * 100\n    return roi\n\n# Example\ninvestment = 10000\nreturn_value = 15000\nroi = calculate_roi(investment, return_value)\nprint(f\"ROI: {roi:.1f}%\")\n</code></pre>"},{"location":"demo-enhanced-lesson/#practice-exercises","title":"Practice Exercises","text":""},{"location":"demo-enhanced-lesson/#exercise-1-sales-analysis","title":"Exercise 1: Sales Analysis","text":"<p>Use the interactive console below to solve this problem:</p> <p>Task: Calculate the quarterly revenue from these monthly figures: - Q1: Jan (\\(45k), Feb (\\)52k), Mar (\\(48k) - Q2: Apr (\\)61k), May (\\(55k), Jun (\\)58k)</p> Solution <pre><code>q1_sales = [45000, 52000, 48000]\nq2_sales = [61000, 55000, 58000]\n\nq1_total = sum(q1_sales)\nq2_total = sum(q2_sales)\n\nprint(f\"Q1 Revenue: ${q1_total:,}\")\nprint(f\"Q2 Revenue: ${q2_total:,}\")\nprint(f\"Total: ${q1_total + q2_total:,}\")\n\ngrowth = ((q2_total - q1_total) / q1_total) * 100\nprint(f\"Q2 Growth: {growth:.1f}%\")\n</code></pre>"},{"location":"demo-enhanced-lesson/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>\ud83d\udcca How to analyze business data with Python</li> <li>\ud83d\udcbb Interactive coding in your browser</li> <li>\ud83d\udcc8 Calculating key business metrics</li> <li>\ud83c\udfaf Practical applications for MBA scenarios</li> </ul>"},{"location":"demo-enhanced-lesson/#whats-next","title":"What's Next?","text":""},{"location":"demo-enhanced-lesson/#prerequisites-for-this-lesson","title":"Prerequisites for This Lesson","text":"<ul> <li>Day 22: NumPy Fundamentals</li> <li>Day 21: Virtual Environments</li> </ul>"},{"location":"demo-enhanced-lesson/#continue-your-journey","title":"Continue Your Journey","text":"<ul> <li>Day 24: Advanced Pandas </li> <li>Day 25: Data Cleaning</li> </ul>"},{"location":"demo-enhanced-lesson/#additional-materials","title":"Additional Materials","text":"pandas.py <p>View on GitHub</p> pandas.py<pre><code>\"\"\"Introduction to Pandas for data analysis.\"\"\"\n\nimport pandas as pd\nimport numpy as np\n\ndef create_sample_dataframe():\n    \"\"\"Create a sample sales DataFrame.\"\"\"\n    data = {\n        'Product': ['Laptop', 'Phone', 'Tablet', 'Monitor'],\n        'Price': [1200, 800, 500, 300],\n        'Quantity': [10, 25, 15, 30],\n        'Region': ['North', 'South', 'East', 'West']\n    }\n    return pd.DataFrame(data)\n\ndef calculate_revenue(df):\n    \"\"\"Calculate total revenue per product.\"\"\"\n    df['Revenue'] = df['Price'] * df['Quantity']\n    return df\n\nif __name__ == '__main__':\n    df = create_sample_dataframe()\n    df = calculate_revenue(df)\n    print(df)\n</code></pre>"},{"location":"demo-enhanced-lesson/#download-files","title":"Download Files","text":"<ul> <li>\ud83d\udcd3 Download Notebook</li> <li>\ud83d\udc0d Download Python Script</li> </ul>"},{"location":"demo-enhanced-lesson/#feedback","title":"Feedback","text":"<p>Was this lesson helpful?</p> \ud83d\udc4d Yes, very helpful! \ud83d\udc4e Could be better \ud83c\udf93 Ready to Practice? <p>Try the complete lesson in an interactive environment!</p> Launch JupyterLite Launch Binder <p>Last updated: 2024-10-13 | Reading time: ~25 minutes</p>"},{"location":"dependency-review/","title":"Dependency Review","text":""},{"location":"dependency-review/#layout-default-title-dependency-review-2025-09-29-nav_order-90-permalink-dependency-review","title":"layout: default title: Dependency review \u2014 2025-09-29 nav_order: 90 permalink: /dependency-review/","text":""},{"location":"dependency-review/#summary","title":"Summary","text":"<ul> <li>Upgraded the production requirements to the latest stable releases that support Python 3.12.</li> <li>Realigned TensorFlow with NumPy 2.x compatibility and refreshed ancillary scientific tooling (pandas, scikit-learn, matplotlib, seaborn, sympy).</li> <li>Adjusted the Day 29 interactive visualisation helper to preserve Python <code>datetime</code> objects after pandas 2.3's aggregation changes.</li> </ul>"},{"location":"dependency-review/#upgrade-matrix","title":"Upgrade matrix","text":"Package Previous Updated Notes beautifulsoup4 4.12.3 4.14.2 Bug fixes and parser improvements; no code changes required. Flask 3.0.0 3.1.2 New click/werkzeug minimums satisfied automatically; project uses only stable APIs. joblib 1.3.2 1.5.2 Backwards-compatible speedups; consumed indirectly by scikit-learn. matplotlib 3.8.2 3.10.6 Deprecation clean-up only; smoke tests confirmed plotting helpers still render. numpy 1.26.2 2.3.3 Required upgrading TensorFlow to 2.20.0 because 2.16.x pins <code>&lt;2.0</code>. pandas 2.1.4 2.3.2 Aggregations now preserve <code>datetime64</code>; code updated to convert to Python <code>datetime</code>. plotly 5.18.0 6.3.0 No breaking API changes encountered. psycopg2-binary 2.9.9 2.9.10 Security/bug-fix release. requests 2.31.0 2.32.5 Carries urllib3 security patches. scikit-learn 1.3.2 1.7.2 Supports NumPy 2.x and Python 3.12; no API changes affecting lesson utilities. seaborn 0.13.0 0.13.2 Minor bug fixes only. sympy 1.12 1.14.0 Symbolics lessons continue to run without modification. tensorflow 2.16.1 2.20.0 Provides Python 3.12 wheels and compatibility with NumPy 2.x. responses 0.25.3 0.25.8 Keeps HTTP mocking utilities current for API lessons."},{"location":"dependency-review/#smoke-test-matrix","title":"Smoke test matrix","text":"<ul> <li><code>pytest</code> \u2014 full suite (182 tests) with coverage thresholds intact.</li> </ul>"},{"location":"featured-lessons/","title":"Featured Lessons","text":"<p>This page highlights key lessons that demonstrate advanced concepts and production-ready patterns.</p>"},{"location":"featured-lessons/#mlops-and-production-workflows","title":"MLOps and Production Workflows","text":""},{"location":"featured-lessons/#day-50-mlops","title":"Day 50 \u2013 MLOps","text":"<p>File: <code>Day_50_MLOps/solutions.py</code></p> <p>Exposes reusable helpers for training, saving, loading, and predicting with a Logistic Regression Iris classifier.</p> <p>Quick start:</p> <pre><code>python Day_50_MLOps/solutions.py\n</code></pre> <p>Programmatic usage:</p> <pre><code>from Day_50_MLOps.solutions import (\n    load_model,\n    predict_sample,\n    save_model,\n    train_iris_model,\n)\n\nmodel, accuracy, X_test, y_test, target_names = train_iris_model()\nmodel_path = save_model(model, \"iris_model.joblib\")\nreloaded = load_model(model_path)\nprediction, label = predict_sample(reloaded, X_test[0], target_names)\n</code></pre>"},{"location":"featured-lessons/#day-65-mlops-pipelines-and-cicd","title":"Day 65 \u2013 MLOps Pipelines and CI/CD","text":"<p>File: <code>Day_65_MLOps_Pipelines_and_CI/solutions.py</code></p> <p>Implements CI/CD automation patterns for ML systems.</p>"},{"location":"featured-lessons/#day-66-model-deployment-and-serving","title":"Day 66 \u2013 Model Deployment and Serving","text":"<p>File: <code>Day_66_Model_Deployment_and_Serving/solutions.py</code></p> <p>Demonstrates REST and gRPC serving patterns for production model deployment.</p>"},{"location":"featured-lessons/#day-67-model-monitoring-and-reliability","title":"Day 67 \u2013 Model Monitoring and Reliability","text":"<p>File: <code>Day_67_Model_Monitoring_and_Reliability/solutions.py</code></p> <p>Covers monitoring, drift detection, and reliability engineering for ML systems.</p>"},{"location":"featured-lessons/#deep-learning-and-modern-nlp","title":"Deep Learning and Modern NLP","text":""},{"location":"featured-lessons/#day-58-transformers-and-attention","title":"Day 58 \u2013 Transformers and Attention","text":"<p>File: <code>Day_58_Transformers_and_Attention/solutions.py</code></p> <p>Builds encoder\u2013decoder stacks, deterministic transformer text classifiers, Hugging Face fine-tuning playbooks, and attention visualisations for rapid experimentation.</p>"},{"location":"featured-lessons/#day-59-generative-models","title":"Day 59 \u2013 Generative Models","text":"<p>File: <code>Day_59_Generative_Models/solutions.py</code></p> <p>Contrasts autoencoders, VAEs, GAN dynamics, and diffusion denoisers with synthetic training loops that log reconstruction improvements.</p>"},{"location":"featured-lessons/#day-64-modern-nlp-pipelines","title":"Day 64 \u2013 Modern NLP Pipelines","text":"<p>File: <code>Day_64_Modern_NLP_Pipelines/solutions.py</code></p> <p>Implements end-to-end NLP pipelines with state-of-the-art techniques.</p>"},{"location":"featured-lessons/#advanced-ml-techniques","title":"Advanced ML Techniques","text":""},{"location":"featured-lessons/#day-60-graph-and-geometric-learning","title":"Day 60 \u2013 Graph and Geometric Learning","text":"<p>File: <code>Day_60_Graph_and_Geometric_Learning/solutions.py</code></p> <p>Implements GraphSAGE and graph attention message passing for toy node-classification graphs with interpretable attention matrices.</p>"},{"location":"featured-lessons/#day-61-reinforcement-and-offline-learning","title":"Day 61 \u2013 Reinforcement and Offline Learning","text":"<p>File: <code>Day_61_Reinforcement_and_Offline_Learning/solutions.py</code></p> <p>Simulates policy-gradient bandits, tabular value iteration, contextual bandits, and offline evaluation with deterministic reward thresholds for regression testing.</p>"},{"location":"featured-lessons/#day-62-model-interpretability-and-fairness","title":"Day 62 \u2013 Model Interpretability and Fairness","text":"<p>File: <code>Day_62_Model_Interpretability_and_Fairness/solutions.py</code></p> <p>Covers SHAP, LIME, counterfactual explanations, and fairness metrics.</p>"},{"location":"featured-lessons/#day-63-causal-inference-and-uplift","title":"Day 63 \u2013 Causal Inference and Uplift","text":"<p>File: <code>Day_63_Causal_Inference_and_Uplift/solutions.py</code></p> <p>Demonstrates causal inference techniques and uplift modeling for business decisions.</p>"},{"location":"featured-lessons/#supervised-and-unsupervised-learning","title":"Supervised and Unsupervised Learning","text":""},{"location":"featured-lessons/#day-51-regularised-models","title":"Day 51 \u2013 Regularised Models","text":"<p>File: <code>Day_51_Regularized_Models/solutions.py</code></p> <p>Compares ridge, lasso, and elastic net pipelines while introducing Poisson regression as a generalised linear model.</p>"},{"location":"featured-lessons/#day-52-ensemble-methods","title":"Day 52 \u2013 Ensemble Methods","text":"<p>File: <code>Day_52_Ensemble_Methods/solutions.py</code></p> <p>Trains random forests, gradient boosting, and stacking ensembles with calibration utilities for trustworthy probabilities.</p>"},{"location":"featured-lessons/#day-53-model-tuning-feature-selection","title":"Day 53 \u2013 Model Tuning &amp; Feature Selection","text":"<p>File: <code>Day_53_Model_Tuning_and_Feature_Selection/solutions.py</code></p> <p>Demonstrates grid search, Bayesian optimisation (via <code>skopt</code>), permutation importance, and recursive feature elimination on a reproducible dataset.</p>"},{"location":"featured-lessons/#day-54-probabilistic-modeling","title":"Day 54 \u2013 Probabilistic Modeling","text":"<p>File: <code>Day_54_Probabilistic_Modeling/solutions.py</code></p> <p>Provides Gaussian mixtures, expectation-maximisation, Bayesian classifiers, and hidden Markov model log-likelihood utilities for reasoning under uncertainty.</p>"},{"location":"featured-lessons/#day-55-advanced-unsupervised-learning","title":"Day 55 \u2013 Advanced Unsupervised Learning","text":"<p>File: <code>Day_55_Advanced_Unsupervised_Learning/solutions.py</code></p> <p>Explores DBSCAN, agglomerative clustering, t-SNE embeddings, autoencoders, and anomaly detection baselines.</p>"},{"location":"featured-lessons/#time-series-and-recommenders","title":"Time Series and Recommenders","text":""},{"location":"featured-lessons/#day-56-time-series-forecasting","title":"Day 56 \u2013 Time Series &amp; Forecasting","text":"<p>File: <code>Day_56_Time_Series_and_Forecasting/solutions.py</code></p> <p>Fits ARIMA/SARIMAX, Holt-Winters, and Prophet-style models with rolling-origin evaluation helpers.</p>"},{"location":"featured-lessons/#day-57-recommender-systems","title":"Day 57 \u2013 Recommender Systems","text":"<p>File: <code>Day_57_Recommender_Systems/solutions.py</code></p> <p>Implements collaborative filtering, matrix factorisation, and ranking metrics for implicit-feedback aware recommenders.</p>"},{"location":"featured-lessons/#data-engineering-and-analytics","title":"Data Engineering and Analytics","text":""},{"location":"featured-lessons/#day-31-relational-databases","title":"Day 31 \u2013 Relational Databases","text":"<p>File: <code>Day_31_Databases/databases.py</code></p> <p>Builds and queries a SQLite database, mirroring production-ready analysis workflows.</p>"},{"location":"featured-lessons/#day-32-other-databases","title":"Day 32 \u2013 Other Databases","text":"<p>File: <code>Day_32_Other_Databases/other_databases.py</code></p> <p>Demonstrates dependency-injected connection patterns for SQL and MongoDB clients so that data access logic remains testable.</p>"},{"location":"featured-lessons/#day-36-capstone-case-study","title":"Day 36 \u2013 Capstone Case Study","text":"<p>File: <code>Day_36_Case_Study/case_study.py</code> and <code>solutions.py</code></p> <p>Ties together the full analytics workflow with a real dataset, cleaning, analysis, and business recommendations.</p>"},{"location":"implementation-guide/","title":"Implementation Guide for Website Enhancements","text":"<p>This guide provides step-by-step instructions for implementing the recommended website improvements.</p>"},{"location":"implementation-guide/#quick-start-essential-features-only","title":"Quick Start (Essential Features Only)","text":"<p>If you want to implement just the core features (JupyterLite + Progress Tracking), follow these steps:</p>"},{"location":"implementation-guide/#1-install-required-dependencies","title":"1. Install Required Dependencies","text":"<pre><code># Add to docs/requirements.txt\npip install jupyterlite-core&gt;=0.3.0\npip install jupyterlite-pyodide-kernel&gt;=0.3.0\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-minify-plugin\n</code></pre>"},{"location":"implementation-guide/#2-integrate-files","title":"2. Integrate Files","text":"<p>Copy these new files to your repository:</p> <ul> <li><code>docs/javascripts/pyodide-console.js</code> \u2192 Interactive Python console</li> <li><code>docs/javascripts/progress-tracker.js</code> \u2192 Progress tracking</li> <li><code>docs/stylesheets/interactive-widgets.css</code> \u2192 Styling for widgets</li> <li><code>jupyter_lite_config.json</code> \u2192 JupyterLite configuration</li> <li><code>tools/integrate_jupyterlite.py</code> \u2192 Build integration tool</li> </ul>"},{"location":"implementation-guide/#3-update-mkdocs-configuration","title":"3. Update MkDocs Configuration","text":"<p>Add to your <code>mkdocs.yml</code>:</p> <pre><code>extra_css:\n  - stylesheets/extra.css\n  - stylesheets/interactive-widgets.css\n\nextra_javascript:\n  - https://cdn.jsdelivr.net/pyodide/v0.25.0/full/pyodide.js\n  - javascripts/pyodide-console.js\n  - javascripts/progress-tracker.js\n</code></pre>"},{"location":"implementation-guide/#4-update-github-actions-workflow","title":"4. Update GitHub Actions Workflow","text":"<p>Modify <code>.github/workflows/docs.yml</code>:</p> <pre><code>- name: Install JupyterLite\n  run: |\n    pip install jupyterlite-core jupyterlite-pyodide-kernel\n\n- name: Build JupyterLite\n  run: python tools/integrate_jupyterlite.py\n\n- name: Build MkDocs site\n  run: mkdocs build --strict\n</code></pre>"},{"location":"implementation-guide/#5-test-locally","title":"5. Test Locally","text":"<pre><code># Build JupyterLite\npython tools/integrate_jupyterlite.py\n\n# Serve documentation\nmkdocs serve\n\n# Visit http://127.0.0.1:8000\n</code></pre>"},{"location":"implementation-guide/#phase-1-interactive-notebooks-week-1","title":"Phase 1: Interactive Notebooks (Week 1)","text":""},{"location":"implementation-guide/#jupyterlite-setup","title":"JupyterLite Setup","text":""},{"location":"implementation-guide/#step-1-install-dependencies","title":"Step 1: Install Dependencies","text":"<pre><code>pip install jupyterlite-core jupyterlite-pyodide-kernel\n</code></pre>"},{"location":"implementation-guide/#step-2-configure-jupyterlite","title":"Step 2: Configure JupyterLite","text":"<p>The <code>jupyter_lite_config.json</code> file is already created. Review and adjust:</p> <pre><code>{\n  \"LiteBuildConfig\": {\n    \"contents\": [\"Day_01_Introduction/\", \"Day_02_Variables_Builtin_Functions/\", ...],\n    \"output_dir\": \"site/jupyterlite\"\n  }\n}\n</code></pre>"},{"location":"implementation-guide/#step-3-build-jupyterlite","title":"Step 3: Build JupyterLite","text":"<pre><code># Manual build\njupyter lite build --output-dir site/jupyterlite\n\n# Or use the integration script\npython tools/integrate_jupyterlite.py\n</code></pre>"},{"location":"implementation-guide/#step-4-add-launch-buttons","title":"Step 4: Add Launch Buttons","text":"<p>The integration script automatically adds buttons, or manually add to lesson pages:</p> <pre><code>## Interactive Notebooks\n\n[\ud83d\ude80 Launch in JupyterLite](../../jupyterlite/lab?path=Day_01_Introduction/introduction.ipynb){ .md-button .md-button--primary }\n</code></pre>"},{"location":"implementation-guide/#step-5-test","title":"Step 5: Test","text":"<ol> <li>Build the site: <code>mkdocs build</code></li> <li>Serve locally: <code>mkdocs serve</code></li> <li>Navigate to a lesson and click the JupyterLite button</li> <li>Verify the notebook loads and runs Python code</li> </ol>"},{"location":"implementation-guide/#binder-integration","title":"Binder Integration","text":""},{"location":"implementation-guide/#step-1-create-binder-configuration","title":"Step 1: Create Binder Configuration","text":"<p>Create <code>.binder/environment.yml</code> or use existing <code>requirements.txt</code>:</p> <pre><code># .binder/environment.yml\nname: coding-mba\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.12\n  - numpy\n  - pandas\n  - matplotlib\n  - scikit-learn\n  - jupyter\n</code></pre> <p>Or just ensure <code>requirements.txt</code> is at the repository root.</p>"},{"location":"implementation-guide/#step-2-add-binder-badges","title":"Step 2: Add Binder Badges","text":"<p>The integration script adds these automatically, or add manually:</p> <pre><code>[![Launch in Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/saint2706/Coding-For-MBA/main?filepath=Day_01_Introduction/introduction.ipynb)\n</code></pre>"},{"location":"implementation-guide/#step-3-test-binder","title":"Step 3: Test Binder","text":"<ol> <li>Click a Binder badge</li> <li>Wait for environment to build (first time takes 5-10 minutes)</li> <li>Verify notebook launches in JupyterLab</li> </ol>"},{"location":"implementation-guide/#phase-2-interactive-widgets-week-2","title":"Phase 2: Interactive Widgets (Week 2)","text":""},{"location":"implementation-guide/#pyodide-console-setup","title":"Pyodide Console Setup","text":""},{"location":"implementation-guide/#step-1-add-javascript-files","title":"Step 1: Add JavaScript Files","text":"<p>Files already created: - <code>docs/javascripts/pyodide-console.js</code></p>"},{"location":"implementation-guide/#step-2-add-to-mkdocs-config","title":"Step 2: Add to MkDocs Config","text":"<pre><code>extra_javascript:\n  - https://cdn.jsdelivr.net/pyodide/v0.25.0/full/pyodide.js\n  - javascripts/pyodide-console.js\n</code></pre>"},{"location":"implementation-guide/#step-3-use-in-lesson-pages","title":"Step 3: Use in Lesson Pages","text":"<p>Mark code blocks as interactive:</p> <p><pre><code>&lt;div class=\"interactive-code\"&gt;\n```python\n# This code will be executable!\nprint(\"Hello from Pyodide!\")\n</code></pre> <pre><code>Or use the widget directly:\n\n```html\n&lt;div id=\"my-widget\"&gt;&lt;/div&gt;\n&lt;script&gt;\n  createInteractiveWidget(\n    document.getElementById('my-widget'),\n    'print(\"Interactive Python!\")'\n  );\n&lt;/script&gt;\n</code></pre></p>"},{"location":"implementation-guide/#step-4-test","title":"Step 4: Test","text":"<ol> <li>Rebuild site</li> <li>Visit a page with interactive code</li> <li>Verify \"Run Code\" button appears</li> <li>Click and verify code executes</li> </ol>"},{"location":"implementation-guide/#progress-tracking-setup","title":"Progress Tracking Setup","text":""},{"location":"implementation-guide/#step-1-add-javascript","title":"Step 1: Add JavaScript","text":"<p>File already created: - <code>docs/javascripts/progress-tracker.js</code></p>"},{"location":"implementation-guide/#step-2-add-to-mkdocs-config_1","title":"Step 2: Add to MkDocs Config","text":"<pre><code>extra_javascript:\n  - javascripts/progress-tracker.js\n</code></pre>"},{"location":"implementation-guide/#step-3-add-css","title":"Step 3: Add CSS","text":"<p>File already created: - <code>docs/stylesheets/interactive-widgets.css</code></p>"},{"location":"implementation-guide/#step-4-test_1","title":"Step 4: Test","text":"<ol> <li>Rebuild and serve site</li> <li>Navigate to any lesson page</li> <li>Look for:</li> <li>Progress widget in sidebar</li> <li>\"Mark as Complete\" button</li> <li>Progress percentage</li> <li>Click \"Mark as Complete\"</li> <li>Verify progress is saved (check localStorage in browser dev tools)</li> <li>Refresh page and verify progress persists</li> </ol>"},{"location":"implementation-guide/#phase-3-enhanced-features-week-3-4","title":"Phase 3: Enhanced Features (Week 3-4)","text":""},{"location":"implementation-guide/#enhanced-search","title":"Enhanced Search","text":""},{"location":"implementation-guide/#step-1-update-search-plugin","title":"Step 1: Update Search Plugin","text":"<pre><code>plugins:\n  - search:\n      lang: en\n      separator: '[\\s\\-\\.,;:!?\\(\\)\\[\\]\\{\\}]+'\n      prebuild_index: true\n      indexing: 'full'\n</code></pre>"},{"location":"implementation-guide/#step-2-index-notebook-content","title":"Step 2: Index Notebook Content","text":"<p>Add to <code>tools/build_docs.py</code>:</p> <pre><code>def extract_notebook_content(notebook_path: Path) -&gt; str:\n    \"\"\"Extract searchable content from notebook.\"\"\"\n    import nbformat\n\n    with open(notebook_path) as f:\n        nb = nbformat.read(f, as_version=4)\n\n    content = []\n    for cell in nb.cells:\n        if cell.cell_type in ('markdown', 'code'):\n            content.append(cell.source)\n\n    return '\\n\\n'.join(content)\n</code></pre>"},{"location":"implementation-guide/#git-revision-dates","title":"Git Revision Dates","text":""},{"location":"implementation-guide/#step-1-install-plugin","title":"Step 1: Install Plugin","text":"<pre><code>pip install mkdocs-git-revision-date-localized-plugin\n</code></pre>"},{"location":"implementation-guide/#step-2-add-to-config","title":"Step 2: Add to Config","text":"<pre><code>plugins:\n  - git-revision-date-localized:\n      enable_creation_date: true\n      type: timeago\n</code></pre>"},{"location":"implementation-guide/#minification","title":"Minification","text":""},{"location":"implementation-guide/#step-1-install-plugin_1","title":"Step 1: Install Plugin","text":"<pre><code>pip install mkdocs-minify-plugin\n</code></pre>"},{"location":"implementation-guide/#step-2-add-to-config_1","title":"Step 2: Add to Config","text":"<pre><code>plugins:\n  - minify:\n      minify_html: true\n      minify_js: true\n      minify_css: true\n</code></pre>"},{"location":"implementation-guide/#phase-4-advanced-features-week-5-6","title":"Phase 4: Advanced Features (Week 5-6)","text":""},{"location":"implementation-guide/#social-cards","title":"Social Cards","text":"<pre><code>pip install pillow cairosvg\n</code></pre> <pre><code>plugins:\n  - social:\n      cards: true\n      cards_color:\n        fill: \"#667eea\"\n        text: \"#FFFFFF\"\n</code></pre>"},{"location":"implementation-guide/#tags-system","title":"Tags System","text":"<pre><code>plugins:\n  - tags:\n      tags_file: tags.md\n</code></pre> <p>Add to lesson frontmatter:</p> <pre><code>---\ntags:\n  - python-basics\n  - data-structures\n---\n</code></pre>"},{"location":"implementation-guide/#analytics-privacy-friendly","title":"Analytics (Privacy-Friendly)","text":"<p>For Plausible Analytics:</p> <pre><code>&lt;!-- Add to docs/overrides/main.html --&gt;\n&lt;script defer data-domain=\"saint2706.github.io\" \n        src=\"https://plausible.io/js/script.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"implementation-guide/#testing-checklist","title":"Testing Checklist","text":""},{"location":"implementation-guide/#functional-testing","title":"Functional Testing","text":"<ul> <li> JupyterLite launches successfully</li> <li> Notebooks load in JupyterLite</li> <li> Code executes in JupyterLite</li> <li> Binder badges work</li> <li> Pyodide console runs code</li> <li> Progress tracking saves/loads</li> <li> Progress persists across page loads</li> <li> Mark complete button works</li> <li> Interactive widgets display correctly</li> </ul>"},{"location":"implementation-guide/#accessibility-testing","title":"Accessibility Testing","text":"<ul> <li> Keyboard navigation works</li> <li> Screen reader announces interactive elements</li> <li> Focus indicators are visible</li> <li> Color contrast meets WCAG 2.1 AA</li> <li> ARIA labels are present</li> <li> Skip links work</li> </ul>"},{"location":"implementation-guide/#performance-testing","title":"Performance Testing","text":"<ul> <li> Page load time &lt; 3 seconds</li> <li> JupyterLite loads &lt; 30 seconds (first time)</li> <li> Search responds &lt; 1 second</li> <li> No console errors</li> <li> Mobile responsive</li> </ul>"},{"location":"implementation-guide/#browser-testing","title":"Browser Testing","text":"<p>Test in: - [ ] Chrome (latest) - [ ] Firefox (latest) - [ ] Safari (latest) - [ ] Edge (latest) - [ ] Mobile browsers</p>"},{"location":"implementation-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"implementation-guide/#jupyterlite-wont-load","title":"JupyterLite Won't Load","text":"<p>Problem: JupyterLite shows blank page or errors</p> <p>Solutions: 1. Check browser console for errors 2. Verify browser supports WebAssembly 3. Clear browser cache 4. Try different browser 5. Check that <code>pyodide.js</code> CDN is accessible</p>"},{"location":"implementation-guide/#notebooks-not-found-in-jupyterlite","title":"Notebooks Not Found in JupyterLite","text":"<p>Problem: Notebooks don't appear in JupyterLite file browser</p> <p>Solutions: 1. Verify notebooks are in <code>Day_*</code> directories 2. Check <code>jupyter_lite_config.json</code> includes correct paths 3. Rebuild JupyterLite: <code>python tools/integrate_jupyterlite.py</code> 4. Verify notebooks are committed to git</p>"},{"location":"implementation-guide/#progress-not-saving","title":"Progress Not Saving","text":"<p>Problem: Progress resets after page refresh</p> <p>Solutions: 1. Check browser allows localStorage 2. Verify JavaScript is enabled 3. Check browser console for errors 4. Try different browser 5. Check if in private/incognito mode</p>"},{"location":"implementation-guide/#pyodide-console-errors","title":"Pyodide Console Errors","text":"<p>Problem: Code won't execute in Pyodide console</p> <p>Solutions: 1. Check code syntax 2. Verify required packages are available in Pyodide 3. Check browser console for specific errors 4. Some packages (like database connectors) won't work in browser 5. Try simpler code to verify Pyodide is working</p>"},{"location":"implementation-guide/#build-fails","title":"Build Fails","text":"<p>Problem: <code>mkdocs build</code> or JupyterLite build fails</p> <p>Solutions: 1. Check all dependencies are installed 2. Verify Python version (3.11+ for JupyterLite) 3. Check for syntax errors in config files 4. Run with <code>--verbose</code> flag for detailed errors 5. Clear build cache: <code>rm -rf site/</code></p>"},{"location":"implementation-guide/#maintenance","title":"Maintenance","text":""},{"location":"implementation-guide/#regular-tasks-monthly","title":"Regular Tasks (Monthly)","text":"<ol> <li> <p>Update dependencies:    <pre><code>pip install --upgrade jupyterlite-core jupyterlite-pyodide-kernel mkdocs-material\n</code></pre></p> </li> <li> <p>Test JupyterLite:</p> </li> <li>Launch a few notebooks</li> <li>Verify common packages work</li> <li> <p>Check for console errors</p> </li> <li> <p>Review analytics:</p> </li> <li>Check which lessons are popular</li> <li>Identify issues from error logs</li> <li>Review user feedback</li> </ol>"},{"location":"implementation-guide/#quarterly-tasks","title":"Quarterly Tasks","text":"<ol> <li>Update Pyodide version:</li> <li>Check for new Pyodide releases</li> <li>Update CDN URL in <code>mkdocs.yml</code></li> <li> <p>Test thoroughly</p> </li> <li> <p>Review accessibility:</p> </li> <li>Run Lighthouse audit</li> <li>Test with screen reader</li> <li> <p>Check keyboard navigation</p> </li> <li> <p>Optimize performance:</p> </li> <li>Review page load times</li> <li>Optimize images</li> <li>Minify assets</li> </ol>"},{"location":"implementation-guide/#annual-tasks","title":"Annual Tasks","text":"<ol> <li>Major version updates:</li> <li>Update MkDocs Material</li> <li>Update JupyterLite</li> <li> <p>Review breaking changes</p> </li> <li> <p>Comprehensive testing:</p> </li> <li>Test all features</li> <li>Check all browsers</li> <li>Mobile testing</li> </ol>"},{"location":"implementation-guide/#getting-help","title":"Getting Help","text":""},{"location":"implementation-guide/#resources","title":"Resources","text":"<ul> <li>JupyterLite Docs: https://jupyterlite.readthedocs.io/</li> <li>MkDocs Material: https://squidfunk.github.io/mkdocs-material/</li> <li>Pyodide Docs: https://pyodide.org/</li> <li>Repository Issues: https://github.com/saint2706/Coding-For-MBA/issues</li> </ul>"},{"location":"implementation-guide/#community-support","title":"Community Support","text":"<ul> <li>Open an issue on GitHub</li> <li>Check existing discussions</li> <li>Review documentation thoroughly</li> </ul>"},{"location":"implementation-guide/#professional-support","title":"Professional Support","text":"<p>For complex implementations, consider: - Hiring a web developer familiar with MkDocs - Consulting with JupyterLite community - Using professional services for accessibility audits</p>"},{"location":"implementation-guide/#next-steps","title":"Next Steps","text":"<ol> <li>\u2705 Review the recommendations document</li> <li>\u2705 Test sample implementations locally</li> <li>\u2705 Choose which features to implement first</li> <li>\u2705 Follow this guide for implementation</li> <li>\u2705 Test thoroughly before deploying</li> <li>\u2705 Deploy to GitHub Pages</li> <li>\u2705 Monitor and iterate based on feedback</li> </ol> <p>Good luck with your implementation! \ud83d\ude80</p>"},{"location":"ml_curriculum/","title":"Machine Learning Curriculum","text":""},{"location":"ml_curriculum/#layout-default-title-machine-learning-curriculum-roadmap-nav_order-2-permalink-ml-curriculum","title":"layout: default title: Machine Learning Curriculum Roadmap nav_order: 2 permalink: /ml-curriculum/","text":"<p>This roadmap outlines a phased journey from classic machine learning foundations to modern deep learning systems and production operations. Each phase highlights anchor lessons from the Coding for MBA series and recommends follow-on topics so you can keep advancing after Day 57.</p> <p>\ud83d\udcd0 Looking for mathematical foundations? Check out the comprehensive ML Theory &amp; Mathematics guide, which covers linear algebra, calculus, probability, neural networks, deep learning architectures, and all the mathematical concepts underlying this curriculum.</p>"},{"location":"ml_curriculum/#phase-1-classic-machine-learning-foundations","title":"Phase 1 \u2013 Classic Machine Learning Foundations","text":"<p>Start here if you are following the Day 40\u201353 sequence.</p> <p>The goal of Phase 1 is to master supervised learning workflows, evaluation techniques, and model selection before layering on deep learning.</p> Day Lesson Key takeaway Day 40 Introduction to Machine Learning Frame ML problems, manage the train/validate/test split, and measure performance with cross-validation. Day 41 Supervised Learning \u2013 Regression Fit linear models, tune regularisation, and interpret coefficients for business insights. Day 42 Supervised Learning \u2013 Classification Part 1 Compare logistic regression and decision trees while diagnosing accuracy, precision, and recall. Day 43 Supervised Learning \u2013 Classification Part 2 Ensemble methods, ROC curves, and thresholding strategy selection. Day 44 Unsupervised Learning Cluster customer segments and reduce dimensionality with PCA. Day 45 Feature Engineering &amp; Evaluation Build repeatable feature pipelines and validate models with more nuanced metrics. Day 46 Intro to Neural Networks Understand perceptrons, activation functions, and gradient descent. Day 47 Convolutional Neural Networks Apply convolutional filters for image classification. Day 48 Recurrent Neural Networks Model sequential data with RNNs, LSTMs, and GRUs. Day 49 Natural Language Processing Build text classification pipelines with tokenisation and embeddings. Day 50 MLOps Package, persist, and monitor models for reliable deployment. Day 51 Regularised Models Contrast ridge, lasso, elastic net, and Poisson GLMs while measuring coefficient shrinkage. Day 52 Ensemble Methods Blend bagging, boosting, and stacking ensembles with calibrated probability estimates. Day 53 Model Tuning &amp; Feature Selection Optimise hyperparameters with grid/Bayesian search and validate feature subsets via permutation importance and RFE. Day 54 Probabilistic Modeling Master Gaussian mixtures, Bayesian classifiers, EM, and HMM log-likelihoods to reason about uncertainty. Day 55 Advanced Unsupervised Learning Deploy DBSCAN, hierarchical clustering, t-SNE/UMAP-style embeddings, and autoencoder-driven anomaly detection. Day 56 Time Series &amp; Forecasting Forecast seasonal demand with ARIMA/SARIMAX, Holt-Winters, and Prophet-style decompositions plus robust evaluation. Day 57 Recommender Systems Build collaborative filtering and matrix factorisation recommenders with implicit-feedback aware ranking metrics."},{"location":"ml_curriculum/#recommended-next-steps-after-phase-1","title":"Recommended next steps after Phase 1","text":"<ul> <li>Apply cross-validation and feature engineering to your own datasets, documenting experiment results in a reproducible format.</li> <li>Practice hyperparameter optimisation with grid search, random search, and Bayesian optimisation frameworks.</li> <li>Explore AutoML tooling (e.g., Auto-sklearn, H2O AutoML) to accelerate baseline model comparisons.</li> </ul>"},{"location":"ml_curriculum/#phase-2-modern-deep-learning-representation-learning","title":"Phase 2 \u2013 Modern Deep Learning &amp; Representation Learning","text":"<p>Deep learning expands the model families available in Phase 1. Focus on building intuition for architectures, transfer learning, and optimisation.</p> Day Lesson Key takeaway Day 58 Transformers and Attention Assemble encoder\u2013decoder stacks, fine-tune pretrained checkpoints, and interpret attention heatmaps with deterministic demos. Day 59 Generative Models Compare autoencoders, VAEs, GANs, and diffusion denoisers while monitoring reconstruction loss curves on synthetic data. Day 60 Graph and Geometric Learning Prototype GraphSAGE and GAT-style message passing networks for toy node classification tasks with interpretable attention scores. Day 61 Reinforcement and Offline Learning Explore policy/value methods, contextual bandits, and offline evaluation baselines that converge to reproducible reward thresholds. <ul> <li>Refresh Python packages for GPU acceleration (PyTorch or TensorFlow) and practice training models on cloud notebooks.</li> <li>Study convolutional network variants (ResNet, EfficientNet) and fine-tune pretrained models for domain-specific images.</li> <li>Learn about transformer architectures for text, vision, and multimodal data, starting with embeddings and attention mechanisms.</li> <li>Experiment with self-supervised learning and contrastive objectives to pre-train representations on unlabelled data.</li> </ul>"},{"location":"ml_curriculum/#phase-3-responsible-ai-model-governance","title":"Phase 3 \u2013 Responsible AI &amp; Model Governance","text":"<p>Ensure that your models meet ethical, legal, and organisational standards before moving them into production.</p> <ul> <li>Perform bias and fairness audits using disparate impact, equal opportunity, and calibration diagnostics.</li> <li>Implement explainability techniques (feature importance, SHAP, LIME) to communicate model decisions to stakeholders.</li> <li>Establish data governance practices: data lineage, versioning, and documentation (model cards, data sheets).</li> <li>Learn the regulatory landscape (GDPR, CCPA, sector-specific rules) and how they influence data collection and model deployment.</li> </ul>"},{"location":"ml_curriculum/#phase-4-mlops-monitoring-and-lifecycle-management","title":"Phase 4 \u2013 MLOps, Monitoring, and Lifecycle Management","text":"<p>Turn prototypes into production systems by investing in reliable infrastructure and collaboration workflows.</p> Day Lesson Key takeaway Day 65 MLOps Pipelines and CI/CD Orchestrate feature stores, registries, and GitHub Actions workflows with DAG-style automation. Day 66 Model Deployment and Serving Compare REST, gRPC, batch, streaming, and edge serving patterns with load-tested adapters. Day 67 Model Monitoring and Reliability Detect data drift, evaluate canaries, and export observability metrics for retraining triggers. <ul> <li>Industrialise feature pipelines with tools like Feature Store platforms or workflow orchestrators (Airflow, Prefect).</li> <li>Automate training and evaluation with CI/CD pipelines, integrating unit tests, data quality checks, and model validation gates.</li> <li>Deploy models to scalable serving infrastructure (REST endpoints, batch scoring, or streaming) with containerisation and serverless platforms.</li> <li>Instrument monitoring for data drift, model drift, and performance regressions; integrate alerting and human-in-the-loop retraining.</li> <li>Track experiments, artefacts, and lineage with MLflow, Weights &amp; Biases, or similar platforms, ensuring reproducibility across teams.</li> </ul>"},{"location":"ml_curriculum/#putting-it-all-together","title":"Putting it all together","text":"<p>Progressing through these phases transforms the Day 40\u201353 lessons into a comprehensive ML competency path. Loop back to earlier phases whenever you encounter new domains or stakeholders\u2014revisiting the fundamentals will keep each new system grounded in sound methodology.</p>"},{"location":"roadmap/","title":"Repository Upgrade Roadmap","text":"<p>This document outlines the future work planned for the <code>Coding-For-MBA</code> repository, continuing the modernization and refactoring efforts.</p>"},{"location":"roadmap/#phase-1-core-data-science-curriculum-days-26-37-complete","title":"Phase 1: Core Data Science Curriculum (Days 26-37) \u2705 COMPLETE","text":"<p>All Phase 1 lessons have been successfully refactored and modernized:</p> <ol> <li>\u2705 All Python scripts refactored into modular, testable functions</li> <li>\u2705 Comprehensive test coverage with <code>pytest</code> (53 tests passing)</li> <li>\u2705 READMEs updated to the detailed, consistent format</li> <li>\u2705 Jupyter notebooks generated for all lessons</li> </ol> <p>Completed lessons:</p> <ul> <li>\u2705 <code>Day_26_Statistics</code></li> <li>\u2705 <code>Day_27_Visualization</code> (Converted to Notebook)</li> <li>\u2705 <code>Day_28_Advanced_Visualization</code> (Converted to Notebook)</li> <li>\u2705 <code>Day_29_Interactive_Visualization</code> (Enhanced notebook)</li> <li>\u2705 <code>Day_30_Web_Scraping</code></li> <li>\u2705 <code>Day_31_Databases</code></li> <li>\u2705 <code>Day_32_Other_Databases</code></li> <li>\u2705 <code>Day_33_API</code></li> <li>\u2705 <code>Day_34_Building_an_API</code></li> <li>\u2705 <code>Day_35_Flask_Web_Framework</code></li> <li>\u2705 <code>Day_36_Case_Study</code></li> <li>\u2705 <code>Day_37_Conclusion</code></li> </ul>"},{"location":"roadmap/#phase-2-machine-learning-curriculum-days-38-50-completed","title":"Phase 2: Machine Learning Curriculum (Days 38-50) \u2705 COMPLETED","text":"<p>The Machine Learning section has been successfully refactored with clear, well-documented code and updated <code>README.md</code> files following the established pattern.</p> <p>Completed lessons:</p> <ul> <li>\u2705 <code>Day_38_Linear_Algebra</code></li> <li>\u2705 <code>Day_39_Calculus</code></li> <li>\u2705 <code>Day_40_Intro_to_ML</code></li> <li>\u2705 <code>Day_41_Supervised_Learning_Regression</code></li> <li>\u2705 <code>Day_42_Supervised_Learning_Classification_Part_1</code></li> <li>\u2705 <code>Day_43_Supervised_Learning_Classification_Part_2</code></li> <li>\u2705 <code>Day_44_Unsupervised_Learning</code></li> <li>\u2705 <code>Day_45_Feature_Engineering_and_Evaluation</code></li> <li>\u2705 <code>Day_46_Intro_to_Neural_Networks</code></li> <li>\u2705 <code>Day_47_Convolutional_Neural_Networks</code></li> <li>\u2705 <code>Day_48_Recurrent_Neural_Networks</code></li> <li>\u2705 <code>Day_49_NLP</code></li> <li>\u2705 <code>Day_50_MLOps</code></li> </ul>"},{"location":"roadmap/#phase-3-advanced-ml-mlops-curriculum-days-51-67-completed","title":"Phase 3: Advanced ML &amp; MLOps Curriculum (Days 51-67) \u2705 COMPLETED","text":"<p>The advanced machine learning and MLOps lessons have been successfully refactored and modernized, maintaining high code quality standards throughout these cutting-edge topics.</p> <p>Completed lessons:</p> <ul> <li>\u2705 <code>Day_51_Regularized_Models</code></li> <li>\u2705 <code>Day_52_Ensemble_Methods</code></li> <li>\u2705 <code>Day_53_Model_Tuning_and_Feature_Selection</code></li> <li>\u2705 <code>Day_54_Probabilistic_Modeling</code></li> <li>\u2705 <code>Day_55_Advanced_Unsupervised_Learning</code></li> <li>\u2705 <code>Day_56_Time_Series_and_Forecasting</code></li> <li>\u2705 <code>Day_57_Recommender_Systems</code></li> <li>\u2705 <code>Day_58_Transformers_and_Attention</code></li> <li>\u2705 <code>Day_59_Generative_Models</code></li> <li>\u2705 <code>Day_60_Graph_and_Geometric_Learning</code></li> <li>\u2705 <code>Day_61_Reinforcement_and_Offline_Learning</code></li> <li>\u2705 <code>Day_62_Model_Interpretability_and_Fairness</code></li> <li>\u2705 <code>Day_63_Causal_Inference_and_Uplift</code></li> <li>\u2705 <code>Day_64_Modern_NLP_Pipelines</code></li> <li>\u2705 <code>Day_65_MLOps_Pipelines_and_CI</code></li> <li>\u2705 <code>Day_66_Model_Deployment_and_Serving</code></li> <li>\u2705 <code>Day_67_Model_Monitoring_and_Reliability</code></li> </ul>"},{"location":"roadmap/#2026-roadmap","title":"2026 Roadmap","text":""},{"location":"roadmap/#q1-2026-january-march","title":"Q1 2026 (January - March)","text":"<p>Infrastructure &amp; Tooling</p> <ul> <li>Migrate to Python 3.13 when stable</li> <li>Implement automated dependency security scanning with Dependabot</li> <li>Add pre-commit hooks for code quality enforcement</li> <li>Set up automated performance benchmarking for data-heavy lessons</li> <li>Create GitHub issue templates for bugs, feature requests, and lesson improvements</li> </ul> <p>Content Enhancement</p> <ul> <li>Complete Phase 1 refactoring (Days 26-37)</li> <li>Add video walkthroughs for Days 1-10 (Python foundations)</li> <li>Create interactive Jupyter Lab environment setup guide</li> <li>Develop downloadable cheat sheets for each phase</li> </ul> <p>Community Building</p> <ul> <li>Launch GitHub Discussions for learner community</li> <li>Create contributing guidelines for external contributors</li> <li>Establish monthly office hours for Q&amp;A sessions</li> <li>Set up a showcase repository for learner projects</li> </ul>"},{"location":"roadmap/#q2-2026-april-june","title":"Q2 2026 (April - June)","text":"<p>Testing &amp; Quality</p> <ul> <li>Increase test coverage to 60% across all lessons</li> <li>Add integration tests for database lessons (Days 31-32)</li> <li>Implement end-to-end tests for API lessons (Days 33-34)</li> <li>Add performance regression tests for ML pipelines</li> <li>Create smoke tests for all Jupyter notebooks</li> </ul> <p>Content Enhancement</p> <ul> <li>Complete Phase 2 refactoring (Days 38-50)</li> <li>Add case studies from real business scenarios</li> <li>Develop companion exercises with auto-grading capabilities</li> <li>Create study guides with learning objectives for each lesson</li> <li>Add accessibility features to all documentation</li> </ul> <p>Documentation</p> <ul> <li>Develop instructor's guide for teaching this curriculum</li> <li>Create lesson dependency graph showing prerequisite relationships</li> <li>Write technical architecture documentation</li> <li>Publish API reference documentation for utility modules</li> </ul>"},{"location":"roadmap/#q3-2026-july-september","title":"Q3 2026 (July - September)","text":"<p>Advanced Content</p> <ul> <li>Begin Phase 3 refactoring (Days 51-67)</li> <li>Add supplementary content on LLM fine-tuning</li> <li>Create advanced workshops on MLOps best practices</li> <li>Develop real-world capstone projects integrating multiple lessons</li> <li>Add content on emerging ML frameworks and tools</li> </ul> <p>Infrastructure</p> <ul> <li>Implement continuous deployment for documentation site</li> <li>Add automated link checking for all documentation</li> <li>Set up analytics for documentation usage patterns</li> <li>Create Docker containers for consistent development environments</li> <li>Establish cloud-based development environment (GitHub Codespaces)</li> </ul> <p>Community &amp; Outreach</p> <ul> <li>Launch certification program with badges</li> <li>Create alumni network for curriculum graduates</li> <li>Partner with MBA programs for pilot implementations</li> <li>Develop case competition based on curriculum content</li> <li>Host first annual virtual summit for learners</li> </ul>"},{"location":"roadmap/#q4-2026-october-december","title":"Q4 2026 (October - December)","text":"<p>Platform Enhancements</p> <ul> <li>Build interactive learning platform with progress tracking</li> <li>Implement solution checking system for exercises</li> <li>Create personalized learning paths based on background</li> <li>Add collaborative coding features</li> <li>Develop mobile-friendly documentation viewer</li> </ul> <p>Content Expansion</p> <ul> <li>Add bonus content on AI ethics and governance</li> <li>Create industry-specific adaptations (finance, healthcare, retail)</li> <li>Develop advanced topics series (quantum ML, federated learning)</li> <li>Add multilingual support (Spanish, Mandarin)</li> <li>Create podcast series discussing each lesson</li> </ul> <p>Quality &amp; Maintenance</p> <ul> <li>Complete Phase 3 refactoring</li> <li>Achieve 75% test coverage milestone</li> <li>Comprehensive dependency audit and updates</li> <li>Performance optimization of all computational notebooks</li> <li>Accessibility audit and remediation</li> </ul>"},{"location":"roadmap/#2027-roadmap","title":"2027 Roadmap","text":""},{"location":"roadmap/#q1-2027-january-march","title":"Q1 2027 (January - March)","text":"<p>Next-Generation Content</p> <ul> <li>Launch \"Days 68-80\" series on emerging AI topics</li> <li>Add content on AI agents and autonomous systems</li> <li>Create lessons on vector databases and RAG systems</li> <li>Develop content on AI safety and alignment</li> <li>Add practical workshops on building AI products</li> </ul> <p>Infrastructure Evolution</p> <ul> <li>Migrate to modular course architecture</li> <li>Implement AI-powered learning assistant</li> <li>Add real-time collaboration features</li> <li>Create API for third-party integrations</li> <li>Develop mobile learning app (iOS/Android)</li> </ul> <p>Research &amp; Innovation</p> <ul> <li>Establish research partnerships with universities</li> <li>Create industry advisory board</li> <li>Launch beta testing program for new content</li> <li>Develop pedagogical research on effectiveness</li> <li>Publish annual impact report</li> </ul>"},{"location":"roadmap/#q2-2027-april-june","title":"Q2 2027 (April - June)","text":"<p>Enterprise Features</p> <ul> <li>Create enterprise learning management system (LMS) integration</li> <li>Develop team learning features with shared environments</li> <li>Add instructor dashboard with student analytics</li> <li>Create customizable curriculum paths for organizations</li> <li>Implement SSO and enterprise security features</li> </ul> <p>Content Quality</p> <ul> <li>Comprehensive curriculum review with industry experts</li> <li>Update all content for latest Python and library versions</li> <li>Add video content for all 67 original days</li> <li>Create interactive coding challenges for each lesson</li> <li>Develop comprehensive assessment system</li> </ul> <p>Community Maturity</p> <ul> <li>Launch mentorship program connecting learners with experts</li> <li>Create regional learning groups worldwide</li> <li>Establish content creator program for community contributions</li> <li>Host in-person meetups in major cities</li> <li>Develop alumni career services network</li> </ul>"},{"location":"roadmap/#q3-2027-july-september","title":"Q3 2027 (July - September)","text":"<p>Advanced Specializations</p> <ul> <li>Launch specialized tracks (Computer Vision, NLP, RL)</li> <li>Create advanced practitioner certifications</li> <li>Develop executive-level AI strategy content</li> <li>Add content on AI product management</li> <li>Create industry transformation case studies</li> </ul> <p>Platform Excellence</p> <ul> <li>Achieve 95% test coverage</li> <li>Implement zero-downtime deployment pipeline</li> <li>Add multi-cloud deployment support</li> <li>Create comprehensive disaster recovery plan</li> <li>Achieve accessibility WCAG 2.2 AA compliance</li> </ul> <p>Global Expansion</p> <ul> <li>Complete multilingual support for top 10 languages</li> <li>Create region-specific case studies</li> <li>Partner with international educational institutions</li> <li>Develop culturally adapted content</li> <li>Launch global marketing campaign</li> </ul>"},{"location":"roadmap/#q4-2027-october-december","title":"Q4 2027 (October - December)","text":"<p>Sustainability &amp; Scale</p> <ul> <li>Implement sustainable funding model</li> <li>Create scholarship program for underrepresented groups</li> <li>Establish open-source contributor fund</li> <li>Develop long-term maintenance plan</li> <li>Build self-sustaining community governance</li> </ul> <p>Future Vision</p> <ul> <li>Launch \"Coding for MBA 2.0\" planning initiative</li> <li>Create 5-year strategic roadmap (2028-2032)</li> <li>Establish research agenda for next-generation curriculum</li> <li>Develop partnerships for emerging technology integration</li> <li>Create innovation lab for experimental content</li> </ul> <p>Excellence &amp; Recognition</p> <ul> <li>Apply for educational technology awards</li> <li>Publish peer-reviewed papers on curriculum effectiveness</li> <li>Achieve industry certification and accreditation</li> <li>Document success stories and testimonials</li> <li>Host year-end celebration and graduation ceremony</li> </ul>"},{"location":"roadmap/#high-level-repository-goals","title":"High-Level Repository Goals","text":"<ul> <li>Increase Test Coverage: While unit tests have been added for the refactored lessons, there is an opportunity to increase coverage and add more integration-style tests.</li> <li>Performance Profiling: The performance of other data-heavy scripts can be profiled and optimized, similar to the work done on <code>Day_25_Data_Cleaning</code>.</li> <li>Interactive Visualizations: Add more interactive visualizations using <code>Plotly</code> to other data-focused lessons to improve user engagement.</li> <li>Dependency Review: Periodically review and update the <code>requirements.txt</code> file to ensure all dependencies are on their latest stable versions.</li> <li>Accessibility First: Ensure all content meets WCAG accessibility standards.</li> <li>Community Driven: Foster an active, inclusive community around the curriculum.</li> <li>Industry Relevant: Keep content aligned with current business and technology needs.</li> <li>Sustainable Growth: Build infrastructure and processes that scale with the community.</li> </ul>"},{"location":"theory/","title":"Machine Learning Theory and Mathematical Foundations","text":"<p>This document provides a comprehensive reference for the mathematical and theoretical concepts underlying the machine learning curriculum (Days 38-67). Each section builds from first principles to practical applications in modern ML systems.</p>"},{"location":"theory/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Linear Algebra Foundations</li> <li>Calculus and Optimization</li> <li>Probability and Statistics</li> <li>Supervised Learning Theory</li> <li>Neural Networks and Deep Learning</li> <li>Regularization Techniques</li> <li>Ensemble Methods</li> <li>Unsupervised Learning</li> <li>Probabilistic Modeling</li> <li>Time Series Analysis</li> <li>Advanced Deep Learning</li> <li>Model Evaluation and Selection</li> </ol>"},{"location":"theory/#1-linear-algebra-foundations","title":"1. Linear Algebra Foundations","text":"<p>Linear algebra provides the mathematical language for representing and manipulating data in machine learning.</p>"},{"location":"theory/#11-vectors","title":"1.1 Vectors","text":"<p>A vector is an ordered collection of numbers that can represent:</p> <ul> <li>A data point with multiple features</li> <li>A direction and magnitude in space</li> <li>A point in n-dimensional space</li> </ul> <p>Notation: A vector v in \u211d\u207f is written as:</p> <pre><code>v = [v\u2081, v\u2082, ..., v\u2099]\u1d40\n</code></pre> <p>Key Operations:</p> <ul> <li>Vector Addition: (v + w)\u1d62 = v\u1d62 + w\u1d62</li> <li>Scalar Multiplication: (\u03b1v)\u1d62 = \u03b1v\u1d62</li> <li>Dot Product: v \u00b7 w = \u03a3\u1d62 v\u1d62w\u1d62 = ||v|| ||w|| cos(\u03b8)</li> <li>Norm (Length): ||v|| = \u221a(\u03a3\u1d62 v\u1d62\u00b2)</li> </ul> <p>ML Application: Feature vectors represent observations; dot products measure similarity between data points.</p>"},{"location":"theory/#12-matrices","title":"1.2 Matrices","text":"<p>A matrix is a rectangular array of numbers with dimensions m \u00d7 n (m rows, n columns).</p> <p>Notation:</p> <pre><code>A = [a\u1d62\u2c7c] where i \u2208 {1,...,m}, j \u2208 {1,...,n}\n</code></pre> <p>Key Operations:</p> <ul> <li>Matrix Addition: (A + B)\u1d62\u2c7c = a\u1d62\u2c7c + b\u1d62\u2c7c</li> <li>Matrix Multiplication: (AB)\u1d62\u2c7c = \u03a3\u2096 a\u1d62\u2096b\u2096\u2c7c</li> <li>Transpose: (A\u1d40)\u1d62\u2c7c = a\u2c7c\u1d62</li> <li>Inverse: AA\u207b\u00b9 = A\u207b\u00b9A = I (when A is square and non-singular)</li> </ul> <p>ML Application: Datasets are stored as matrices where rows are observations and columns are features. Matrix multiplication implements linear transformations and neural network layers.</p>"},{"location":"theory/#13-eigenvalues-and-eigenvectors","title":"1.3 Eigenvalues and Eigenvectors","text":"<p>For a square matrix A, an eigenvector v and eigenvalue \u03bb satisfy:</p> <pre><code>Av = \u03bbv\n</code></pre> <p>This means A stretches v by a factor of \u03bb without changing its direction.</p> <p>Eigendecomposition:</p> <pre><code>A = Q\u039bQ\u1d40\n</code></pre> <p>where Q contains eigenvectors and \u039b is a diagonal matrix of eigenvalues.</p> <p>ML Application:</p> <ul> <li>Principal Component Analysis (PCA): Uses eigenvectors of the covariance matrix to find directions of maximum variance</li> <li>Dimensionality Reduction: Project data onto top-k eigenvectors to reduce dimensions while preserving variance</li> <li>Spectral Clustering: Uses eigenvectors of graph Laplacian matrices</li> </ul>"},{"location":"theory/#14-singular-value-decomposition-svd","title":"1.4 Singular Value Decomposition (SVD)","text":"<p>For any m \u00d7 n matrix A:</p> <pre><code>A = U\u03a3V\u1d40\n</code></pre> <p>where:</p> <ul> <li>U (m \u00d7 m): Left singular vectors</li> <li>\u03a3 (m \u00d7 n): Diagonal matrix of singular values</li> <li>V (n \u00d7 n): Right singular vectors</li> </ul> <p>ML Application:</p> <ul> <li>Generalization of eigendecomposition to non-square matrices</li> <li>Used in recommender systems (matrix factorization)</li> <li>Basis for PCA and low-rank approximations</li> </ul>"},{"location":"theory/#2-calculus-and-optimization","title":"2. Calculus and Optimization","text":"<p>Calculus enables us to find optimal model parameters by following gradients of loss functions.</p>"},{"location":"theory/#21-derivatives","title":"2.1 Derivatives","text":"<p>The derivative measures the rate of change of a function:</p> <pre><code>f'(x) = lim(h\u21920) [f(x + h) - f(x)] / h\n</code></pre> <p>Common Derivatives:</p> <ul> <li>d/dx [x\u207f] = nx\u207f\u207b\u00b9</li> <li>d/dx [e\u02e3] = e\u02e3</li> <li>d/dx [ln(x)] = 1/x</li> <li>d/dx [sin(x)] = cos(x)</li> </ul> <p>ML Application: Derivatives tell us how much the loss changes when we adjust a parameter.</p>"},{"location":"theory/#22-partial-derivatives-and-gradients","title":"2.2 Partial Derivatives and Gradients","text":"<p>For multivariate functions f(x\u2081, x\u2082, ..., x\u2099), the partial derivative with respect to x\u1d62 is:</p> <pre><code>\u2202f/\u2202x\u1d62 = lim(h\u21920) [f(..., x\u1d62 + h, ...) - f(..., x\u1d62, ...)] / h\n</code></pre> <p>The gradient is the vector of all partial derivatives:</p> <pre><code>\u2207f = [\u2202f/\u2202x\u2081, \u2202f/\u2202x\u2082, ..., \u2202f/\u2202x\u2099]\u1d40\n</code></pre> <p>Properties:</p> <ul> <li>Gradient points in the direction of steepest ascent</li> <li>Negative gradient points toward steepest descent</li> <li>Gradient is zero at local minima, maxima, and saddle points</li> </ul> <p>ML Application: Gradient descent algorithms follow -\u2207L(\u03b8) to minimize the loss function L with respect to parameters \u03b8.</p>"},{"location":"theory/#23-chain-rule","title":"2.3 Chain Rule","text":"<p>The chain rule enables differentiation of composite functions:</p> <pre><code>d/dx [f(g(x))] = f'(g(x)) \u00b7 g'(x)\n</code></pre> <p>For multivariate compositions:</p> <pre><code>\u2202z/\u2202x = (\u2202z/\u2202y)(\u2202y/\u2202x)\n</code></pre> <p>ML Application: Backpropagation in neural networks repeatedly applies the chain rule to compute gradients through multiple layers.</p>"},{"location":"theory/#24-gradient-descent","title":"2.4 Gradient Descent","text":"<p>Gradient Descent is an iterative optimization algorithm:</p> <pre><code>\u03b8\u209c\u208a\u2081 = \u03b8\u209c - \u03b1\u2207L(\u03b8\u209c)\n</code></pre> <p>where:</p> <ul> <li>\u03b8\u209c: Parameters at iteration t</li> <li>\u03b1: Learning rate (step size)</li> <li>\u2207L(\u03b8\u209c): Gradient of loss function at \u03b8\u209c</li> </ul> <p>Variants:</p> <ol> <li>Batch Gradient Descent: Uses entire dataset to compute gradient</li> </ol> <pre><code>\u2207L(\u03b8) = (1/n)\u03a3\u1d62 \u2207L\u1d62(\u03b8)\n</code></pre> <ol> <li>Stochastic Gradient Descent (SGD): Uses single random example</li> </ol> <pre><code>\u03b8\u209c\u208a\u2081 = \u03b8\u209c - \u03b1\u2207L\u1d62(\u03b8\u209c)\n</code></pre> <ol> <li>Mini-batch Gradient Descent: Uses small batch of examples</li> </ol> <pre><code>\u2207L(\u03b8) = (1/|B|)\u03a3\u1d62\u2208B \u2207L\u1d62(\u03b8)\n</code></pre> <p>Advanced Optimizers:</p> <ul> <li>Momentum: Accumulates velocity to smooth updates</li> </ul> <pre><code>v\u209c = \u03b2v\u209c\u208b\u2081 + \u2207L(\u03b8\u209c)\n\u03b8\u209c\u208a\u2081 = \u03b8\u209c - \u03b1v\u209c\n</code></pre> <ul> <li>Adam: Adaptive learning rates with momentum</li> </ul> <pre><code>m\u209c = \u03b2\u2081m\u209c\u208b\u2081 + (1-\u03b2\u2081)\u2207L(\u03b8\u209c)\nv\u209c = \u03b2\u2082v\u209c\u208b\u2081 + (1-\u03b2\u2082)(\u2207L(\u03b8\u209c))\u00b2\n\u03b8\u209c\u208a\u2081 = \u03b8\u209c - \u03b1\u00b7m\u209c/(\u221av\u209c + \u03b5)\n</code></pre>"},{"location":"theory/#25-convexity","title":"2.5 Convexity","text":"<p>A function f is convex if:</p> <pre><code>f(\u03bbx + (1-\u03bb)y) \u2264 \u03bbf(x) + (1-\u03bb)f(y) for all \u03bb \u2208 [0,1]\n</code></pre> <p>Properties:</p> <ul> <li>Local minimum is also global minimum</li> <li>Gradient descent converges to global optimum</li> <li>Examples: Linear regression, logistic regression (with convex regularization)</li> </ul> <p>ML Application: Many ML loss functions are convex, guaranteeing convergence to optimal solutions.</p>"},{"location":"theory/#3-probability-and-statistics","title":"3. Probability and Statistics","text":"<p>Probability theory provides the foundation for reasoning about uncertainty in data and predictions.</p>"},{"location":"theory/#31-probability-fundamentals","title":"3.1 Probability Fundamentals","text":"<p>Probability Axioms:</p> <ol> <li>P(A) \u2265 0 for any event A</li> <li>P(\u03a9) = 1 (total probability)</li> <li>P(A \u222a B) = P(A) + P(B) if A and B are disjoint</li> </ol> <p>Conditional Probability:</p> <pre><code>P(A|B) = P(A \u2229 B) / P(B)\n</code></pre> <p>Bayes' Theorem:</p> <pre><code>P(A|B) = P(B|A)P(A) / P(B)\n</code></pre> <p>ML Application: Bayesian inference, Naive Bayes classifiers, probabilistic modeling.</p>"},{"location":"theory/#32-random-variables-and-distributions","title":"3.2 Random Variables and Distributions","text":"<p>A random variable X maps outcomes to real numbers.</p> <p>Expectation (Mean):</p> <pre><code>E[X] = \u03a3\u1d62 x\u1d62P(X = x\u1d62)  (discrete)\nE[X] = \u222b xf(x)dx        (continuous)\n</code></pre> <p>Variance:</p> <pre><code>Var(X) = E[(X - E[X])\u00b2] = E[X\u00b2] - (E[X])\u00b2\n</code></pre> <p>Common Distributions:</p> <ol> <li>Normal (Gaussian):</li> </ol> <pre><code>f(x) = (1/\u221a(2\u03c0\u03c3\u00b2))exp(-(x-\u03bc)\u00b2/(2\u03c3\u00b2))\n</code></pre> <ul> <li> <p>Used in: Linear regression errors, Gaussian processes</p> </li> <li> <p>Bernoulli:</p> </li> </ul> <pre><code>P(X = 1) = p, P(X = 0) = 1-p\n</code></pre> <ul> <li> <p>Used in: Binary classification</p> </li> <li> <p>Multinomial:</p> </li> </ul> <pre><code>P(X = k) = p\u2096 where \u03a3\u2096 p\u2096 = 1\n</code></pre> <ul> <li> <p>Used in: Multi-class classification</p> </li> <li> <p>Poisson:</p> </li> </ul> <pre><code>P(X = k) = (\u03bb\u1d4fe\u207b\u1d4f) / k!\n</code></pre> <ul> <li>Used in: Count data (arrivals, events)</li> </ul>"},{"location":"theory/#33-maximum-likelihood-estimation-mle","title":"3.3 Maximum Likelihood Estimation (MLE)","text":"<p>Find parameters \u03b8 that maximize the likelihood of observed data:</p> <pre><code>\u03b8\u0302_MLE = argmax_\u03b8 L(\u03b8|x) = argmax_\u03b8 P(x|\u03b8)\n</code></pre> <p>Often use log-likelihood for convenience:</p> <pre><code>\u03b8\u0302_MLE = argmax_\u03b8 log L(\u03b8|x) = argmax_\u03b8 \u03a3\u1d62 log P(x\u1d62|\u03b8)\n</code></pre> <p>ML Application: Training most ML models is MLE under specific distributional assumptions.</p>"},{"location":"theory/#34-statistical-inference","title":"3.4 Statistical Inference","text":"<p>Hypothesis Testing:</p> <ul> <li>Null hypothesis H\u2080 vs. alternative H\u2081</li> <li>p-value: Probability of observing data at least as extreme as observed, given H\u2080</li> <li>Significance level \u03b1 (typically 0.05)</li> <li>Reject H\u2080 if p-value &lt; \u03b1</li> </ul> <p>Confidence Intervals:</p> <pre><code>CI = \u03b8\u0302 \u00b1 z_(\u03b1/2) \u00b7 SE(\u03b8\u0302)\n</code></pre> <p>where SE is the standard error.</p> <p>ML Application: Evaluating model significance, A/B testing, feature selection.</p>"},{"location":"theory/#4-supervised-learning-theory","title":"4. Supervised Learning Theory","text":"<p>Supervised learning involves learning a mapping from inputs X to outputs Y given labeled training data.</p>"},{"location":"theory/#41-problem-formulation","title":"4.1 Problem Formulation","text":"<p>Training Data: {(x\u2081, y\u2081), (x\u2082, y\u2082), ..., (x\u2099, y\u2099)}</p> <p>Goal: Learn function f: X \u2192 Y such that f(x) \u2248 y</p> <p>Types:</p> <ul> <li>Regression: Y is continuous (e.g., price, temperature)</li> <li>Classification: Y is discrete (e.g., spam/not spam, digit 0-9)</li> </ul>"},{"location":"theory/#42-linear-regression","title":"4.2 Linear Regression","text":"<p>Model:</p> <pre><code>\u0177 = \u03b8\u2080 + \u03b8\u2081x\u2081 + \u03b8\u2082x\u2082 + ... + \u03b8\u209ax\u209a = \u03b8\u1d40x\n</code></pre> <p>Loss Function (Mean Squared Error):</p> <pre><code>L(\u03b8) = (1/n)\u03a3\u1d62 (y\u1d62 - \u03b8\u1d40x\u1d62)\u00b2\n</code></pre> <p>Normal Equation (Closed Form):</p> <pre><code>\u03b8\u0302 = (X\u1d40X)\u207b\u00b9X\u1d40y\n</code></pre> <p>Gradient:</p> <pre><code>\u2207L(\u03b8) = (2/n)X\u1d40(X\u03b8 - y)\n</code></pre> <p>Assumptions:</p> <ol> <li>Linearity: True relationship is linear</li> <li>Independence: Observations are independent</li> <li>Homoscedasticity: Constant variance of errors</li> <li>Normality: Errors follow normal distribution</li> </ol> <p>Coefficient Interpretation:</p> <ul> <li>\u03b8\u2c7c represents the change in y for a unit change in x\u2c7c, holding other features constant</li> </ul>"},{"location":"theory/#43-logistic-regression","title":"4.3 Logistic Regression","text":"<p>Model (Binary Classification):</p> <pre><code>P(y = 1|x) = \u03c3(\u03b8\u1d40x) = 1 / (1 + e\u207b\u1dbf\u1d40\u02e3)\n</code></pre> <p>where \u03c3 is the sigmoid function.</p> <p>Decision Boundary:</p> <pre><code>Predict y = 1 if P(y = 1|x) \u2265 0.5\n\u27fa \u03b8\u1d40x \u2265 0\n</code></pre> <p>Loss Function (Binary Cross-Entropy):</p> <pre><code>L(\u03b8) = -(1/n)\u03a3\u1d62 [y\u1d62 log(\u0177\u1d62) + (1-y\u1d62)log(1-\u0177\u1d62)]\n</code></pre> <p>Gradient:</p> <pre><code>\u2207L(\u03b8) = (1/n)X\u1d40(\u03c3(X\u03b8) - y)\n</code></pre> <p>Multi-class Extension (Softmax):</p> <pre><code>P(y = k|x) = exp(\u03b8\u2096\u1d40x) / \u03a3\u2c7c exp(\u03b8\u2c7c\u1d40x)\n</code></pre>"},{"location":"theory/#44-support-vector-machines-svm","title":"4.4 Support Vector Machines (SVM)","text":"<p>Goal: Find hyperplane that maximally separates classes with largest margin.</p> <p>Hard Margin SVM:</p> <pre><code>minimize (1/2)||w||\u00b2\nsubject to y\u1d62(w\u1d40x\u1d62 + b) \u2265 1 for all i\n</code></pre> <p>Soft Margin SVM (with slack variables \u03be\u1d62):</p> <pre><code>minimize (1/2)||w||\u00b2 + C\u00b7\u03a3\u1d62 \u03be\u1d62\nsubject to y\u1d62(w\u1d40x\u1d62 + b) \u2265 1 - \u03be\u1d62, \u03be\u1d62 \u2265 0\n</code></pre> <p>Kernel Trick: Map data to higher dimensions using kernel functions:</p> <ul> <li>Linear: K(x, x') = x\u1d40x'</li> <li>Polynomial: K(x, x') = (x\u1d40x' + c)\u1d48</li> <li>RBF (Gaussian): K(x, x') = exp(-\u03b3||x - x'||\u00b2)</li> </ul> <p>ML Application: Effective for high-dimensional spaces, memory-efficient (uses support vectors only).</p>"},{"location":"theory/#45-decision-trees","title":"4.5 Decision Trees","text":"<p>Splitting Criterion:</p> <p>For regression, use Mean Squared Error:</p> <pre><code>MSE = (1/n)\u03a3\u1d62 (y\u1d62 - \u0233)\u00b2\n</code></pre> <p>For classification, use Gini Impurity or Entropy:</p> <pre><code>Gini = 1 - \u03a3\u2096 p\u2096\u00b2\nEntropy = -\u03a3\u2096 p\u2096 log(p\u2096)\n</code></pre> <p>Information Gain:</p> <pre><code>IG = H(parent) - \u03a3(|child|/|parent|)H(child)\n</code></pre> <p>Algorithm:</p> <ol> <li>Start with all data at root</li> <li>For each feature, find best split that maximizes information gain</li> <li>Recursively split until stopping criterion met</li> <li>Assign leaf prediction (majority class or mean value)</li> </ol> <p>Advantages:</p> <ul> <li>Interpretable</li> <li>Handles non-linear relationships</li> <li>No feature scaling needed</li> </ul> <p>Disadvantages:</p> <ul> <li>Prone to overfitting</li> <li>Unstable (small data changes cause different trees)</li> </ul>"},{"location":"theory/#5-neural-networks-and-deep-learning","title":"5. Neural Networks and Deep Learning","text":"<p>Neural networks are compositions of simple non-linear functions that can approximate complex mappings.</p>"},{"location":"theory/#51-perceptron","title":"5.1 Perceptron","text":"<p>The basic building block:</p> <pre><code>z = \u03a3\u1d62 w\u1d62x\u1d62 + b = w\u1d40x + b\na = g(z)\n</code></pre> <p>where:</p> <ul> <li>w: weights</li> <li>b: bias</li> <li>g: activation function</li> <li>a: output activation</li> </ul>"},{"location":"theory/#52-activation-functions","title":"5.2 Activation Functions","text":"<p>Purpose: Introduce non-linearity (without them, deep networks collapse to linear models).</p> <p>Common Activations:</p> <ol> <li>Sigmoid:</li> </ol> <pre><code>\u03c3(z) = 1 / (1 + e\u207b\u1dbb)\n\u03c3'(z) = \u03c3(z)(1 - \u03c3(z))\n</code></pre> <ul> <li>Range: (0, 1)</li> <li> <p>Issues: Vanishing gradients, not zero-centered</p> </li> <li> <p>Tanh:</p> </li> </ul> <pre><code>tanh(z) = (e\u1dbb - e\u207b\u1dbb) / (e\u1dbb + e\u207b\u1dbb)\ntanh'(z) = 1 - tanh\u00b2(z)\n</code></pre> <ul> <li>Range: (-1, 1)</li> <li> <p>Zero-centered, but still vanishing gradients</p> </li> <li> <p>ReLU (Rectified Linear Unit):</p> </li> </ul> <pre><code>ReLU(z) = max(0, z)\nReLU'(z) = 1 if z &gt; 0, else 0\n</code></pre> <ul> <li>Most popular for hidden layers</li> <li>Computationally efficient</li> <li> <p>Issues: \"Dying ReLU\" (neurons can permanently output 0)</p> </li> <li> <p>Leaky ReLU:</p> </li> </ul> <pre><code>LeakyReLU(z) = max(\u03b1z, z) where \u03b1 \u2248 0.01\n</code></pre> <ul> <li> <p>Prevents dying ReLU problem</p> </li> <li> <p>Softmax (Output Layer for Multi-class):</p> </li> </ul> <pre><code>softmax(z\u1d62) = exp(z\u1d62) / \u03a3\u2c7c exp(z\u2c7c)\n</code></pre>"},{"location":"theory/#53-multi-layer-perceptron-mlp","title":"5.3 Multi-Layer Perceptron (MLP)","text":"<p>Architecture:</p> <pre><code>Layer 1: a\u207d\u00b9\u207e = g\u207d\u00b9\u207e(W\u207d\u00b9\u207ex + b\u207d\u00b9\u207e)\nLayer 2: a\u207d\u00b2\u207e = g\u207d\u00b2\u207e(W\u207d\u00b2\u207ea\u207d\u00b9\u207e + b\u207d\u00b2\u207e)\n...\nOutput: \u0177 = a\u207d\u1d38\u207e\n</code></pre> <p>Universal Approximation Theorem: A neural network with a single hidden layer and enough neurons can approximate any continuous function arbitrarily well.</p>"},{"location":"theory/#54-backpropagation","title":"5.4 Backpropagation","text":"<p>Forward Pass: Compute predictions layer by layer.</p> <p>Backward Pass: Compute gradients using chain rule.</p> <p>Output Layer Gradient:</p> <pre><code>\u03b4\u207d\u1d38\u207e = \u2202L/\u2202z\u207d\u1d38\u207e = (a\u207d\u1d38\u207e - y) \u2299 g'(z\u207d\u1d38\u207e)\n</code></pre> <p>Hidden Layer Gradient:</p> <pre><code>\u03b4\u207d\u02e1\u207e = (W\u207d\u02e1\u207a\u00b9\u207e\u1d40\u03b4\u207d\u02e1\u207a\u00b9\u207e) \u2299 g'(z\u207d\u02e1\u207e)\n</code></pre> <p>Parameter Gradients:</p> <pre><code>\u2202L/\u2202W\u207d\u02e1\u207e = \u03b4\u207d\u02e1\u207ea\u207d\u02e1\u207b\u00b9\u207e\u1d40\n\u2202L/\u2202b\u207d\u02e1\u207e = \u03b4\u207d\u02e1\u207e\n</code></pre> <p>Update Rule:</p> <pre><code>W\u207d\u02e1\u207e \u2190 W\u207d\u02e1\u207e - \u03b1(\u2202L/\u2202W\u207d\u02e1\u207e)\nb\u207d\u02e1\u207e \u2190 b\u207d\u02e1\u207e - \u03b1(\u2202L/\u2202b\u207d\u02e1\u207e)\n</code></pre>"},{"location":"theory/#55-common-loss-functions","title":"5.5 Common Loss Functions","text":"<p>Regression:</p> <ol> <li>Mean Squared Error (MSE):</li> </ol> <pre><code>L = (1/n)\u03a3\u1d62 (y\u1d62 - \u0177\u1d62)\u00b2\n</code></pre> <ol> <li>Mean Absolute Error (MAE):</li> </ol> <pre><code>L = (1/n)\u03a3\u1d62 |y\u1d62 - \u0177\u1d62|\n</code></pre> <p>Classification:</p> <ol> <li>Binary Cross-Entropy:</li> </ol> <pre><code>L = -(1/n)\u03a3\u1d62 [y\u1d62 log(\u0177\u1d62) + (1-y\u1d62)log(1-\u0177\u1d62)]\n</code></pre> <ol> <li>Categorical Cross-Entropy:</li> </ol> <pre><code>L = -(1/n)\u03a3\u1d62 \u03a3\u2096 y\u1d62\u2096 log(\u0177\u1d62\u2096)\n</code></pre>"},{"location":"theory/#56-initialization-strategies","title":"5.6 Initialization Strategies","text":"<p>Problem: Poor initialization can cause vanishing/exploding gradients.</p> <p>Xavier/Glorot Initialization:</p> <pre><code>W ~ U[-\u221a(6/(n\u1d62\u2099 + n\u2092\u1d64\u209c)), \u221a(6/(n\u1d62\u2099 + n\u2092\u1d64\u209c))]\n</code></pre> <p>He Initialization (for ReLU):</p> <pre><code>W ~ N(0, 2/n\u1d62\u2099)\n</code></pre>"},{"location":"theory/#57-batch-normalization","title":"5.7 Batch Normalization","text":"<p>Normalize activations within each mini-batch:</p> <pre><code>x\u0302\u1d62 = (x\u1d62 - \u03bc_B) / \u221a(\u03c3\u00b2_B + \u03b5)\ny\u1d62 = \u03b3x\u0302\u1d62 + \u03b2\n</code></pre> <p>Benefits:</p> <ul> <li>Faster training</li> <li>Higher learning rates possible</li> <li>Reduces sensitivity to initialization</li> <li>Acts as regularization</li> </ul>"},{"location":"theory/#6-regularization-techniques","title":"6. Regularization Techniques","text":"<p>Regularization prevents overfitting by constraining model complexity.</p>"},{"location":"theory/#61-l2-regularization-ridge","title":"6.1 L2 Regularization (Ridge)","text":"<p>Modified Loss:</p> <pre><code>L_ridge(\u03b8) = L(\u03b8) + \u03bb||\u03b8||\u2082\u00b2 = L(\u03b8) + \u03bb\u03a3\u2c7c \u03b8\u2c7c\u00b2\n</code></pre> <p>Effect:</p> <ul> <li>Shrinks coefficients toward zero</li> <li>Never exactly zero (all features retained)</li> <li>Closed-form solution exists for linear regression:   <pre><code>\u03b8\u0302 = (X\u1d40X + \u03bbI)\u207b\u00b9X\u1d40y\n</code></pre></li> </ul> <p>Bayesian Interpretation: Equivalent to placing Gaussian prior on parameters.</p>"},{"location":"theory/#62-l1-regularization-lasso","title":"6.2 L1 Regularization (Lasso)","text":"<p>Modified Loss:</p> <pre><code>L_lasso(\u03b8) = L(\u03b8) + \u03bb||\u03b8||\u2081 = L(\u03b8) + \u03bb\u03a3\u2c7c |\u03b8\u2c7c|\n</code></pre> <p>Effect:</p> <ul> <li>Produces sparse solutions (many coefficients exactly zero)</li> <li>Automatic feature selection</li> <li>No closed-form solution (requires iterative methods)</li> </ul> <p>Bayesian Interpretation: Equivalent to placing Laplace prior on parameters.</p>"},{"location":"theory/#63-elastic-net","title":"6.3 Elastic Net","text":"<p>Combines L1 and L2:</p> <pre><code>L_elastic(\u03b8) = L(\u03b8) + \u03bb\u2081||\u03b8||\u2081 + \u03bb\u2082||\u03b8||\u2082\u00b2\n</code></pre> <p>Advantages:</p> <ul> <li>Sparse solutions like Lasso</li> <li>Better than Lasso when features are correlated</li> <li>More stable than Lasso</li> </ul>"},{"location":"theory/#64-dropout","title":"6.4 Dropout","text":"<p>Training: Randomly set a fraction p of neurons to zero in each iteration.</p> <p>Effect:</p> <ul> <li>Prevents co-adaptation of neurons</li> <li>Approximates ensemble of exponentially many networks</li> <li>Acts as strong regularizer</li> </ul> <p>Inference: Scale activations by (1-p) or train with inverted dropout.</p>"},{"location":"theory/#65-early-stopping","title":"6.5 Early Stopping","text":"<p>Method: Monitor validation loss during training; stop when it starts increasing.</p> <p>Equivalent to: Implicit regularization by limiting model capacity to fit training data.</p>"},{"location":"theory/#66-data-augmentation","title":"6.6 Data Augmentation","text":"<p>Increase effective dataset size by applying transformations:</p> <ul> <li>Images: rotation, flipping, cropping, color jitter</li> <li>Text: synonym replacement, back-translation</li> <li>Time series: jittering, scaling, window slicing</li> </ul> <p>Effect: Regularizes by exposing model to variations.</p>"},{"location":"theory/#7-ensemble-methods","title":"7. Ensemble Methods","text":"<p>Ensemble methods combine multiple models to achieve better performance than individual models.</p>"},{"location":"theory/#71-bagging-bootstrap-aggregating","title":"7.1 Bagging (Bootstrap Aggregating)","text":"<p>Algorithm:</p> <ol> <li>Create m bootstrap samples (sample with replacement)</li> <li>Train model on each bootstrap sample</li> <li>Aggregate predictions (vote for classification, average for regression)</li> </ol> <p>Prediction:</p> <pre><code>\u0177 = (1/m)\u03a3\u1d62 f\u1d62(x)  (regression)\n\u0177 = mode{f\u2081(x), ..., f\u2098(x)}  (classification)\n</code></pre> <p>Random Forest: Bagging with decision trees + random feature selection at each split.</p> <p>Feature Importance:</p> <pre><code>Importance(x\u2c7c) = \u03a3_trees \u03a3_splits \u0394Impurity(x\u2c7c)\n</code></pre> <p>Out-of-Bag (OOB) Error:</p> <ul> <li>Each tree uses ~63% of data for training</li> <li>Remaining ~37% used for validation (OOB samples)</li> <li>OOB error provides unbiased performance estimate</li> </ul>"},{"location":"theory/#72-boosting","title":"7.2 Boosting","text":"<p>Idea: Train models sequentially, each correcting errors of previous ones.</p> <p>AdaBoost (Adaptive Boosting):</p> <ol> <li>Initialize weights: w\u1d62 = 1/n for all i</li> <li>For t = 1 to T:</li> <li>Train classifier f\u209c on weighted data</li> <li>Compute error: \u03b5\u209c = \u03a3\u1d62 w\u1d62 \u00b7 I(y\u1d62 \u2260 f\u209c(x\u1d62))</li> <li>Compute weight: \u03b1\u209c = (\u00bd)log((1-\u03b5\u209c)/\u03b5\u209c)</li> <li>Update weights: w\u1d62 \u2190 w\u1d62 \u00b7 exp(-\u03b1\u209cy\u1d62f\u209c(x\u1d62))</li> <li>Normalize weights</li> <li>Final prediction: F(x) = sign(\u03a3\u209c \u03b1\u209cf\u209c(x))</li> </ol> <p>Gradient Boosting:</p> <p>General framework for boosting any differentiable loss function.</p> <ol> <li>Initialize: F\u2080(x) = argmin_\u03b3 \u03a3\u1d62 L(y\u1d62, \u03b3)</li> <li>For t = 1 to T:</li> <li>Compute pseudo-residuals:      <pre><code>r\u1d62\u209c = -\u2202L(y\u1d62, F(x\u1d62))/\u2202F(x\u1d62) |_(F=F\u209c\u208b\u2081)\n</code></pre></li> <li>Fit base learner h\u209c to residuals</li> <li>Find optimal step size:      <pre><code>\u03b3\u209c = argmin_\u03b3 \u03a3\u1d62 L(y\u1d62, F\u209c\u208b\u2081(x\u1d62) + \u03b3h\u209c(x\u1d62))\n</code></pre></li> <li>Update: F\u209c(x) = F\u209c\u208b\u2081(x) + \u03b3\u209ch\u209c(x)</li> </ol> <p>XGBoost (Extreme Gradient Boosting):</p> <p>Adds regularization to gradient boosting:</p> <pre><code>Obj = \u03a3\u1d62 L(y\u1d62, \u0177\u1d62) + \u03a3\u209c \u03a9(f\u209c)\n</code></pre> <p>where \u03a9(f) = \u03b3T + (\u00bd)\u03bb||w||\u00b2 (T = number of leaves)</p>"},{"location":"theory/#73-stacking-stacked-generalization","title":"7.3 Stacking (Stacked Generalization)","text":"<p>Two-Level Architecture:</p> <p>Level 0 (Base Models):</p> <ul> <li>Train diverse models (e.g., random forest, SVM, neural network)</li> <li>Generate out-of-fold predictions on training set</li> </ul> <p>Level 1 (Meta-Model):</p> <ul> <li>Train on base model predictions as features</li> <li>Learn optimal way to combine base model predictions</li> </ul> <p>Algorithm:</p> <ol> <li>Split data into K folds</li> <li>For each base model:</li> <li>Train on K-1 folds, predict on held-out fold (repeat K times)</li> <li>Train on full training set, predict on test set</li> <li>Train meta-model on out-of-fold predictions</li> <li>Meta-model predicts on base model test predictions</li> </ol>"},{"location":"theory/#8-unsupervised-learning","title":"8. Unsupervised Learning","text":"<p>Unsupervised learning discovers patterns in unlabeled data.</p>"},{"location":"theory/#81-k-means-clustering","title":"8.1 K-Means Clustering","text":"<p>Objective: Minimize within-cluster variance:</p> <pre><code>minimize \u03a3\u2096 \u03a3_{x\u1d62\u2208C\u2096} ||x\u1d62 - \u03bc\u2096||\u00b2\n</code></pre> <p>Algorithm (Lloyd's Algorithm):</p> <ol> <li>Initialize k centroids randomly</li> <li>Repeat until convergence:</li> <li>Assignment: Assign each point to nearest centroid      <pre><code>C\u2096 = {x\u1d62 : ||x\u1d62 - \u03bc\u2096|| \u2264 ||x\u1d62 - \u03bc\u2c7c|| for all j}\n</code></pre></li> <li>Update: Recompute centroids      <pre><code>\u03bc\u2096 = (1/|C\u2096|)\u03a3_{x\u1d62\u2208C\u2096} x\u1d62\n</code></pre></li> </ol> <p>Choosing k:</p> <ul> <li>Elbow method: Plot inertia vs k, look for \"elbow\"</li> <li>Silhouette score: Measures cluster cohesion and separation</li> <li>Domain knowledge</li> </ul> <p>Limitations:</p> <ul> <li>Assumes spherical clusters</li> <li>Sensitive to initialization</li> <li>Must specify k in advance</li> </ul>"},{"location":"theory/#82-hierarchical-clustering","title":"8.2 Hierarchical Clustering","text":"<p>Agglomerative (Bottom-Up):</p> <ol> <li>Start with each point as its own cluster</li> <li>Repeatedly merge closest clusters</li> <li>Stop when all points in one cluster</li> </ol> <p>Linkage Criteria:</p> <ul> <li>Single: min distance between any pair</li> <li>Complete: max distance between any pair</li> <li>Average: average distance between all pairs</li> <li>Ward: minimize within-cluster variance</li> </ul> <p>Dendrogram: Tree diagram showing merge sequence.</p>"},{"location":"theory/#83-dbscan-density-based-spatial-clustering","title":"8.3 DBSCAN (Density-Based Spatial Clustering)","text":"<p>Parameters:</p> <ul> <li>\u03b5: Maximum radius of neighborhood</li> <li>MinPts: Minimum points to form dense region</li> </ul> <p>Point Types:</p> <ul> <li>Core Point: Has \u2265 MinPts neighbors within \u03b5</li> <li>Border Point: In neighborhood of core point but not core itself</li> <li>Noise Point: Neither core nor border</li> </ul> <p>Advantages:</p> <ul> <li>Discovers clusters of arbitrary shape</li> <li>Robust to outliers</li> <li>No need to specify number of clusters</li> </ul>"},{"location":"theory/#84-principal-component-analysis-pca","title":"8.4 Principal Component Analysis (PCA)","text":"<p>Goal: Find orthogonal directions of maximum variance.</p> <p>Mathematical Formulation:</p> <ol> <li>Center data: X_centered = X - X\u0304</li> <li>Compute covariance matrix:    <pre><code>\u03a3 = (1/n)X_centered\u1d40X_centered\n</code></pre></li> <li>Compute eigendecomposition:    <pre><code>\u03a3 = Q\u039bQ\u1d40\n</code></pre></li> <li>Principal components are eigenvectors (columns of Q)</li> <li>Project data:    <pre><code>Z = X_centered \u00b7 Q[:, :k]  (keep top k components)\n</code></pre></li> </ol> <p>Variance Explained:</p> <pre><code>Variance explained by PC_j = \u03bb\u2c7c / \u03a3\u1d62 \u03bb\u1d62\n</code></pre> <p>Applications:</p> <ul> <li>Dimensionality reduction</li> <li>Data visualization (project to 2D/3D)</li> <li>Noise reduction</li> <li>Feature extraction</li> </ul>"},{"location":"theory/#85-t-sne-t-distributed-stochastic-neighbor-embedding","title":"8.5 t-SNE (t-Distributed Stochastic Neighbor Embedding)","text":"<p>Goal: Preserve local structure in low-dimensional embedding.</p> <p>Algorithm:</p> <ol> <li>Compute pairwise similarities in high-dimensional space:</li> </ol> <pre><code>p_{j|i} = exp(-||x\u1d62 - x\u2c7c||\u00b2/(2\u03c3\u1d62\u00b2)) / \u03a3\u2096\u2260\u1d62 exp(-||x\u1d62 - x\u2096||\u00b2/(2\u03c3\u1d62\u00b2))\n</code></pre> <ol> <li>Compute symmetrized similarities:</li> </ol> <pre><code>p\u1d62\u2c7c = (p_{j|i} + p_{i|j}) / (2n)\n</code></pre> <ol> <li>Define similarities in low-dimensional space using t-distribution:</li> </ol> <pre><code>q\u1d62\u2c7c = (1 + ||y\u1d62 - y\u2c7c||\u00b2)\u207b\u00b9 / \u03a3\u2096\u2260\u02e1 (1 + ||y\u2096 - y\u02e1||\u00b2)\u207b\u00b9\n</code></pre> <ol> <li>Minimize KL divergence using gradient descent:</li> </ol> <pre><code>KL(P||Q) = \u03a3\u1d62 \u03a3\u2c7c p\u1d62\u2c7c log(p\u1d62\u2c7c/q\u1d62\u2c7c)\n</code></pre> <p>Properties:</p> <ul> <li>Excellent for visualization</li> <li>Non-parametric (cannot embed new points directly)</li> <li>Stochastic (different runs give different results)</li> </ul>"},{"location":"theory/#86-umap-uniform-manifold-approximation-and-projection","title":"8.6 UMAP (Uniform Manifold Approximation and Projection)","text":"<p>Similar to t-SNE but:</p> <ul> <li>Faster</li> <li>Better preserves global structure</li> <li>Can project new points</li> <li>Based on manifold theory and topological data analysis</li> </ul>"},{"location":"theory/#9-probabilistic-modeling","title":"9. Probabilistic Modeling","text":"<p>Probabilistic models explicitly represent uncertainty using probability distributions.</p>"},{"location":"theory/#91-naive-bayes","title":"9.1 Naive Bayes","text":"<p>Assumption: Features are conditionally independent given class:</p> <pre><code>P(x\u2081, ..., x\u209a | y) = \u220f\u2c7c P(x\u2c7c | y)\n</code></pre> <p>Classification Rule:</p> <pre><code>\u0177 = argmax_y P(y) \u220f\u2c7c P(x\u2c7c | y)\n</code></pre> <p>Variants:</p> <ul> <li>Gaussian Naive Bayes: P(x\u2c7c|y) ~ N(\u03bc\u2c7c\u1d67, \u03c3\u00b2\u2c7c\u1d67)</li> <li>Multinomial Naive Bayes: For count data (e.g., text)</li> <li>Bernoulli Naive Bayes: For binary features</li> </ul> <p>Advantages:</p> <ul> <li>Fast training and prediction</li> <li>Works well with high-dimensional data</li> <li>Often surprisingly effective despite strong independence assumption</li> </ul>"},{"location":"theory/#92-gaussian-mixture-models-gmm","title":"9.2 Gaussian Mixture Models (GMM)","text":"<p>Model: Data generated from mixture of k Gaussian distributions:</p> <pre><code>p(x) = \u03a3\u2096 \u03c0\u2096 N(x | \u03bc\u2096, \u03a3\u2096)\n</code></pre> <p>where \u03c0\u2096 are mixing coefficients (\u03a3\u2096 \u03c0\u2096 = 1).</p> <p>Latent Variable: z\u1d62 indicates which Gaussian generated x\u1d62.</p> <p>Complete-Data Likelihood:</p> <pre><code>p(x, z | \u03b8) = \u220f\u1d62 \u220f\u2096 [\u03c0\u2096 N(x\u1d62 | \u03bc\u2096, \u03a3\u2096)]^{z\u1d62\u2096}\n</code></pre>"},{"location":"theory/#93-expectation-maximization-em-algorithm","title":"9.3 Expectation-Maximization (EM) Algorithm","text":"<p>General Framework for Latent Variable Models:</p> <p>Maximize: p(x | \u03b8) = \u03a3_z p(x, z | \u03b8)</p> <p>E-Step: Compute posterior over latents:</p> <pre><code>Q(\u03b8 | \u03b8\u207d\u1d57\u207e) = E_{z|x,\u03b8\u207d\u1d57\u207e}[log p(x, z | \u03b8)]\n</code></pre> <p>M-Step: Maximize expected complete-data log-likelihood:</p> <pre><code>\u03b8\u207d\u1d57\u207a\u00b9\u207e = argmax_\u03b8 Q(\u03b8 | \u03b8\u207d\u1d57\u207e)\n</code></pre> <p>For GMM:</p> <p>E-Step (Responsibility):</p> <pre><code>\u03b3\u1d62\u2096 = \u03c0\u2096 N(x\u1d62 | \u03bc\u2096, \u03a3\u2096) / \u03a3\u2c7c \u03c0\u2c7c N(x\u1d62 | \u03bc\u2c7c, \u03a3\u2c7c)\n</code></pre> <p>M-Step:</p> <pre><code>N\u2096 = \u03a3\u1d62 \u03b3\u1d62\u2096\n\u03c0\u2096 = N\u2096 / n\n\u03bc\u2096 = (1/N\u2096)\u03a3\u1d62 \u03b3\u1d62\u2096x\u1d62\n\u03a3\u2096 = (1/N\u2096)\u03a3\u1d62 \u03b3\u1d62\u2096(x\u1d62 - \u03bc\u2096)(x\u1d62 - \u03bc\u2096)\u1d40\n</code></pre>"},{"location":"theory/#94-hidden-markov-models-hmm","title":"9.4 Hidden Markov Models (HMM)","text":"<p>Components:</p> <ul> <li>States: S = {s\u2081, ..., s\u2099}</li> <li>Observations: O = {o\u2081, ..., o\u2098}</li> <li>Initial probabilities: \u03c0</li> <li>Transition probabilities: A (a\u1d62\u2c7c = P(s\u209c\u208a\u2081=j | s\u209c=i))</li> <li>Emission probabilities: B (b\u2c7c(o\u209c) = P(o\u209c | s\u209c=j))</li> </ul> <p>Forward Algorithm (Compute Likelihood):</p> <pre><code>\u03b1_t(i) = P(o\u2081, ..., o\u209c, s\u209c=i)\n\u03b1_t(j) = [\u03a3\u1d62 \u03b1_{t-1}(i)a\u1d62\u2c7c]b\u2c7c(o\u209c)\np(O) = \u03a3\u1d62 \u03b1_T(i)\n</code></pre> <p>Viterbi Algorithm (Most Likely State Sequence):</p> <pre><code>\u03b4_t(i) = max_{s\u2081,...,s_{t-1}} P(s\u2081,...,s_{t-1},s\u209c=i,o\u2081,...,o\u209c)\n\u03b4_t(j) = [max\u1d62 \u03b4_{t-1}(i)a\u1d62\u2c7c]b\u2c7c(o\u209c)\n</code></pre> <p>Applications:</p> <ul> <li>Speech recognition</li> <li>Part-of-speech tagging</li> <li>Gene prediction</li> </ul>"},{"location":"theory/#95-bayesian-inference","title":"9.5 Bayesian Inference","text":"<p>Bayes' Theorem for Parameters:</p> <pre><code>p(\u03b8 | D) = p(D | \u03b8)p(\u03b8) / p(D)\n</code></pre> <p>where:</p> <ul> <li>p(\u03b8): Prior distribution</li> <li>p(D | \u03b8): Likelihood</li> <li>p(\u03b8 | D): Posterior distribution</li> <li>p(D): Marginal likelihood (evidence)</li> </ul> <p>Posterior Predictive Distribution:</p> <pre><code>p(x_new | D) = \u222b p(x_new | \u03b8)p(\u03b8 | D)d\u03b8\n</code></pre> <p>Maximum A Posteriori (MAP) Estimation:</p> <pre><code>\u03b8\u0302_MAP = argmax_\u03b8 p(\u03b8 | D) = argmax_\u03b8 [p(D | \u03b8)p(\u03b8)]\n</code></pre> <p>Conjugate Priors: Prior and posterior have same functional form.</p> <p>Examples:</p> <ul> <li>Beta prior + Binomial likelihood \u2192 Beta posterior</li> <li>Dirichlet prior + Multinomial likelihood \u2192 Dirichlet posterior</li> <li>Normal prior + Normal likelihood \u2192 Normal posterior</li> </ul>"},{"location":"theory/#10-time-series-analysis","title":"10. Time Series Analysis","text":"<p>Time series data consists of observations ordered in time, exhibiting temporal dependencies.</p>"},{"location":"theory/#101-time-series-components","title":"10.1 Time Series Components","text":"<p>Decomposition:</p> <pre><code>Y_t = T_t + S_t + R_t\n</code></pre> <p>where:</p> <ul> <li>T_t: Trend (long-term direction)</li> <li>S_t: Seasonality (periodic patterns)</li> <li>R_t: Residual (irregular component)</li> </ul> <p>Multiplicative Model:</p> <pre><code>Y_t = T_t \u00d7 S_t \u00d7 R_t\n</code></pre>"},{"location":"theory/#102-stationarity","title":"10.2 Stationarity","text":"<p>A time series is stationary if:</p> <ol> <li>Constant mean: E[Y_t] = \u03bc</li> <li>Constant variance: Var(Y_t) = \u03c3\u00b2</li> <li>Autocovariance depends only on lag: Cov(Y_t, Y_{t+k}) = \u03b3_k</li> </ol> <p>Tests:</p> <ul> <li>Augmented Dickey-Fuller (ADF): Tests for unit root</li> <li>KPSS: Tests for stationarity</li> </ul> <p>Making Non-Stationary Data Stationary:</p> <ul> <li>Differencing: \u0394Y_t = Y_t - Y_{t-1}</li> <li>Log transformation: log(Y_t)</li> <li>Seasonal differencing: Y_t - Y_{t-s}</li> </ul>"},{"location":"theory/#103-autocorrelation","title":"10.3 Autocorrelation","text":"<p>Autocorrelation Function (ACF):</p> <pre><code>\u03c1_k = Cov(Y_t, Y_{t-k}) / Var(Y_t)\n</code></pre> <p>Partial Autocorrelation Function (PACF):</p> <p>Correlation between Y_t and Y_{t-k} after removing linear dependence on Y_{t-1}, ..., Y_{t-k+1}.</p>"},{"location":"theory/#104-arima-models","title":"10.4 ARIMA Models","text":"<p>AR(p) - Autoregressive:</p> <pre><code>Y_t = c + \u03a3\u1d62 \u03c6\u1d62Y_{t-i} + \u03b5_t\n</code></pre> <p>MA(q) - Moving Average:</p> <pre><code>Y_t = \u03bc + \u03b5_t + \u03a3\u1d62 \u03b8\u1d62\u03b5_{t-i}\n</code></pre> <p>ARMA(p,q):</p> <pre><code>Y_t = c + \u03a3\u1d62 \u03c6\u1d62Y_{t-i} + \u03a3\u2c7c \u03b8\u2c7c\u03b5_{t-j} + \u03b5_t\n</code></pre> <p>ARIMA(p,d,q): ARMA on differenced series (d differences)</p> <p>Model Selection:</p> <ul> <li>Use ACF/PACF plots</li> <li>Use information criteria (AIC, BIC)</li> <li>Grid search over (p,d,q)</li> </ul>"},{"location":"theory/#105-seasonal-arima-sarimax","title":"10.5 Seasonal ARIMA (SARIMAX)","text":"<p>SARIMA(p,d,q)(P,D,Q)_s:</p> <p>Combines non-seasonal and seasonal components:</p> <pre><code>\u03c6(B)\u03a6(B\u02e2)(1-B)\u1d48(1-B\u02e2)\u1d30Y_t = \u03b8(B)\u0398(B\u02e2)\u03b5_t\n</code></pre> <p>where:</p> <ul> <li>(p,d,q): Non-seasonal orders</li> <li>(P,D,Q): Seasonal orders</li> <li>s: Seasonal period</li> <li>B: Backshift operator (BY_t = Y_{t-1})</li> </ul> <p>SARIMAX: SARIMA + exogenous variables</p>"},{"location":"theory/#106-exponential-smoothing","title":"10.6 Exponential Smoothing","text":"<p>Simple Exponential Smoothing:</p> <pre><code>\u0177_{t+1} = \u03b1y_t + (1-\u03b1)\u0177_t\n</code></pre> <p>Holt's Linear Trend:</p> <pre><code>Level: \u2113_t = \u03b1y_t + (1-\u03b1)(\u2113_{t-1} + b_{t-1})\nTrend: b_t = \u03b2(\u2113_t - \u2113_{t-1}) + (1-\u03b2)b_{t-1}\nForecast: \u0177_{t+h} = \u2113_t + hb_t\n</code></pre> <p>Holt-Winters (Seasonal):</p> <p>Adds seasonal component:</p> <pre><code>Level: \u2113_t = \u03b1(y_t - s_{t-m}) + (1-\u03b1)(\u2113_{t-1} + b_{t-1})\nTrend: b_t = \u03b2(\u2113_t - \u2113_{t-1}) + (1-\u03b2)b_{t-1}\nSeason: s_t = \u03b3(y_t - \u2113_t) + (1-\u03b3)s_{t-m}\nForecast: \u0177_{t+h} = \u2113_t + hb_t + s_{t+h-m}\n</code></pre>"},{"location":"theory/#107-prophet","title":"10.7 Prophet","text":"<p>Additive Model:</p> <pre><code>y(t) = g(t) + s(t) + h(t) + \u03b5_t\n</code></pre> <p>where:</p> <ul> <li>g(t): Piecewise linear or logistic growth</li> <li>s(t): Fourier series for seasonality</li> <li>h(t): Holiday effects</li> <li>\u03b5_t: Error term</li> </ul> <p>Advantages:</p> <ul> <li>Handles missing data and outliers</li> <li>Intuitive hyperparameters</li> <li>Automatic seasonality detection</li> <li>Works well with irregular data</li> </ul>"},{"location":"theory/#108-evaluation-metrics","title":"10.8 Evaluation Metrics","text":"<p>Mean Absolute Error (MAE):</p> <pre><code>MAE = (1/n)\u03a3|y_t - \u0177_t|\n</code></pre> <p>Root Mean Squared Error (RMSE):</p> <pre><code>RMSE = \u221a[(1/n)\u03a3(y_t - \u0177_t)\u00b2]\n</code></pre> <p>Mean Absolute Percentage Error (MAPE):</p> <pre><code>MAPE = (100/n)\u03a3|y_t - \u0177_t|/|y_t|\n</code></pre> <p>Symmetric MAPE (sMAPE):</p> <pre><code>sMAPE = (100/n)\u03a3 2|y_t - \u0177_t|/(|y_t| + |\u0177_t|)\n</code></pre>"},{"location":"theory/#11-advanced-deep-learning","title":"11. Advanced Deep Learning","text":""},{"location":"theory/#111-convolutional-neural-networks-cnns","title":"11.1 Convolutional Neural Networks (CNNs)","text":"<p>Convolution Operation:</p> <pre><code>(f * g)[i,j] = \u03a3\u2098\u03a3\u2099 f[m,n]g[i-m, j-n]\n</code></pre> <p>Convolutional Layer:</p> <pre><code>Output[i,j,k] = \u03c3(\u03a3\u2098 \u03a3\u2099 \u03a3_c Input[i+m, j+n, c] \u00d7 Kernel[m,n,c,k] + Bias[k])\n</code></pre> <p>Key Concepts:</p> <ul> <li>Filters/Kernels: Small matrices that slide over input</li> <li>Feature Maps: Outputs of applying filters</li> <li>Stride: Step size for sliding kernel</li> <li>Padding: Adding zeros around input to control output size</li> </ul> <p>Pooling Layers:</p> <p>Max Pooling:</p> <pre><code>Output[i,j] = max_{m,n \u2208 window} Input[i\u00d7s+m, j\u00d7s+n]\n</code></pre> <p>Average Pooling:</p> <pre><code>Output[i,j] = mean_{m,n \u2208 window} Input[i\u00d7s+m, j\u00d7s+n]\n</code></pre> <p>Architecture Patterns:</p> <pre><code>Input \u2192 [Conv \u2192 ReLU \u2192 Pool]\u00d7N \u2192 Flatten \u2192 [FC \u2192 ReLU]\u00d7M \u2192 Output\n</code></pre> <p>Classic Architectures:</p> <ul> <li>LeNet-5: Early CNN for digit recognition</li> <li>AlexNet: First deep CNN to win ImageNet (2012)</li> <li>VGGNet: Very deep with small 3\u00d73 filters</li> <li>ResNet: Skip connections enable training very deep networks</li> <li>Inception: Multi-scale processing with parallel paths</li> </ul>"},{"location":"theory/#112-recurrent-neural-networks-rnns","title":"11.2 Recurrent Neural Networks (RNNs)","text":"<p>Vanilla RNN:</p> <pre><code>h_t = tanh(W_hh h_{t-1} + W_xh x_t + b_h)\ny_t = W_hy h_t + b_y\n</code></pre> <p>Problems:</p> <ul> <li>Vanishing gradients: Gradients shrink exponentially with sequence length</li> <li>Exploding gradients: Gradients grow exponentially</li> </ul> <p>Gradient Flow:</p> <pre><code>\u2202L/\u2202h_0 = \u2202L/\u2202h_T \u00d7 \u220f_{t=1}^T \u2202h_t/\u2202h_{t-1}\n</code></pre>"},{"location":"theory/#113-long-short-term-memory-lstm","title":"11.3 Long Short-Term Memory (LSTM)","text":"<p>Gates:</p> <pre><code>Forget gate: f_t = \u03c3(W_f[h_{t-1}, x_t] + b_f)\nInput gate:  i_t = \u03c3(W_i[h_{t-1}, x_t] + b_i)\nOutput gate: o_t = \u03c3(W_o[h_{t-1}, x_t] + b_o)\n\nCell candidate: C\u0303_t = tanh(W_C[h_{t-1}, x_t] + b_C)\nCell state: C_t = f_t \u2299 C_{t-1} + i_t \u2299 C\u0303_t\nHidden state: h_t = o_t \u2299 tanh(C_t)\n</code></pre> <p>Key Innovation: Cell state C_t provides uninterrupted gradient flow.</p>"},{"location":"theory/#114-gated-recurrent-unit-gru","title":"11.4 Gated Recurrent Unit (GRU)","text":"<p>Simplified LSTM with fewer parameters:</p> <pre><code>Reset gate: r_t = \u03c3(W_r[h_{t-1}, x_t])\nUpdate gate: z_t = \u03c3(W_z[h_{t-1}, x_t])\n\nCandidate: h\u0303_t = tanh(W[r_t \u2299 h_{t-1}, x_t])\nHidden state: h_t = (1 - z_t) \u2299 h_{t-1} + z_t \u2299 h\u0303_t\n</code></pre>"},{"location":"theory/#115-attention-mechanism","title":"11.5 Attention Mechanism","text":"<p>Problem: Fixed-length context vector is information bottleneck.</p> <p>Solution: Allow decoder to \"attend\" to different parts of input.</p> <p>Attention Scores:</p> <pre><code>e_{t,i} = score(h_t, h\u0304_i) = h_t^T W h\u0304_i  (or dot product, or MLP)\n</code></pre> <p>Attention Weights (via softmax):</p> <pre><code>\u03b1_{t,i} = exp(e_{t,i}) / \u03a3\u2c7c exp(e_{t,j})\n</code></pre> <p>Context Vector:</p> <pre><code>c_t = \u03a3\u1d62 \u03b1_{t,i} h\u0304_i\n</code></pre> <p>Decoder with Attention:</p> <pre><code>h_t = RNN(h_{t-1}, [y_{t-1}; c_t])\n</code></pre>"},{"location":"theory/#116-transformers","title":"11.6 Transformers","text":"<p>Core Idea: Replace recurrence with self-attention.</p> <p>Scaled Dot-Product Attention:</p> <pre><code>Attention(Q, K, V) = softmax(QK^T / \u221ad_k)V\n</code></pre> <p>where:</p> <ul> <li>Q: Query matrix</li> <li>K: Key matrix</li> <li>V: Value matrix</li> <li>d_k: Key dimension (for scaling)</li> </ul> <p>Multi-Head Attention:</p> <pre><code>MultiHead(Q,K,V) = Concat(head\u2081, ..., head_h)W^O\n\nwhere head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\n</code></pre> <p>Transformer Block:</p> <pre><code>1. Multi-head self-attention\n2. Add &amp; Normalize (residual connection)\n3. Feed-forward network (2-layer MLP)\n4. Add &amp; Normalize\n</code></pre> <p>Positional Encoding:</p> <p>Since no recurrence, inject position information:</p> <pre><code>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n</code></pre> <p>Advantages:</p> <ul> <li>Parallelizable (unlike RNNs)</li> <li>Captures long-range dependencies</li> <li>State-of-the-art on many tasks</li> </ul> <p>Popular Models:</p> <ul> <li>BERT: Bidirectional encoder for understanding</li> <li>GPT: Autoregressive decoder for generation</li> <li>T5: Encoder-decoder for text-to-text tasks</li> <li>Vision Transformer (ViT): Applies transformers to images</li> </ul>"},{"location":"theory/#117-generative-models","title":"11.7 Generative Models","text":"<p>Variational Autoencoders (VAE):</p> <p>Encoder (Recognition Model):</p> <pre><code>q_\u03c6(z|x) \u2248 p(z|x)\n</code></pre> <p>Decoder (Generative Model):</p> <pre><code>p_\u03b8(x|z)\n</code></pre> <p>Loss (ELBO - Evidence Lower Bound):</p> <pre><code>L = E_q[log p_\u03b8(x|z)] - KL(q_\u03c6(z|x) || p(z))\n</code></pre> <p>Reconstruction loss + Regularization term</p> <p>Reparameterization Trick:</p> <pre><code>z = \u03bc + \u03c3 \u2299 \u03b5 where \u03b5 ~ N(0, I)\n</code></pre> <p>Allows backpropagation through sampling.</p> <p>Generative Adversarial Networks (GANs):</p> <p>Two Networks:</p> <ul> <li>Generator G: Generates fake samples from noise</li> <li>Discriminator D: Distinguishes real from fake</li> </ul> <p>Minimax Game:</p> <pre><code>min_G max_D E_x[log D(x)] + E_z[log(1 - D(G(z)))]\n</code></pre> <p>Training:</p> <ol> <li>Train D to maximize discriminator accuracy</li> <li>Train G to maximize discriminator's mistakes</li> </ol> <p>Challenges:</p> <ul> <li>Mode collapse</li> <li>Training instability</li> <li>Vanishing gradients</li> </ul> <p>Improvements:</p> <ul> <li>Wasserstein GAN (WGAN)</li> <li>Spectral normalization</li> <li>Progressive growing</li> </ul>"},{"location":"theory/#12-model-evaluation-and-selection","title":"12. Model Evaluation and Selection","text":""},{"location":"theory/#121-bias-variance-tradeoff","title":"12.1 Bias-Variance Tradeoff","text":"<p>Prediction Error Decomposition:</p> <pre><code>E[(y - \u0177)\u00b2] = Bias\u00b2 + Variance + Irreducible Error\n</code></pre> <p>Bias: Error from approximating complex functions with simple models</p> <ul> <li>High bias \u2192 Underfitting</li> <li>Examples: Linear model for non-linear data</li> </ul> <p>Variance: Error from sensitivity to training set fluctuations</p> <ul> <li>High variance \u2192 Overfitting</li> <li>Examples: Deep decision tree, high-degree polynomial</li> </ul> <p>Tradeoff: Increasing model complexity reduces bias but increases variance.</p>"},{"location":"theory/#122-cross-validation","title":"12.2 Cross-Validation","text":"<p>K-Fold Cross-Validation:</p> <ol> <li>Split data into K folds</li> <li>For k = 1 to K:</li> <li>Train on K-1 folds</li> <li>Validate on fold k</li> <li>Average performance across folds</li> </ol> <p>Stratified K-Fold: Maintain class proportions in each fold.</p> <p>Leave-One-Out (LOO): K = n (expensive but low-bias estimate)</p> <p>Time Series Split: Respect temporal ordering</p> <pre><code>Fold 1: Train[1:100], Test[101:150]\nFold 2: Train[1:150], Test[151:200]\n...\n</code></pre>"},{"location":"theory/#123-classification-metrics","title":"12.3 Classification Metrics","text":"<p>Confusion Matrix:</p> <pre><code>                Predicted\n              Pos    Neg\nActual  Pos   TP     FN\n        Neg   FP     TN\n</code></pre> <p>Metrics:</p> <p>Accuracy:</p> <pre><code>Accuracy = (TP + TN) / (TP + TN + FP + FN)\n</code></pre> <p>Precision (Positive Predictive Value):</p> <pre><code>Precision = TP / (TP + FP)\n</code></pre> <p>Recall (Sensitivity, True Positive Rate):</p> <pre><code>Recall = TP / (TP + FN)\n</code></pre> <p>F1 Score (Harmonic Mean of Precision and Recall):</p> <pre><code>F1 = 2 \u00d7 (Precision \u00d7 Recall) / (Precision + Recall)\n</code></pre> <p>Specificity (True Negative Rate):</p> <pre><code>Specificity = TN / (TN + FP)\n</code></pre>"},{"location":"theory/#124-roc-and-auc","title":"12.4 ROC and AUC","text":"<p>ROC Curve: Plot TPR vs FPR at various thresholds</p> <pre><code>TPR = TP / (TP + FN)  (y-axis)\nFPR = FP / (FP + TN)  (x-axis)\n</code></pre> <p>AUC (Area Under Curve):</p> <ul> <li>Perfect classifier: AUC = 1.0</li> <li>Random classifier: AUC = 0.5</li> <li>Interpretation: Probability that model ranks random positive higher than random negative</li> </ul> <p>Precision-Recall Curve: Better for imbalanced datasets</p>"},{"location":"theory/#125-regression-metrics","title":"12.5 Regression Metrics","text":"<p>Mean Squared Error (MSE):</p> <pre><code>MSE = (1/n)\u03a3(y\u1d62 - \u0177\u1d62)\u00b2\n</code></pre> <p>Root Mean Squared Error (RMSE):</p> <pre><code>RMSE = \u221aMSE\n</code></pre> <p>Mean Absolute Error (MAE):</p> <pre><code>MAE = (1/n)\u03a3|y\u1d62 - \u0177\u1d62|\n</code></pre> <p>R\u00b2 Score (Coefficient of Determination):</p> <pre><code>R\u00b2 = 1 - SS_res/SS_tot = 1 - \u03a3(y\u1d62 - \u0177\u1d62)\u00b2/\u03a3(y\u1d62 - \u0233)\u00b2\n</code></pre> <ul> <li>R\u00b2 = 1: Perfect predictions</li> <li>R\u00b2 = 0: Model as good as mean baseline</li> <li>R\u00b2 &lt; 0: Model worse than mean baseline</li> </ul> <p>Adjusted R\u00b2:</p> <pre><code>R\u00b2_adj = 1 - (1 - R\u00b2)(n - 1)/(n - p - 1)\n</code></pre> <p>Penalizes adding features that don't improve fit.</p>"},{"location":"theory/#126-model-selection-criteria","title":"12.6 Model Selection Criteria","text":"<p>Akaike Information Criterion (AIC):</p> <pre><code>AIC = 2k - 2log(L\u0302)\n</code></pre> <p>where k = number of parameters, L\u0302 = maximum likelihood</p> <p>Bayesian Information Criterion (BIC):</p> <pre><code>BIC = k\u00b7log(n) - 2log(L\u0302)\n</code></pre> <p>Stronger penalty for model complexity than AIC.</p> <p>Principle: Lower is better. Balance fit quality with model simplicity.</p>"},{"location":"theory/#127-hyperparameter-tuning","title":"12.7 Hyperparameter Tuning","text":"<p>Grid Search:</p> <ul> <li>Define grid of hyperparameter values</li> <li>Evaluate all combinations via cross-validation</li> <li>Select combination with best performance</li> </ul> <p>Random Search:</p> <ul> <li>Sample hyperparameter combinations randomly</li> <li>Often more efficient than grid search</li> <li>Better explores high-dimensional spaces</li> </ul> <p>Bayesian Optimization:</p> <ul> <li>Build probabilistic model of objective function</li> <li>Use model to select promising hyperparameters</li> <li>Update model with new evaluations</li> <li>More sample-efficient than grid/random search</li> </ul> <p>Learning Curves:</p> <p>Plot training and validation performance vs:</p> <ul> <li>Training set size: Diagnose bias/variance</li> <li>Training iterations: Detect convergence/overfitting</li> </ul>"},{"location":"theory/#conclusion","title":"Conclusion","text":"<p>This theory document covers the mathematical foundations underlying the ML curriculum from Days 38-67. Each concept builds on previous ones, forming a coherent framework for understanding modern machine learning:</p> <ol> <li>Linear Algebra provides the language for representing data and transformations</li> <li>Calculus enables optimization through gradient-based methods</li> <li>Probability allows reasoning about uncertainty</li> <li>Supervised Learning applies these foundations to learn mappings from inputs to outputs</li> <li>Deep Learning extends linear models with non-linear compositions</li> <li>Regularization prevents overfitting through various constraint mechanisms</li> <li>Ensembles combine models for robust predictions</li> <li>Unsupervised Learning discovers structure in unlabeled data</li> <li>Probabilistic Models explicitly represent uncertainty</li> <li>Time Series handles temporal dependencies</li> <li>Advanced Deep Learning tackles complex patterns in images, sequences, and text</li> <li>Evaluation ensures models generalize beyond training data</li> </ol> <p>For practical implementations of these concepts, refer to the corresponding lesson days (38-67) in the curriculum. Each lesson provides executable code, worked examples, and exercises to solidify understanding.</p>"},{"location":"theory/#further-reading","title":"Further Reading","text":""},{"location":"theory/#books","title":"Books","text":"<ul> <li> <p>Linear Algebra:</p> </li> <li> <p>Gilbert Strang, Linear Algebra and Its Applications</p> </li> <li> <p>Sheldon Axler, Linear Algebra Done Right</p> </li> <li> <p>Calculus and Optimization:</p> </li> <li> <p>Stephen Boyd &amp; Lieven Vandenberghe, Convex Optimization</p> </li> <li> <p>Jorge Nocedal &amp; Stephen Wright, Numerical Optimization</p> </li> <li> <p>Probability and Statistics:</p> </li> <li> <p>Larry Wasserman, All of Statistics</p> </li> <li> <p>Dimitri Bertsekas &amp; John Tsitsiklis, Introduction to Probability</p> </li> <li> <p>Machine Learning:</p> </li> <li> <p>Christopher Bishop, Pattern Recognition and Machine Learning</p> </li> <li>Trevor Hastie et al., The Elements of Statistical Learning</li> <li> <p>Kevin Murphy, Probabilistic Machine Learning: An Introduction</p> </li> <li> <p>Deep Learning:</p> </li> <li> <p>Ian Goodfellow et al., Deep Learning</p> </li> <li>Fran\u00e7ois Chollet, Deep Learning with Python</li> </ul>"},{"location":"theory/#online-resources","title":"Online Resources","text":"<ul> <li>Stanford CS229: Machine Learning (Andrew Ng)</li> <li>Stanford CS231n: Convolutional Neural Networks</li> <li>Stanford CS224n: Natural Language Processing with Deep Learning</li> <li>Fast.ai: Practical Deep Learning for Coders</li> <li>Distill.pub: Interactive visual explanations</li> </ul>"},{"location":"theory/#papers","title":"Papers","text":"<ul> <li>Rumelhart et al. (1986): \"Learning representations by back-propagating errors\"</li> <li>Hochreiter &amp; Schmidhuber (1997): \"Long Short-Term Memory\"</li> <li>Vaswani et al. (2017): \"Attention Is All You Need\"</li> <li>Kingma &amp; Ba (2014): \"Adam: A Method for Stochastic Optimization\"</li> <li>He et al. (2015): \"Deep Residual Learning for Image Recognition\"</li> </ul> <p>This theory document is maintained as part of the Coding for MBA curriculum. For questions or suggestions, please open an issue on the GitHub repository.</p>"},{"location":"website-improvements/","title":"Website Enhancement Recommendations","text":""},{"location":"website-improvements/#executive-summary","title":"Executive Summary","text":"<p>This document outlines comprehensive recommendations for enhancing the Coding-For-MBA GitHub Pages website to make it more dynamic, accessible, and interactive. The primary focus is on embedding Jupyter notebooks with runtime capabilities, improving accessibility, and creating a more engaging learning experience.</p>"},{"location":"website-improvements/#current-state-analysis","title":"Current State Analysis","text":""},{"location":"website-improvements/#strengths","title":"Strengths","text":"<ul> <li>\u2705 MkDocs with Material theme provides excellent foundation</li> <li>\u2705 Good accessibility CSS already in place</li> <li>\u2705 Automated build pipeline with GitHub Actions</li> <li>\u2705 Notebook conversion system exists</li> <li>\u2705 Clean, organized lesson structure</li> </ul>"},{"location":"website-improvements/#limitations","title":"Limitations","text":"<ul> <li>\u274c Static notebooks only - no interactive runtime</li> <li>\u274c Users cannot execute code directly on the website</li> <li>\u274c Limited dynamic features</li> <li>\u274c No progress tracking or personalization</li> <li>\u274c Basic search functionality</li> </ul>"},{"location":"website-improvements/#recommended-improvements","title":"Recommended Improvements","text":""},{"location":"website-improvements/#1-interactive-notebook-runtime-high-priority","title":"1. Interactive Notebook Runtime (HIGH PRIORITY)","text":""},{"location":"website-improvements/#11-jupyterlite-integration","title":"1.1 JupyterLite Integration","text":"<p>What: JupyterLite is a lightweight JupyterLab distribution that runs entirely in the browser using WebAssembly.</p> <p>Benefits: - \u2705 No server infrastructure required - \u2705 Works perfectly with GitHub Pages - \u2705 Full Jupyter experience in the browser - \u2705 Pre-installed packages available via Pyodide - \u2705 Can load notebooks from the repository</p> <p>Implementation Steps:</p> <ol> <li> <p>Install JupyterLite:    <pre><code>pip install jupyterlite-core jupyterlite-pyodide-kernel\n</code></pre></p> </li> <li> <p>Create JupyterLite configuration (<code>jupyter_lite_config.json</code>):    <pre><code>{\n  \"LiteBuildConfig\": {\n    \"contents\": [\"Day_*/\"],\n    \"ignore_sys_prefix\": [\"share\"]\n  },\n  \"PipliteAddon\": {\n    \"piplite_urls\": [\n      \"https://pypi.org/simple\"\n    ]\n  }\n}\n</code></pre></p> </li> <li> <p>Add build step to documentation workflow:    <pre><code>- name: Build JupyterLite\n  run: |\n    jupyter lite build --contents . --output-dir site/jupyterlite\n</code></pre></p> </li> <li> <p>Add launch buttons to lesson pages:    <pre><code>[\ud83d\ude80 Launch Interactive Notebook](../jupyterlite/lab?path=Day_01_Introduction/introduction.ipynb){ .md-button .md-button--primary }\n</code></pre></p> </li> </ol> <p>Estimated Effort: 4-6 hours Impact: HIGH - Enables full interactive coding experience</p>"},{"location":"website-improvements/#12-thebe-integration","title":"1.2 Thebe Integration","text":"<p>What: Thebe makes static HTML pages interactive by connecting code cells to a Jupyter kernel (via Binder).</p> <p>Benefits: - \u2705 Makes existing code blocks executable - \u2705 Less intrusive than full JupyterLite - \u2705 Good for simple examples</p> <p>Implementation:</p> <ol> <li> <p>Add Thebe JavaScript to MkDocs extra_javascript:    <pre><code>extra_javascript:\n  - https://unpkg.com/thebe@latest/lib/index.js\n  - javascripts/thebe-config.js\n</code></pre></p> </li> <li> <p>Create Thebe configuration (<code>docs/javascripts/thebe-config.js</code>):    <pre><code>thebelab.on(\"ready\", function() {\n  thebelab.bootstrap({\n    requestKernel: true,\n    binderOptions: {\n      repo: \"saint2706/Coding-For-MBA\",\n      ref: \"main\",\n    },\n    kernelOptions: {\n      name: \"python3\",\n      kernelName: \"python3\",\n    },\n    selector: \"div.executable\",\n  });\n});\n</code></pre></p> </li> <li> <p>Mark code blocks as executable in markdown:    <pre><code>&lt;div class=\"executable\" data-executable=\"true\"&gt;\n```python\nprint(\"Hello, World!\")\n</code></pre>     ```</p> </li> </ol> <p>Estimated Effort: 2-3 hours Impact: MEDIUM - Good for inline examples</p>"},{"location":"website-improvements/#13-binder-integration","title":"1.3 Binder Integration","text":"<p>What: Add \"Launch Binder\" badges to open notebooks in a cloud environment.</p> <p>Benefits: - \u2705 Full computing environment - \u2705 No browser limitations - \u2705 Can handle heavy computations</p> <p>Implementation:</p> <ol> <li>Create Binder configuration files:</li> <li><code>environment.yml</code> or <code>requirements.txt</code> at root</li> <li> <p><code>.binder/</code> directory with configuration</p> </li> <li> <p>Add Binder badges to lesson pages:    <pre><code>[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/saint2706/Coding-For-MBA/main?filepath=Day_01_Introduction/introduction.ipynb)\n</code></pre></p> </li> <li> <p>Update build_docs.py to automatically add badges:    <pre><code>def _add_binder_badge(day_dir: Path, notebook: Path) -&gt; str:\n    filepath = notebook.relative_to(ROOT)\n    url = f\"https://mybinder.org/v2/gh/{repo_slug}/main?filepath={filepath}\"\n    return f\"[![Launch Binder](https://mybinder.org/badge_logo.svg)]({url})\"\n</code></pre></p> </li> </ol> <p>Estimated Effort: 1-2 hours Impact: MEDIUM - Good fallback option</p>"},{"location":"website-improvements/#2-enhanced-interactivity","title":"2. Enhanced Interactivity","text":""},{"location":"website-improvements/#21-pyodide-based-interactive-code-widgets","title":"2.1 Pyodide-based Interactive Code Widgets","text":"<p>What: Embed lightweight Python interpreters directly in pages using Pyodide.</p> <p>Implementation:</p> <ol> <li> <p>Add Pyodide loader (<code>docs/javascripts/pyodide-console.js</code>):    <pre><code>async function initPyodide() {\n  let pyodide = await loadPyodide();\n  return pyodide;\n}\n\nasync function runPython(code) {\n  const pyodide = await initPyodide();\n  try {\n    let result = await pyodide.runPython(code);\n    return result;\n  } catch (err) {\n    return `Error: ${err}`;\n  }\n}\n</code></pre></p> </li> <li> <p>Create interactive code widget component:    <pre><code>&lt;div class=\"pyodide-console\"&gt;\n  &lt;textarea id=\"code-input\" rows=\"5\"&gt;&lt;/textarea&gt;\n  &lt;button onclick=\"executeCode()\"&gt;Run Code&lt;/button&gt;\n  &lt;pre id=\"code-output\"&gt;&lt;/pre&gt;\n&lt;/div&gt;\n</code></pre></p> </li> </ol> <p>Estimated Effort: 4-5 hours Impact: HIGH - Great for quick examples</p>"},{"location":"website-improvements/#22-interactive-quizzes-and-exercises","title":"2.2 Interactive Quizzes and Exercises","text":"<p>What: Add self-assessment tools to lessons.</p> <p>Implementation:</p> <ol> <li> <p>Use MkDocs plugins:    <pre><code>plugins:\n  - quiz:\n      questions_dir: docs/quizzes\n</code></pre></p> </li> <li> <p>Create quiz files in YAML format:    <pre><code>questions:\n  - question: \"What is the output of `print(2 + 2)`?\"\n    answers:\n      - \"4\"\n      - \"22\"\n      - \"Error\"\n    correct: 0\n    explanation: \"Python evaluates 2 + 2 as 4\"\n</code></pre></p> </li> </ol> <p>Estimated Effort: 6-8 hours (including content creation) Impact: MEDIUM - Improves learning outcomes</p>"},{"location":"website-improvements/#23-progress-tracking","title":"2.3 Progress Tracking","text":"<p>What: Track lesson completion using localStorage.</p> <p>Implementation:</p> <ol> <li> <p>Add progress tracking JavaScript (<code>docs/javascripts/progress-tracker.js</code>):    <pre><code>class ProgressTracker {\n  constructor() {\n    this.storageKey = 'coding-mba-progress';\n  }\n\n  markComplete(lessonId) {\n    let progress = this.getProgress();\n    progress[lessonId] = {\n      completed: true,\n      timestamp: Date.now()\n    };\n    localStorage.setItem(this.storageKey, JSON.stringify(progress));\n    this.updateUI();\n  }\n\n  getProgress() {\n    return JSON.parse(localStorage.getItem(this.storageKey) || '{}');\n  }\n\n  calculatePercentage() {\n    const total = 67; // Total lessons\n    const completed = Object.keys(this.getProgress()).length;\n    return Math.round((completed / total) * 100);\n  }\n}\n</code></pre></p> </li> <li> <p>Add UI elements:    <pre><code>&lt;div class=\"progress-badge\"&gt;\n  &lt;span id=\"progress-percentage\"&gt;0%&lt;/span&gt; Complete\n&lt;/div&gt;\n&lt;button onclick=\"progressTracker.markComplete(currentLesson)\"&gt;\n  \u2713 Mark as Complete\n&lt;/button&gt;\n</code></pre></p> </li> </ol> <p>Estimated Effort: 3-4 hours Impact: MEDIUM - Motivates learners</p>"},{"location":"website-improvements/#3-accessibility-enhancements","title":"3. Accessibility Enhancements","text":""},{"location":"website-improvements/#31-aria-labels-and-semantic-html","title":"3.1 ARIA Labels and Semantic HTML","text":"<p>Current: Good foundation exists in <code>extra.css</code></p> <p>Enhancements:</p> <ol> <li> <p>Add ARIA labels to interactive elements:    <pre><code>&lt;button \n  aria-label=\"Run Python code in browser\"\n  aria-describedby=\"code-output\"&gt;\n  Run Code\n&lt;/button&gt;\n</code></pre></p> </li> <li> <p>Enhance keyboard navigation:    <pre><code>// Add keyboard shortcuts\ndocument.addEventListener('keydown', (e) =&gt; {\n  if (e.ctrlKey &amp;&amp; e.key === 'Enter') {\n    executeCode();\n  }\n});\n</code></pre></p> </li> <li> <p>Add skip links for interactive components:    <pre><code>&lt;a href=\"#main-content\" class=\"skip-link\"&gt;\n  Skip to main content\n&lt;/a&gt;\n&lt;a href=\"#interactive-console\" class=\"skip-link\"&gt;\n  Skip to interactive console\n&lt;/a&gt;\n</code></pre></p> </li> </ol> <p>Estimated Effort: 2-3 hours Impact: HIGH - Legal compliance and inclusivity</p>"},{"location":"website-improvements/#32-screen-reader-support","title":"3.2 Screen Reader Support","text":"<p>Implementation:</p> <ol> <li> <p>Add live regions for dynamic content:    <pre><code>&lt;div role=\"status\" aria-live=\"polite\" aria-atomic=\"true\" id=\"code-status\"&gt;\n  &lt;!-- Status messages appear here --&gt;\n&lt;/div&gt;\n</code></pre></p> </li> <li> <p>Add descriptive labels to code blocks:    <pre><code>```python title=\"Example: Calculate Business Metrics\" aria-label=\"Python code example showing calculation of revenue metrics\"\nrevenue = 1000000\ncosts = 750000\nprofit = revenue - costs\n</code></pre>    ```</p> </li> </ol> <p>Estimated Effort: 2 hours Impact: MEDIUM - Improves screen reader experience</p>"},{"location":"website-improvements/#33-color-contrast-and-visual-improvements","title":"3.3 Color Contrast and Visual Improvements","text":"<p>Enhancements:</p> <ol> <li> <p>Add high-contrast theme option:    <pre><code>theme:\n  palette:\n    - scheme: slate-high-contrast\n      primary: blue\n      accent: yellow\n</code></pre></p> </li> <li> <p>Improve code block contrast in <code>extra.css</code>:    <pre><code>/* High contrast mode for code blocks */\n@media (prefers-contrast: high) {\n  .md-typeset code {\n    background-color: #000;\n    color: #fff;\n    border: 2px solid #fff;\n  }\n}\n</code></pre></p> </li> </ol> <p>Estimated Effort: 1-2 hours Impact: MEDIUM - Helps visually impaired users</p>"},{"location":"website-improvements/#4-search-and-discovery-improvements","title":"4. Search and Discovery Improvements","text":""},{"location":"website-improvements/#41-enhanced-search-with-notebook-content","title":"4.1 Enhanced Search with Notebook Content","text":"<p>What: Index notebook content in search.</p> <p>Implementation:</p> <ol> <li> <p>Add search plugin with custom configuration:    <pre><code>plugins:\n  - search:\n      lang: en\n      separator: '[\\s\\-\\.]+'\n      indexing: 'full'\n      prebuild_index: true\n</code></pre></p> </li> <li> <p>Index notebook cells during build:    <pre><code>def extract_searchable_content(notebook_path: Path) -&gt; str:\n    \"\"\"Extract text from notebook cells for search indexing.\"\"\"\n    with open(notebook_path) as f:\n        nb = nbformat.read(f, as_version=4)\n\n    content = []\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            content.append(cell.source)\n        elif cell.cell_type == 'code':\n            content.append(f\"```python\\n{cell.source}\\n```\")\n\n    return \"\\n\\n\".join(content)\n</code></pre></p> </li> </ol> <p>Estimated Effort: 3-4 hours Impact: HIGH - Better content discovery</p>"},{"location":"website-improvements/#42-advanced-search-filters","title":"4.2 Advanced Search Filters","text":"<p>What: Add filters for lesson type, difficulty, topics.</p> <p>Implementation:</p> <ol> <li> <p>Add metadata to lesson pages:    <pre><code>---\ntags:\n  - python-basics\n  - data-structures\ndifficulty: beginner\nestimated_time: 30min\n---\n</code></pre></p> </li> <li> <p>Create search UI with filters:    <pre><code>class SearchFilter {\n  filterByTag(tag) {\n    // Filter search results by tag\n  }\n\n  filterByDifficulty(level) {\n    // Filter by difficulty\n  }\n}\n</code></pre></p> </li> </ol> <p>Estimated Effort: 4-5 hours Impact: MEDIUM - Improved navigation</p>"},{"location":"website-improvements/#5-dynamic-features-and-user-experience","title":"5. Dynamic Features and User Experience","text":""},{"location":"website-improvements/#51-estimated-readingcompletion-time","title":"5.1 Estimated Reading/Completion Time","text":"<p>What: Show estimated time for each lesson.</p> <p>Implementation:</p> <ol> <li> <p>Calculate during build:    <pre><code>def estimate_reading_time(content: str) -&gt; int:\n    \"\"\"Estimate reading time in minutes.\"\"\"\n    words = len(content.split())\n    # Average reading speed: 200-250 words per minute\n    return max(1, words // 225)\n</code></pre></p> </li> <li> <p>Add to lesson metadata:    <pre><code>!!! info \"Lesson Overview\"\n    **Estimated Time**: 25 minutes\n    **Difficulty**: Intermediate\n    **Prerequisites**: Day 22 (NumPy)\n</code></pre></p> </li> </ol> <p>Estimated Effort: 1-2 hours Impact: LOW - Nice to have</p>"},{"location":"website-improvements/#52-related-lessons-and-prerequisites","title":"5.2 Related Lessons and Prerequisites","text":"<p>What: Show lesson relationships and prerequisites.</p> <p>Implementation:</p> <ol> <li> <p>Define relationships in config:    <pre><code>LESSON_PREREQS = {\n    \"Day_23_Pandas\": [\"Day_22_NumPy\"],\n    \"Day_24_Pandas_Advanced\": [\"Day_23_Pandas\"],\n}\n</code></pre></p> </li> <li> <p>Generate navigation links:    <pre><code>## Prerequisites\n- [Day 22: NumPy](day-22-numpy.md)\n\n## What's Next\n- [Day 24: Advanced Pandas](day-24-pandas-advanced.md)\n</code></pre></p> </li> </ol> <p>Estimated Effort: 2-3 hours Impact: MEDIUM - Better learning path</p>"},{"location":"website-improvements/#53-code-playground-sidebar","title":"5.3 Code Playground Sidebar","text":"<p>What: Persistent code playground sidebar for experimentation.</p> <p>Implementation:</p> <ol> <li> <p>Add sidebar widget:    <pre><code>&lt;div class=\"playground-sidebar\"&gt;\n  &lt;h3&gt;Quick Playground&lt;/h3&gt;\n  &lt;textarea id=\"playground-code\"&gt;&lt;/textarea&gt;\n  &lt;button onclick=\"runPlayground()\"&gt;Run&lt;/button&gt;\n  &lt;pre id=\"playground-output\"&gt;&lt;/pre&gt;\n&lt;/div&gt;\n</code></pre></p> </li> <li> <p>Make it sticky:    <pre><code>.playground-sidebar {\n  position: sticky;\n  top: 80px;\n  max-height: calc(100vh - 100px);\n  overflow-y: auto;\n}\n</code></pre></p> </li> </ol> <p>Estimated Effort: 3-4 hours Impact: MEDIUM - Encourages experimentation</p>"},{"location":"website-improvements/#54-exportshare-features","title":"5.4 Export/Share Features","text":"<p>What: Allow users to export code snippets or share lessons.</p> <p>Implementation:</p> <ol> <li> <p>Add export buttons:    <pre><code>function exportCode() {\n  const code = document.getElementById('code-input').value;\n  const blob = new Blob([code], { type: 'text/x-python' });\n  const url = URL.createObjectURL(blob);\n  const a = document.createElement('a');\n  a.href = url;\n  a.download = 'my_code.py';\n  a.click();\n}\n</code></pre></p> </li> <li> <p>Add share buttons:    <pre><code>&lt;button onclick=\"shareLesson()\"&gt;\n  Share this lesson\n&lt;/button&gt;\n</code></pre></p> </li> </ol> <p>Estimated Effort: 2 hours Impact: LOW - Social features</p>"},{"location":"website-improvements/#6-analytics-and-feedback","title":"6. Analytics and Feedback","text":""},{"location":"website-improvements/#61-anonymous-usage-analytics","title":"6.1 Anonymous Usage Analytics","text":"<p>What: Track which lessons are most popular (privacy-respecting).</p> <p>Implementation:</p> <ol> <li> <p>Use Plausible or similar privacy-friendly analytics:    <pre><code>extra:\n  analytics:\n    provider: custom\n    property: plausible\n</code></pre></p> </li> <li> <p>Add to site:    <pre><code>&lt;script defer data-domain=\"saint2706.github.io\" \n        src=\"https://plausible.io/js/script.js\"&gt;&lt;/script&gt;\n</code></pre></p> </li> </ol> <p>Estimated Effort: 1 hour Impact: LOW - Helps improve content</p>"},{"location":"website-improvements/#62-feedback-widget","title":"6.2 Feedback Widget","text":"<p>What: Allow users to provide feedback on lessons.</p> <p>Implementation:</p> <ol> <li> <p>Add simple feedback form:    <pre><code>&lt;div class=\"feedback-widget\"&gt;\n  &lt;p&gt;Was this lesson helpful?&lt;/p&gt;\n  &lt;button onclick=\"submitFeedback('yes')\"&gt;\ud83d\udc4d Yes&lt;/button&gt;\n  &lt;button onclick=\"submitFeedback('no')\"&gt;\ud83d\udc4e No&lt;/button&gt;\n&lt;/div&gt;\n</code></pre></p> </li> <li> <p>Store in GitHub Issues or external service:    <pre><code>async function submitFeedback(rating) {\n  const lesson = getCurrentLesson();\n  // Send to backend or create GitHub issue\n}\n</code></pre></p> </li> </ol> <p>Estimated Effort: 2-3 hours Impact: MEDIUM - Helps content improvement</p>"},{"location":"website-improvements/#implementation-priorities","title":"Implementation Priorities","text":""},{"location":"website-improvements/#phase-1-essential-week-1-2","title":"Phase 1: Essential (Week 1-2)","text":"<ol> <li>\u2705 JupyterLite integration</li> <li>\u2705 Binder badges</li> <li>\u2705 Enhanced accessibility (ARIA labels)</li> <li>\u2705 Progress tracking</li> </ol> <p>Estimated Total: 12-16 hours</p>"},{"location":"website-improvements/#phase-2-enhanced-experience-week-3-4","title":"Phase 2: Enhanced Experience (Week 3-4)","text":"<ol> <li>\u2705 Thebe integration</li> <li>\u2705 Pyodide code widgets</li> <li>\u2705 Enhanced search</li> <li>\u2705 Related lessons navigation</li> </ol> <p>Estimated Total: 12-15 hours</p>"},{"location":"website-improvements/#phase-3-nice-to-have-week-5-6","title":"Phase 3: Nice to Have (Week 5-6)","text":"<ol> <li>\u2705 Interactive quizzes</li> <li>\u2705 Code playground sidebar</li> <li>\u2705 Analytics and feedback</li> <li>\u2705 Export/share features</li> </ol> <p>Estimated Total: 12-15 hours</p>"},{"location":"website-improvements/#technical-requirements","title":"Technical Requirements","text":""},{"location":"website-improvements/#dependencies-to-add","title":"Dependencies to Add","text":"<p>Python packages (<code>docs/requirements.txt</code>): <pre><code>mkdocs-material&gt;=9.5.16\njupyterlite-core&gt;=0.3.0\njupyterlite-pyodide-kernel&gt;=0.3.0\nmkdocs-jupyter&gt;=0.24.0\n</code></pre></p> <p>JavaScript libraries (via CDN): <pre><code>extra_javascript:\n  - https://unpkg.com/thebe@latest/lib/index.js\n  - https://cdn.jsdelivr.net/pyodide/v0.25.0/full/pyodide.js\n  - javascripts/pyodide-console.js\n  - javascripts/thebe-config.js\n  - javascripts/progress-tracker.js\n</code></pre></p> <p>MkDocs plugins: <pre><code>plugins:\n  - search:\n      lang: en\n      indexing: 'full'\n  - mkdocs-jupyter:\n      include_source: true\n      execute: false\n</code></pre></p>"},{"location":"website-improvements/#maintenance-considerations","title":"Maintenance Considerations","text":""},{"location":"website-improvements/#1-jupyterlite-updates","title":"1. JupyterLite Updates","text":"<ul> <li>Frequency: Quarterly</li> <li>Effort: 1-2 hours</li> <li>Tasks: Update Pyodide packages, rebuild lite distribution</li> </ul>"},{"location":"website-improvements/#2-content-updates","title":"2. Content Updates","text":"<ul> <li>Frequency: As lessons are added/modified</li> <li>Effort: Automatic via CI/CD</li> <li>Tasks: Ensure new notebooks are properly indexed</li> </ul>"},{"location":"website-improvements/#3-dependency-management","title":"3. Dependency Management","text":"<ul> <li>Frequency: Monthly</li> <li>Effort: 1 hour</li> <li>Tasks: Update JavaScript libraries, Python packages</li> </ul>"},{"location":"website-improvements/#4-analytics-review","title":"4. Analytics Review","text":"<ul> <li>Frequency: Monthly</li> <li>Effort: 1 hour</li> <li>Tasks: Review usage patterns, identify popular content</li> </ul>"},{"location":"website-improvements/#cost-analysis","title":"Cost Analysis","text":""},{"location":"website-improvements/#infrastructure-costs","title":"Infrastructure Costs","text":"<ul> <li>GitHub Pages: FREE \u2705</li> <li>JupyterLite: FREE \u2705 (runs client-side)</li> <li>Binder: FREE \u2705 (open service)</li> <li>CDN for JavaScript: FREE \u2705</li> <li>Total: $0/month</li> </ul>"},{"location":"website-improvements/#development-costs","title":"Development Costs","text":"<ul> <li>Phase 1: 12-16 hours</li> <li>Phase 2: 12-15 hours</li> <li>Phase 3: 12-15 hours</li> <li>Total: 36-46 hours initial development</li> </ul>"},{"location":"website-improvements/#maintenance-costs","title":"Maintenance Costs","text":"<ul> <li>Monthly: 2-3 hours</li> <li>Quarterly: 5-6 hours (includes updates)</li> </ul>"},{"location":"website-improvements/#success-metrics","title":"Success Metrics","text":""},{"location":"website-improvements/#user-engagement","title":"User Engagement","text":"<ul> <li>\u2705 Time spent on lesson pages</li> <li>\u2705 Number of code executions</li> <li>\u2705 Lesson completion rate</li> <li>\u2705 Return visitor rate</li> </ul>"},{"location":"website-improvements/#technical-metrics","title":"Technical Metrics","text":"<ul> <li>\u2705 Page load time (&lt;3 seconds)</li> <li>\u2705 Interactive runtime initialization (&lt;5 seconds)</li> <li>\u2705 Accessibility score (&gt;95 on Lighthouse)</li> <li>\u2705 Search response time (&lt;1 second)</li> </ul>"},{"location":"website-improvements/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>\u2705 Lesson completion rate</li> <li>\u2705 Quiz scores (if implemented)</li> <li>\u2705 User feedback ratings</li> <li>\u2705 GitHub repository stars/forks</li> </ul>"},{"location":"website-improvements/#accessibility-compliance","title":"Accessibility Compliance","text":"<p>All recommendations follow: - \u2705 WCAG 2.1 Level AA standards - \u2705 Section 508 compliance - \u2705 ARIA 1.2 specifications - \u2705 Keyboard navigation support - \u2705 Screen reader compatibility</p>"},{"location":"website-improvements/#browser-compatibility","title":"Browser Compatibility","text":""},{"location":"website-improvements/#supported-browsers","title":"Supported Browsers","text":"<ul> <li>\u2705 Chrome/Edge 90+ (JupyterLite requires modern browsers)</li> <li>\u2705 Firefox 88+</li> <li>\u2705 Safari 14+</li> <li>\u274c Internet Explorer (not supported)</li> </ul>"},{"location":"website-improvements/#fallbacks","title":"Fallbacks","text":"<ul> <li>Static HTML for unsupported browsers</li> <li>Binder links as alternative</li> <li>Download notebook option</li> </ul>"},{"location":"website-improvements/#next-steps","title":"Next Steps","text":""},{"location":"website-improvements/#immediate-actions","title":"Immediate Actions","text":"<ol> <li>Review and approve this proposal</li> <li>Set up development branch</li> <li>Install JupyterLite and test basic setup</li> <li>Create prototype with 2-3 lessons</li> </ol>"},{"location":"website-improvements/#short-term-1-2-weeks","title":"Short Term (1-2 weeks)","text":"<ol> <li>Implement JupyterLite for all lessons</li> <li>Add Binder badges</li> <li>Enhance accessibility</li> <li>Deploy to staging environment</li> </ol>"},{"location":"website-improvements/#medium-term-3-4-weeks","title":"Medium Term (3-4 weeks)","text":"<ol> <li>Add interactive code widgets</li> <li>Implement progress tracking</li> <li>Enhance search functionality</li> <li>User testing and feedback</li> </ol>"},{"location":"website-improvements/#long-term-5-6-weeks","title":"Long Term (5-6 weeks)","text":"<ol> <li>Add quizzes and assessments</li> <li>Implement advanced features</li> <li>Monitor analytics</li> <li>Continuous improvement</li> </ol>"},{"location":"website-improvements/#resources-and-references","title":"Resources and References","text":""},{"location":"website-improvements/#jupyterlite","title":"JupyterLite","text":"<ul> <li>Documentation: https://jupyterlite.readthedocs.io/</li> <li>Examples: https://jupyterlite.github.io/demo/</li> <li>GitHub: https://github.com/jupyterlite/jupyterlite</li> </ul>"},{"location":"website-improvements/#thebe","title":"Thebe","text":"<ul> <li>Documentation: https://thebe.readthedocs.io/</li> <li>Examples: https://thebe.readthedocs.io/en/stable/examples.html</li> </ul>"},{"location":"website-improvements/#binder","title":"Binder","text":"<ul> <li>Documentation: https://mybinder.org/</li> <li>Example repository: https://github.com/binder-examples/</li> </ul>"},{"location":"website-improvements/#accessibility","title":"Accessibility","text":"<ul> <li>WCAG Guidelines: https://www.w3.org/WAI/WCAG21/quickref/</li> <li>ARIA Practices: https://www.w3.org/WAI/ARIA/apg/</li> </ul>"},{"location":"website-improvements/#mkdocs-material","title":"MkDocs Material","text":"<ul> <li>Documentation: https://squidfunk.github.io/mkdocs-material/</li> <li>Plugins: https://github.com/mkdocs/catalog</li> </ul>"},{"location":"website-improvements/#conclusion","title":"Conclusion","text":"<p>These recommendations provide a comprehensive roadmap for transforming the Coding-For-MBA documentation site into an interactive, accessible, and engaging learning platform. The phased approach allows for incremental improvements while maintaining the current functionality.</p> <p>Key Benefits: - \ud83d\ude80 Interactive notebooks run directly in the browser - \u267f Enhanced accessibility for all learners - \ud83d\udcca Better tracking and personalization - \ud83c\udfaf Improved learning outcomes - \ud83d\udcb0 Zero infrastructure costs</p> <p>Next step: Review this proposal and decide which features to prioritize for implementation.</p>"},{"location":"lessons/","title":"Coding for MBA Documentation","text":"<p>Welcome to the official documentation hub for the Coding for MBA curriculum. This MkDocs-powered site pulls content directly from the repository so you can:</p> <ul> <li>Navigate the entire 67-day learning path in one place.</li> <li>Read every lesson summary without digging through folders.</li> <li>Jump straight into the companion notebooks or Python scripts for each topic.</li> </ul> <p>Prefer a darker UI?</p> <p>Use the theme toggle in the header to switch between the light and dark palettes.</p>"},{"location":"lessons/#whats-inside","title":"What's inside","text":"<ul> <li>Machine Learning Curriculum \u2013 Phased roadmap that explains how the Day 40\u201367 sequence ladders into an end-to-end ML capability.</li> <li>Lessons \u2013 Direct access to every <code>Day_*</code> lesson README, enhanced with quick links to the supporting notebooks and scripts.</li> <li>Accessible lesson exports \u2013 Screen-reader-ready HTML and Markdown versions of every notebook live under <code>docs/lessons/</code> for offline or assistive-technology-first study.</li> </ul>"},{"location":"lessons/#lessons","title":"Lessons","text":"<p>Use the table below to jump straight into any lesson in the 67-day journey.</p> Day Lesson Day 01 \ud83d\udcd8 Day 1: Python for Business Analytics - First Steps Day 02 \ud83d\udcd8 Day 2: Storing and Analyzing Business Data Day 03 \ud83d\udcd8 Day 3: Operators - The Tools for Business Calculation and Logic Day 04 \ud83d\udcd8 Day 4: Working with Text Data - Strings Day 05 \ud83d\udcd8 Day 5: Managing Collections of Business Data with Lists Day 06 \ud83d\udcd8 Day 6: Tuples - Storing Immutable Business Data Day 07 \ud83d\udcd8 Day 7: Sets - Managing Unique Business Data Day 08 \ud83d\udcd8 Day 8: Dictionaries - Structuring Complex Business Data Day 09 \ud83d\udcd8 Day 9: Conditionals - Implementing Business Logic Day 10 \ud83d\udcd8 Day 10: Loops - Automating Repetitive Business Tasks Day 11 \ud83d\udcd8 Day 11: Functions - Creating Reusable Business Tools Day 12 \ud83d\udcd8 Day 12: List Comprehension - Elegant Data Manipulation Day 13 \ud83d\udcd8 Day 13: Higher-Order Functions &amp; Lambda Day 14 \ud83d\udcd8 Day 14: Modules - Organizing Your Business Logic Day 15 \ud83d\udcd8 Day 15: Exception Handling - Building Robust Business Logic Day 16 \ud83d\udcd8 Day 16: File Handling for Business Analytics Day 17 \ud83d\udcd8 Day 17: Regular Expressions for Text Pattern Matching Day 18 \ud83d\udcd8 Day 18: Classes and Objects - Modeling Business Concepts Day 19 \ud83d\udcd8 Day 19: Working with Dates and Times Day 20 \ud83d\udcd8 Day 20: Python Package Manager (pip) &amp; Third-Party Libraries Day 21 \ud83d\udcd8 Day 21: Virtual Environments - Professional Project Management Day 22 \ud83d\udcd8 Day 22: NumPy - The Foundation of Numerical Computing Day 23 \ud83d\udcd8 Day 23: Pandas - Your Data Analysis Superpower Day 24 \ud83d\udcd8 Day 24: Advanced Pandas - Working with Real Data Day 25 \ud83d\udcd8 Day 25: Data Cleaning - The Most Important Skill in Analytics Day 26 \ud83d\udcd8 Day 26: Practical Statistics for Business Analysis Day 27 \ud83d\udcd8 Day 27: Data Visualization - Communicating Insights Day 28 \ud83d\udcd8 Day 28: Advanced Visualization &amp; Customization Day 29 \ud83d\udcd8 Day 29: Interactive Visualization with Plotly Day 30 \ud83d\udcd8 Day 30: Web Scraping - Extracting Data from the Web Day 31 \ud83d\udcd8 Day 31: Working with Databases in Python Day 32 \ud83d\udcd8 Day 32: Connecting to Other Databases (MySQL &amp; MongoDB) Day 33 \ud83d\udcd8 Day 33: Accessing Web APIs with <code>requests</code> Day 34 \ud83d\udcd8 Day 34: Building a Simple API with Flask Day 35 \ud83c\udf10 Day 35: Flask Web Framework Day 36 \ud83d\udcca Day 36 \u2013 Capstone Case Study Day 37 \ud83c\udf89 Day 37: Conclusion &amp; Your Journey Forward \ud83c\udf89 Day 38 Day 38: Math Foundations - Linear Algebra Day 39 Day 39: Math Foundations - Calculus Day 40 Day 40: Introduction to Machine Learning &amp; Core Concepts Day 41 Day 41 \u00b7 Supervised Learning \u2013 Regression Day 42 Day 42 \u00b7 Supervised Learning \u2013 Classification (Part 1) Day 43 Day 43 \u00b7 Supervised Learning \u2013 Classification (Part 2) Day 44 Day 44: Unsupervised Learning Day 45 Day 45: Feature Engineering &amp; Model Evaluation Day 46 Day 46: Introduction to Neural Networks &amp; Frameworks Day 47 Day 47: Convolutional Neural Networks (CNNs) for Computer Vision Day 48 Day 48: Recurrent Neural Networks (RNNs) for Sequence Data Day 49 Day 49: Natural Language Processing (NLP) Day 50 Day 50: MLOps - Model Deployment Day 51 Day 51 \u2013 Regularised Models Day 52 Day 52 \u2013 Ensemble Methods Day 53 Day 53 \u2013 Model Tuning and Feature Selection Day 54 Day 54 \u2013 Probabilistic Modeling Day 55 Day 55 \u2013 Advanced Unsupervised Learning Day 56 Day 56 \u2013 Time Series and Forecasting Day 57 Day 57 \u2013 Recommender Systems Day 58 Day 58 \u2013 Transformers and Attention Day 59 Day 59 \u2013 Generative Models Day 60 Day 60 \u2013 Graph and Geometric Learning Day 61 Day 61 \u2013 Reinforcement and Offline Learning Day 62 Day 62 \u2013 Model Interpretability and Fairness Day 63 Day 63 \u2013 Causal Inference and Uplift Modeling Day 64 Day 64 \u2013 Modern NLP Pipelines Day 65 Day 65 \u2013 MLOps Pipelines and CI/CD Automation Day 66 Day 66 \u2013 Model Deployment and Serving Patterns Day 67 Day 67 \u2013 Model Monitoring and Reliability Engineering"},{"location":"lessons/#contributing-updates","title":"Contributing updates","text":"<p>Contributions are welcome! See the repository README for setup instructions and the documentation contribution workflow.</p>"},{"location":"lessons/day-01-introduction/","title":"\ud83d\udcd8 Day 1: Python for Business Analytics - First Steps","text":""},{"location":"lessons/day-01-introduction/#welcome-to-the-course","title":"Welcome to the Course","text":"<p>Welcome, future business leader! You're about to take your first step into a larger world of data-driven decision-making. In today's business landscape, the ability to understand and leverage data is not just a technical skill\u2014it's a core competency for effective management. This course is designed specifically for MBA students like you. We'll skip the abstract computer science jargon and focus on one thing: using Python as a powerful tool to solve real-world business problems.</p> <p>You don't need any prior coding experience. We'll start from zero and build your skills step-by-step. By the end of this 50-day journey, you'll be able to manipulate data, generate insights, and even build predictive models.</p>"},{"location":"lessons/day-01-introduction/#why-python-for-business","title":"Why Python for Business?","text":"<ol> <li>The Lingua Franca of Data: Python is the most widely used language for data science, machine learning, and analytics.</li> <li>Automation of Tedious Tasks: Automate cleaning messy Excel sheets, gathering web data, or generating weekly reports.</li> <li>Powerful Analytics at Your Fingertips: Python's extensive libraries allow for complex statistical analysis and compelling data visualizations.</li> <li>Strategic Advantage: Understanding the language of your data science team gives you a significant strategic advantage.</li> </ol>"},{"location":"lessons/day-01-introduction/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, please follow the setup instructions in the main README.md at the root of this repository. This will guide you through:</p> <ol> <li>Cloning the project.</li> <li>Setting up a virtual environment.</li> <li>Installing all the necessary libraries from <code>requirements.txt</code>.</li> </ol> <p>Once you have completed those steps and activated your virtual environment, you are ready to start this lesson.</p>"},{"location":"lessons/day-01-introduction/#your-first-python-script-a-business-calculation","title":"Your First Python Script: A Business Calculation","text":"<p>Let's explore the code for today's lesson. The script for this lesson is <code>helloworld.py</code>.</p> <ol> <li>Review the Code: Open <code>Day_01_Introduction/helloworld.py</code> in your code editor. You will see that the code is now organized into functions, which is a best practice for writing clean and reusable code.</li> <li>Run the Script: To run the script, make sure your terminal is in the root directory of this project (the <code>Coding-For-MBA</code> folder) and your virtual environment is active. Then, execute the script by running:    <pre><code>python Day_01_Introduction/helloworld.py\n</code></pre></li> </ol> <p>You will see the output of the business calculations printed to your console.</p>"},{"location":"lessons/day-01-introduction/#exercises-day-1","title":"\ud83d\udcbb Exercises: Day 1","text":"<p>The exercises are designed to help you practice the fundamental concepts introduced in the script.</p> <ol> <li> <p>Company Introduction:</p> </li> <li> <p>Create a new Python file named <code>my_solutions.py</code> in the <code>Day_01_Introduction</code> folder.</p> </li> <li>In your new script, use the <code>print()</code> function to introduce a fictional company.</li> <li> <p>Example: <code>print(\"Welcome to InnovateCorp Analytics\")</code></p> </li> <li> <p>Quarterly Sales Calculation:</p> </li> <li> <p>A company had quarterly sales of $110,000, $120,000, $135,000, and $140,000.</p> </li> <li> <p>In your script, use the <code>print()</code> function to calculate and display the total annual sales. (Hint: you can do math right inside the print statement: <code>print(110000 + 120000 + ...)</code>).</p> </li> <li> <p>Checking Data Types in Business:</p> </li> <li> <p>Use the <code>type()</code> function to check the data types of the following business-related data points.</p> <ul> <li><code>1500</code> (e.g., number of units sold)</li> <li><code>1500.75</code> (e.g., a price or a financial metric)</li> <li><code>'InnovateCorp'</code> (e.g., a company name)</li> <li><code>True</code> (e.g., is the product in stock?)</li> </ul> </li> </ol> <p>\ud83c\udf89 Congratulations! You've just run your first refactored Python script and are on your way to becoming a data-savvy leader.</p>"},{"location":"lessons/day-01-introduction/#additional-materials","title":"Additional Materials","text":"<ul> <li>helloworld.ipynb</li> <li>solutions.ipynb</li> </ul> helloworld.py <p>View on GitHub</p> helloworld.py<pre><code>\"\"\"\nDay 1: Python for Business Analytics - First Steps (Refactored)\n\nThis script demonstrates basic Python concepts using business-relevant examples.\nWe will perform a simple profit calculation and check the types of various\nbusiness-related data points. This version is refactored to use functions.\n\"\"\"\n\n\ndef calculate_gross_profit(revenue, cogs):\n    \"\"\"Calculates the gross profit from revenue and COGS.\"\"\"\n    return revenue - cogs\n\n\ndef calculate_gross_profit_margin(gross_profit, revenue):\n    \"\"\"Calculates the gross profit margin.\"\"\"\n    if revenue == 0:\n        return 0\n    return (gross_profit / revenue) * 100\n\n\ndef display_business_analytics(revenue, cogs):\n    \"\"\"Calculates and displays key business metrics.\"\"\"\n    print(\"Welcome to the Quarterly Business Review Dashboard\")\n    print()\n\n    gross_profit = calculate_gross_profit(revenue, cogs)\n    gross_profit_margin = calculate_gross_profit_margin(gross_profit, revenue)\n\n    print(f\"Total Revenue: ${revenue}\")\n    print(f\"Cost of Goods Sold: ${cogs}\")\n    print(f\"Gross Profit: ${gross_profit}\")\n    print()\n    print(f\"Gross Profit Margin: {gross_profit_margin:.2f}%\")\n    print(\"-\" * 20)\n\n\ndef display_data_types():\n    \"\"\"Displays the types of various business-related data points.\"\"\"\n    print(\"Checking the types of some common business data points:\")\n\n    units_sold = 1500\n    product_price = 49.99\n    company_name = \"InnovateCorp\"\n    is_in_stock = True\n    quarterly_sales = [110000, 120000, 135000, 140000]\n\n    print(f\"Data: {units_sold}, Type: {type(units_sold)}\")\n    print(f\"Data: {product_price}, Type: {type(product_price)}\")\n    print(f\"Data: '{company_name}', Type: {type(company_name)}\")\n    print(f\"Data: {is_in_stock}, Type: {type(is_in_stock)}\")\n    print(f\"Data: {quarterly_sales}, Type: {type(quarterly_sales)}\")\n\n\nif __name__ == \"__main__\":\n    # --- Basic Business Calculations ---\n    revenue_main = 500000\n    cogs_main = 350000\n    display_business_analytics(revenue_main, cogs_main)\n\n    # --- Understanding Data Types in a Business Context ---\n    display_data_types()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 1: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Company Introduction ---\nprint(\"--- Solution to Exercise 1 ---\")\nprint(\"Welcome to InnovateCorp Analytics\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Quarterly Sales Calculation ---\nprint(\"--- Solution to Exercise 2 ---\")\n# The calculation is done directly inside the print function.\nprint(\"Total Annual Sales:\")\nprint(110000 + 120000 + 135000 + 140000)\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Checking Data Types in Business ---\nprint(\"--- Solution to Exercise 3 ---\")\n# Using the type() function to inspect each data point.\nprint(\"Data point: 1500, Type:\", type(1500))\nprint(\"Data point: 1500.75, Type:\", type(1500.75))\nprint(\"Data point: 'InnovateCorp', Type:\", type(\"InnovateCorp\"))\nprint(\"Data point: True, Type:\", type(True))\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-02-variables-builtin-functions/","title":"\ud83d\udcd8 Day 2: Storing and Analyzing Business Data","text":"<p>In Day 1, we performed basic calculations. Now, we'll learn how to store data in variables and use Python's built-in functions to analyze it.</p>"},{"location":"lessons/day-02-variables-builtin-functions/#what-is-a-variable","title":"What is a Variable?","text":"<p>A variable is a labeled container for information. Instead of using a raw number like <code>500000</code>, we can store it in a variable called <code>revenue</code>, making our code more readable and manageable.</p> <p><code>revenue = 500000</code></p>"},{"location":"lessons/day-02-variables-builtin-functions/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Be Descriptive: <code>quarterly_sales</code> is better than <code>qs</code>.</li> <li>Use Snake Case: Separate words with underscores, like <code>cost_of_goods_sold</code>.</li> </ul>"},{"location":"lessons/day-02-variables-builtin-functions/#built-in-functions-your-basic-toolkit","title":"Built-in Functions: Your Basic Toolkit","text":"<p>Python includes pre-built functions for common tasks:</p> <ul> <li><code>len()</code>: Finds the length (e.g., number of items in a list).</li> <li><code>sum()</code>: Calculates the sum of numbers in a list.</li> <li><code>min()</code> and <code>max()</code>: Find the minimum and maximum values.</li> <li><code>round()</code>: Rounds a number to a specified number of decimal places.</li> </ul>"},{"location":"lessons/day-02-variables-builtin-functions/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-02-variables-builtin-functions/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>variables.py</code>, has been refactored into functions to promote code reuse and testability.</p> <ol> <li>Review the Code: Open <code>Day_02_Variables_Builtin_Functions/variables.py</code>. Notice how the logic is now organized into functions like <code>display_company_profile()</code> and <code>analyze_weekly_sales()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script:    <pre><code>python Day_02_Variables_Builtin_Functions/variables.py\n</code></pre>    You will see the output from the example scenarios defined at the bottom of the script.</li> <li>Run the Tests: You can also run the tests for this lesson to see how we verify the functions' correctness:    <pre><code>pytest tests/test_day_02.py\n</code></pre></li> </ol>"},{"location":"lessons/day-02-variables-builtin-functions/#exercises-day-2","title":"\ud83d\udcbb Exercises: Day 2","text":"<ol> <li> <p>Company Profile Variables:</p> </li> <li> <p>In a new Python script (<code>my_solutions_02.py</code>), declare variables for a fictional company: <code>company_name</code>, <code>year_founded</code>, <code>current_revenue</code>, and <code>is_publicly_traded</code>.</p> </li> <li> <p>Print each of these variables with a descriptive label.</p> </li> <li> <p>Sales Analysis:</p> </li> <li> <p>A list of sales transactions is: <code>[150.50, 200.00, 75.25, 300.75, 120.00]</code>.</p> </li> <li>Store these in a variable called <code>weekly_sales</code>.</li> <li> <p>Use built-in functions to calculate and print:</p> <ul> <li>The total number of sales (<code>len()</code>).</li> <li>The total revenue (<code>sum()</code>).</li> <li>The smallest and largest sales (<code>min()</code>, <code>max()</code>).</li> <li>The average sale amount.</li> </ul> </li> <li> <p>Profit Calculator Function:</p> </li> <li> <p>Create a function <code>calculate_profit(revenue, expenses)</code> that takes two numbers and returns the difference.</p> </li> <li>Call this function with some example numbers (e.g., <code>calculate_profit(50000, 35000)</code>) and print the result.</li> </ol> <p>\ud83c\udf89 Well done! You've learned how to store data in variables and use Python's built-in functions for analysis\u2014foundational skills for everything that comes next.</p>"},{"location":"lessons/day-02-variables-builtin-functions/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> <li>variables.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 2: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Company Profile Variables ---\nprint(\"--- Solution to Exercise 1 ---\")\ncompany_name = \"InnovateCorp\"\nyear_founded = 2015\ncurrent_revenue = 2500000.50\nis_publicly_traded = False\n\nprint(f\"Company Name: {company_name}\")\nprint(f\"Year Founded: {year_founded}\")\nprint(f\"Current Revenue: ${current_revenue}\")\nprint(f\"Is Publicly Traded: {is_publicly_traded}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Sales Analysis ---\nprint(\"--- Solution to Exercise 2 ---\")\nweekly_sales = [150.50, 200.00, 75.25, 300.75, 120.00]\nprint(f\"Sales data: {weekly_sales}\")\n\n# The total number of sales transactions\nnum_transactions = len(weekly_sales)\nprint(f\"Total number of transactions: {num_transactions}\")\n\n# The total revenue for the week\ntotal_revenue = sum(weekly_sales)\nprint(f\"Total revenue: ${total_revenue:.2f}\")\n\n# The smallest sale\nmin_sale = min(weekly_sales)\nprint(f\"Smallest sale: ${min_sale:.2f}\")\n\n# The largest sale\nmax_sale = max(weekly_sales)\nprint(f\"Largest sale: ${max_sale:.2f}\")\n\n# The average sale amount\naverage_sale = total_revenue / num_transactions\nprint(f\"Average sale amount: ${average_sale:.2f}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: User Input for a Profit Calculator ---\nprint(\"--- Solution to Exercise 3 ---\")\n# Note: To run this interactively, you would run the python file\n# in your terminal. The input() function will pause the script\n# and wait for you to type a value and press Enter.\n\n# We wrap the code in a try...except block to handle cases where the user\n# might enter text instead of a number, which would cause a crash.\ntry:\n    # float() converts the string from input() into a floating-point number\n    revenue_input = float(input(\"Enter total revenue: \"))\n    expenses_input = float(input(\"Enter total expenses: \"))\n\n    profit = revenue_input - expenses_input\n    print(f\"Calculated Profit: ${profit:.2f}\")\n\nexcept ValueError:\n    print(\"Invalid input. Please make sure to enter numbers only.\")\n\nprint(\"-\" * 20)\n</code></pre> variables.py <p>View on GitHub</p> variables.py<pre><code>\"\"\"\nDay 2: Storing and Analyzing Business Data (Refactored)\n\nThis script demonstrates the use of variables to store business data\nand built-in functions to perform basic analysis. This version is\nrefactored into functions for better organization and testability.\n\"\"\"\n\n\ndef display_company_profile(name, founded, revenue, is_public):\n    \"\"\"Displays the company's profile information.\"\"\"\n    print(\"--- Company Profile ---\")\n    print(f\"Company Name: {name}\")\n    print(f\"Year Founded: {founded}\")\n    print(f\"Current Revenue: ${revenue}\")\n    print(f\"Is Publicly Traded: {is_public}\")\n    print(\"-\" * 20)\n\n\ndef analyze_weekly_sales(sales_data):\n    \"\"\"Analyzes and prints a summary of weekly sales data.\"\"\"\n    if not sales_data:\n        print(\"No sales data to analyze.\")\n        return\n\n    print(\"--- Weekly Sales Analysis ---\")\n    num_transactions = len(sales_data)\n    total_revenue = sum(sales_data)\n    smallest_sale = min(sales_data)\n    largest_sale = max(sales_data)\n    average_sale = total_revenue / num_transactions if num_transactions &gt; 0 else 0\n\n    print(f\"Number of Transactions: {num_transactions}\")\n    print(f\"Total Weekly Revenue: ${total_revenue:.2f}\")\n    print(f\"Smallest Sale: ${smallest_sale:.2f}\")\n    print(f\"Largest Sale: ${largest_sale:.2f}\")\n    print(f\"Average Sale Amount: ${round(average_sale, 2)}\")\n    print(\"-\" * 20)\n\n    return {\n        \"num_transactions\": num_transactions,\n        \"total_revenue\": total_revenue,\n        \"smallest_sale\": smallest_sale,\n        \"largest_sale\": largest_sale,\n        \"average_sale\": average_sale,\n    }\n\n\ndef interactive_profit_calculator():\n    \"\"\"Handles user input to calculate and display profit.\"\"\"\n    print(\"--- Interactive Profit Calculator ---\")\n    try:\n        user_revenue = float(input(\"Enter your total revenue: \"))\n        user_expenses = float(input(\"Enter your total expenses: \"))\n        profit = user_revenue - user_expenses\n        print(f\"Your calculated profit is: ${profit:.2f}\")\n        return profit\n    except ValueError:\n        print(\"Invalid input. Please enter numbers only.\")\n        return None\n\n\nif __name__ == \"__main__\":\n    # --- Storing Company Profile in Variables ---\n    display_company_profile(\"InnovateCorp\", 2015, 2500000.50, False)\n\n    # --- Using Built-in Functions for Sales Analysis ---\n    weekly_sales_data = [150.50, 200.00, 75.25, 300.75, 120.00, 450.50, 275.00]\n    analyze_weekly_sales(weekly_sales_data)\n\n    # --- Getting User Input ---\n    # Note: This part is not easily testable in an automated way without mocking input.\n    # The function is separated to keep the core logic testable.\n    # interactive_profit_calculator() # Uncomment to run the interactive part\n</code></pre>"},{"location":"lessons/day-03-operators/","title":"\ud83d\udcd8 Day 3: Operators - The Tools for Business Calculation and Logic","text":"<p>An operator is a symbol that tells the computer to perform a specific mathematical or logical manipulation. For a business analyst, operators are the tools you'll use to calculate financial metrics, compare results, and create business rules.</p>"},{"location":"lessons/day-03-operators/#key-operator-types","title":"Key Operator Types","text":"<ul> <li>Arithmetic Operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>**</code>): The foundation of any quantitative analysis, used for calculations like profit margin and compound interest.</li> <li>Assignment Operators (<code>=</code>, <code>+=</code>, <code>-=</code>): Used to assign and update values in variables, such as accumulating total sales.</li> <li>Comparison Operators (<code>==</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>): Used to compare two values, resulting in <code>True</code> or <code>False</code>. This is the basis for filtering data and making decisions.</li> <li>Logical Operators (<code>and</code>, <code>or</code>, <code>not</code>): Used to combine conditional statements to create complex business rules, like determining bonus eligibility.</li> </ul>"},{"location":"lessons/day-03-operators/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-03-operators/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>operators.py</code>, has been refactored into functions to make the logic clear, reusable, and testable.</p> <ol> <li>Review the Code: Open <code>Day_03_Operators/operators.py</code>. Each business calculation or rule (e.g., <code>calculate_compound_interest()</code>, <code>check_bonus_eligibility()</code>) is now its own function.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_03_Operators/operators.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_03.py\n</code></pre></li> </ol>"},{"location":"lessons/day-03-operators/#exercises-day-3","title":"\ud83d\udcbb Exercises: Day 3","text":"<ol> <li> <p>Calculate Net Profit Margin:</p> </li> <li> <p>In a new script (<code>my_solutions_03.py</code>), create a function <code>calculate_net_profit_margin(revenue, expenses)</code>.</p> </li> <li>The function should return the net profit margin (<code>(revenue - expenses) / revenue</code>).</li> <li>Call the function with a <code>revenue</code> of 1,200,000 and <code>total_expenses</code> of 850,000.</li> <li> <p>Print the result formatted as a percentage with two decimal places.</p> </li> <li> <p>Inventory Check Function:</p> </li> <li> <p>Create a function <code>check_reorder_status(inventory_count, low_stock_threshold, reorder_threshold)</code>.</p> </li> <li>The function should return a dictionary with two keys: <code>is_low_stock</code> (boolean) and <code>needs_reorder</code> (boolean).</li> <li> <p>Call the function with an <code>inventory_count</code> of 45, a <code>low_stock_threshold</code> of 50, and a <code>reorder_threshold</code> of 25. Print the results.</p> </li> <li> <p>Sales Bonus Eligibility Function:</p> </li> <li> <p>The logic for bonus eligibility is already in the <code>check_bonus_eligibility</code> function in <code>operators.py</code>.</p> </li> <li>In your own script, import this function: <code>from Day_03_Operators.operators import check_bonus_eligibility</code>.</li> <li>Call the function with a few different scenarios for <code>sales</code>, <code>years_of_service</code>, and <code>top_performer_last_quarter</code> to see the results.</li> </ol> <p>\ud83c\udf89 Excellent work! You're now equipped with the operators needed to perform the vast majority of business calculations and logical checks you'll encounter.</p>"},{"location":"lessons/day-03-operators/#additional-materials","title":"Additional Materials","text":"<ul> <li>operators.ipynb</li> <li>solutions.ipynb</li> </ul> operators.py <p>View on GitHub</p> operators.py<pre><code>\"\"\"\nDay 3: Operators in Action for Business Analysis (Refactored)\n\nThis script demonstrates how different Python operators can be used\nto perform business calculations and logical checks. This version is\nrefactored into functions for better organization and testability.\n\"\"\"\n\n\ndef calculate_compound_interest(principal, rate, time, n=1):\n    \"\"\"\n    Calculates the final amount of an investment with compound interest.\n    A = P(1 + r/n)^(nt)\n    \"\"\"\n    # The ** operator is used for exponents\n    final_amount = principal * (1 + rate / n) ** (n * time)\n    return final_amount\n\n\ndef accumulate_sales(initial_sales, daily_sales):\n    \"\"\"\n    Accumulates daily sales into a total.\n    \"\"\"\n    total = initial_sales\n    for sale in daily_sales:\n        total += sale  # The += operator adds a value to a variable\n    return total\n\n\ndef check_inventory_status(inventory_count, low_stock_threshold):\n    \"\"\"Checks if the inventory count is below the low stock threshold.\"\"\"\n    # The &lt; operator checks if a value is less than another\n    return inventory_count &lt; low_stock_threshold\n\n\ndef check_sales_target(current_sales, sales_target):\n    \"\"\"Checks if the current sales have met or exceeded the sales target.\"\"\"\n    # The &gt;= operator checks for \"greater than or equal to\"\n    return current_sales &gt;= sales_target\n\n\ndef check_bonus_eligibility(sales, years_of_service, top_performer_last_quarter):\n    \"\"\"\n    Determines bonus eligibility based on complex business rules.\n    The 'and' requires both conditions to be true.\n    The 'or' allows either condition to be true.\n    \"\"\"\n    is_eligible = (sales &gt; 10000 and years_of_service &gt; 2) or top_performer_last_quarter\n    return is_eligible\n\n\nif __name__ == \"__main__\":\n    # --- Arithmetic Operators for Financial Calculations ---\n    print(\"--- Financial Calculations ---\")\n    principal_amount = 10000\n    interest_rate = 0.05\n    investment_time = 3\n    final_investment_amount = calculate_compound_interest(\n        principal_amount, interest_rate, investment_time\n    )\n    print(\n        f\"Investment of ${principal_amount} after {investment_time} years at {interest_rate * 100}% interest will be: ${final_investment_amount:.2f}\"\n    )\n    print(\"-\" * 20)\n\n    # --- Assignment Operators for Accumulating Data ---\n    print(\"--- Accumulating Daily Sales ---\")\n    sales_over_three_days = [1500, 2200, 1850]\n    total_sales_figure = accumulate_sales(0, sales_over_three_days)\n    print(f\"Total sales after 3 days: ${total_sales_figure}\")\n    print(\"-\" * 20)\n\n    # --- Comparison Operators for Business Rules ---\n    print(\"--- Inventory and Sales Target Checks ---\")\n    is_low = check_inventory_status(inventory_count=45, low_stock_threshold=50)\n    print(f\"Is inventory low? {is_low}\")\n\n    target_met = check_sales_target(current_sales=265000, sales_target=250000)\n    print(f\"Has the sales target been met? {target_met}\")\n    print(\"-\" * 20)\n\n    # --- Logical Operators for Complex Eligibility Rules ---\n    print(\"--- Sales Bonus Eligibility Test ---\")\n    # Scenario 1\n    eligible_s1 = check_bonus_eligibility(\n        sales=12000, years_of_service=1, top_performer_last_quarter=False\n    )\n    print(f\"Scenario 1 (High Sales, New Employee): Eligible? {eligible_s1}\")\n    # Scenario 2\n    eligible_s2 = check_bonus_eligibility(\n        sales=8000, years_of_service=3, top_performer_last_quarter=True\n    )\n    print(f\"Scenario 2 (Top Performer): Eligible? {eligible_s2}\")\n    # Scenario 3\n    eligible_s3 = check_bonus_eligibility(\n        sales=9000, years_of_service=1, top_performer_last_quarter=False\n    )\n    print(f\"Scenario 3 (Not Eligible): Eligible? {eligible_s3}\")\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 3: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Calculate Net Profit Margin ---\nprint(\"--- Solution to Exercise 1 ---\")\nrevenue = 1200000\ntotal_expenses = 850000\n\n# Calculate the profit\nprofit = revenue - total_expenses\n\n# Calculate the net profit margin\n# It's good practice to check if revenue is zero to avoid a DivisionByZeroError\nif revenue &gt; 0:\n    net_profit_margin = (profit / revenue) * 100\n    print(f\"Revenue: ${revenue:,.2f}\")\n    print(f\"Expenses: ${total_expenses:,.2f}\")\n    print(f\"Profit: ${profit:,.2f}\")\n    print(f\"Net Profit Margin: {net_profit_margin:.2f}%\")\nelse:\n    print(\"Cannot calculate margin as revenue is zero.\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Inventory Check ---\nprint(\"--- Solution to Exercise 2 ---\")\ninventory_count = 45\nlow_stock_threshold = 50\nreorder_threshold = 25\n\nis_low_stock = inventory_count &lt; low_stock_threshold\nreorder_required = inventory_count &lt;= reorder_threshold\n\nprint(f\"Inventory count: {inventory_count} units\")\nprint(f\"Is inventory considered low stock? {is_low_stock}\")\nprint(f\"Is a reorder required? {reorder_required}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Sales Bonus Eligibility ---\nprint(\"--- Solution to Exercise 3 ---\")\n\n\ndef check_bonus_eligibility(sales, years_of_service, top_performer_last_quarter):\n    \"\"\"A helper function to test different scenarios easily.\"\"\"\n    is_eligible = (sales &gt; 10000 and years_of_service &gt; 2) or top_performer_last_quarter\n    print(\n        f\"Scenario: Sales=${sales}, Service={years_of_service}yrs, Top Performer={top_performer_last_quarter} -&gt; Eligible? {is_eligible}\"\n    )\n\n\n# Scenario 1: High sales but new employee\ncheck_bonus_eligibility(12000, 1, False)\n\n# Scenario 2: Low sales but top performer\ncheck_bonus_eligibility(8000, 3, True)\n\n# Scenario 3: High sales and long service\ncheck_bonus_eligibility(15000, 5, False)\n\n# Scenario 4: Not eligible on any count\ncheck_bonus_eligibility(9000, 1, False)\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-04-strings/","title":"\ud83d\udcd8 Day 4: Working with Text Data - Strings","text":"<p>In business analytics, text data is everywhere\u2014customer names, product reviews, addresses, and report narratives. In Python, we handle text using strings.</p>"},{"location":"lessons/day-04-strings/#key-string-concepts","title":"Key String Concepts","text":"<ul> <li>F-Strings: The modern and most readable way to format strings. They let you embed variables and expressions directly inside a string.   <pre><code>report_summary = f\"Company: {company_name}, Revenue: ${revenue}\"\n</code></pre></li> <li>String Methods: Built-in functions attached to strings that let you manipulate them. They are essential for data cleaning and preparation.</li> </ul> Method Description Business Use Case <code>.lower()</code>/<code>.upper()</code> Converts case. Standardizing categories. <code>.strip()</code> Removes whitespace from the beginning and end. Cleaning user-entered data. <code>.replace()</code> Replaces a substring with another. Correcting or reformatting data. <code>.split()</code> Splits the string into a list of substrings. Parsing comma-separated data. <code>.startswith()</code> Checks if the string starts with a substring. Identifying invoice numbers. <code>.endswith()</code> Checks if the string ends with a substring. Checking file types."},{"location":"lessons/day-04-strings/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-04-strings/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>strings.py</code>, has been refactored into functions to make each string manipulation a reusable and testable unit of logic.</p> <ol> <li>Review the Code: Open <code>Day_04_Strings/strings.py</code>. Each data transformation (e.g., <code>generate_report_header()</code>, <code>clean_and_format_name()</code>) is now its own function.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_04_Strings/strings.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_04.py\n</code></pre></li> </ol>"},{"location":"lessons/day-04-strings/#exercises-day-4","title":"\ud83d\udcbb Exercises: Day 4","text":"<ol> <li> <p>Generate a Report Header:</p> </li> <li> <p>In a new script (<code>my_solutions_04.py</code>), create a function <code>format_report_title(title, date)</code>.</p> </li> <li>The function should take a title string and a date string and return a formatted header like: <code>--- MONTHLY MARKETING REPORT: 2024-07 ---</code>.</li> <li> <p>Call the function and print the result.</p> </li> <li> <p>Clean Up Product Codes:</p> </li> <li> <p>You have a list of raw product codes: <code>[\" prod-001 \", \"prod-002\", \" Prod-003 \"]</code>.</p> </li> <li>Create a function <code>clean_product_codes(codes)</code> that takes a list of codes.</li> <li>Inside the function, loop through the list, and for each code, remove whitespace and convert it to uppercase.</li> <li>The function should return a new list of cleaned codes.</li> <li> <p>Call the function and print the cleaned list.</p> </li> <li> <p>Validate Email Addresses:</p> </li> <li> <p>Create a function <code>is_valid_email(email)</code> that performs two simple checks:</p> <ul> <li>Does the email contain an <code>@</code> symbol?</li> <li>Does the email end with <code>.com</code>?</li> </ul> </li> <li>The function should return <code>True</code> if both conditions are met, otherwise <code>False</code>.</li> <li>Test your function with a valid email (<code>\"test@example.com\"</code>) and an invalid one (<code>\"test-example.com\"</code>).</li> </ol> <p>\ud83c\udf89 Fantastic! You can now manipulate text data, which is a massive part of any real-world data analysis task. Cleaning, formatting, and parsing strings are skills you'll use every single day.</p>"},{"location":"lessons/day-04-strings/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> <li>strings.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 4: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Generate a Report Header ---\nprint(\"--- Solution to Exercise 1 ---\")\nreport_title = \"Quarterly Sales Report\"\nfiscal_year = 2024\n\n# Using .upper() to make the title all caps for emphasis\n# and an f-string to combine everything.\nheader = f\"*** {report_title.upper()} - FY{fiscal_year} ***\"\nprint(header)\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Clean Up Customer Data ---\nprint(\"--- Solution to Exercise 2 ---\")\ncustomer_name = \"  john doe  \"\n\n# .strip() removes the leading/trailing whitespace\n# .title() capitalizes the first letter of each word\ncleaned_name = customer_name.strip().title()\n\nprint(f\"Original name: '{customer_name}'\")\nprint(f\"Cleaned name: '{cleaned_name}'\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Parse Product SKU ---\nprint(\"--- Solution to Exercise 3 ---\")\nsku = \"PROD-GADGET-001\"\n\n# .split('-') breaks the string into a list of substrings,\n# using the hyphen as the separator.\nsku_parts = sku.split(\"-\")\n\n# We can access the parts of the list by their index.\nproduct_type = sku_parts[0]\nproduct_name = sku_parts[1]\nproduct_id = sku_parts[2]\n\nprint(f\"Original SKU: {sku}\")\nprint(f\"Product Type: {product_type}\")\nprint(f\"Product Name: {product_name}\")\nprint(f\"Product ID: {product_id}\")\nprint(\"-\" * 20)\n</code></pre> strings.py <p>View on GitHub</p> strings.py<pre><code>\"\"\"\nDay 4: Manipulating Business Text Data with Strings (Refactored)\n\nThis script demonstrates common string manipulations and methods\napplied to business-related text data. This version is refactored\ninto functions for better organization and testability.\n\"\"\"\n\n\ndef generate_report_header(title, year):\n    \"\"\"Creates a formatted report header.\"\"\"\n    return f\"*** {title.upper()} - FY{year} ***\"\n\n\ndef clean_and_format_name(raw_name):\n    \"\"\"Cleans and capitalizes a raw name string.\"\"\"\n    return raw_name.strip().title()\n\n\ndef format_date_string(date_str, old_separator=\"-\", new_separator=\"/\"):\n    \"\"\"Replaces separators in a date string.\"\"\"\n    return date_str.replace(old_separator, new_separator)\n\n\ndef parse_sku(sku):\n    \"\"\"Parses a SKU string into its component parts.\"\"\"\n    parts = sku.split(\"-\")\n    if len(parts) == 3:\n        return {\"type\": parts[0], \"name\": parts[1], \"id\": parts[2]}\n    return None\n\n\ndef is_transaction_type(transaction_id, prefix):\n    \"\"\"Checks if a transaction ID starts with a given prefix.\"\"\"\n    return transaction_id.startswith(prefix)\n\n\ndef has_file_extension(filename, extension):\n    \"\"\"Checks if a filename ends with a given extension.\"\"\"\n    return filename.endswith(extension)\n\n\ndef feedback_contains_keyword(feedback, keyword):\n    \"\"\"Checks if a feedback string contains a specific keyword.\"\"\"\n    return feedback.find(keyword) != -1\n\n\nif __name__ == \"__main__\":\n    # --- Formatting Strings for Reports ---\n    print(\"--- Generating Report Headers ---\")\n    header_text = generate_report_header(\"Quarterly Sales Report\", 2024)\n    print(header_text)\n    print(\"-\" * 20)\n\n    # --- Cleaning Customer and Product Data ---\n    print(\"--- Data Cleaning Examples ---\")\n    customer_name = \"  john doe  \"\n    formatted_customer_name = clean_and_format_name(customer_name)\n    print(\n        f\"Raw name: '{customer_name}', Final formatted name: '{formatted_customer_name}'\"\n    )\n\n    date_string = \"2023-Jan-15\"\n    formatted_date_str = format_date_string(date_string)\n    print(f\"Original date: {date_string}, Formatted date: {formatted_date_str}\")\n    print(\"-\" * 20)\n\n    # --- Parsing and Extracting Information from Strings ---\n    print(\"--- Parsing Product and Transaction IDs ---\")\n    product_sku = \"PROD-GADGET-001\"\n    parsed_sku = parse_sku(product_sku)\n    if parsed_sku:\n        print(f\"SKU: {product_sku}\")\n        print(f\"  Product Type: {parsed_sku['type']}\")\n        print(f\"  Product Name: {parsed_sku['name']}\")\n        print(f\"  Product ID: {parsed_sku['id']}\")\n    print()\n\n    trans_id = \"INV-2024-03-15-998\"\n    is_inv = is_transaction_type(trans_id, \"INV\")\n    print(f\"Transaction '{trans_id}' is an invoice: {is_inv}\")\n\n    report_filename = \"q1_sales_report.pdf\"\n    is_a_pdf = has_file_extension(report_filename, \".pdf\")\n    print(f\"Report file '{report_filename}' is a PDF: {is_a_pdf}\")\n    print(\"-\" * 20)\n\n    # --- Searching for keywords ---\n    customer_feedback_text = \"The new CRM is great, but the reporting feature is slow.\"\n    if feedback_contains_keyword(customer_feedback_text, \"slow\"):\n        print(\"Feedback contains the word 'slow'. Action may be required.\")\n    else:\n        print(\"Feedback does not contain the word 'slow'.\")\n</code></pre>"},{"location":"lessons/day-05-lists/","title":"\ud83d\udcd8 Day 5: Managing Collections of Business Data with Lists","text":"<p>In business, you often work with collections of data: lists of customers, quarterly sales figures, products, and more. Python's most fundamental tool for managing ordered collections is the list.</p>"},{"location":"lessons/day-05-lists/#what-is-a-list","title":"What is a List?","text":"<p>A list is an ordered, changeable collection of items, created by placing items inside square brackets <code>[]</code>.</p> <pre><code># A list of department names (strings)\ndepartments = [\"Sales\", \"Marketing\", \"Human Resources\", \"Engineering\"]\n\n# A list of quarterly sales figures (floats)\nquarterly_sales = [120000.50, 135000.75, 110000.00, 145000.25]\n</code></pre>"},{"location":"lessons/day-05-lists/#key-list-operations","title":"Key List Operations","text":"<ul> <li>Accessing Data: Use an item's index (position starting from 0) to access it. <code>departments[0]</code> returns <code>\"Sales\"</code>. Negative indexing like <code>departments[-1]</code> gets the last item.</li> <li>Slicing: Get a sub-section of a list, like <code>quarterly_sales[0:2]</code> for the first two quarters.</li> <li>Modifying Data: Lists are dynamic. Key methods include:</li> <li><code>.append()</code>: Adds an item to the end.</li> <li><code>.remove()</code>: Removes the first item with a specified value.</li> <li><code>.sort()</code>: Sorts the list in place, which is great for ranking.</li> </ul>"},{"location":"lessons/day-05-lists/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-05-lists/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>lists.py</code>, has been refactored into functions to make the logic for each list operation reusable and testable.</p> <ol> <li>Review the Code: Open <code>Day_05_Lists/lists.py</code>. Each list operation (e.g., <code>add_product()</code>, <code>analyze_team_sales()</code>) is now its own function. Notice that the functions return a new list rather than modifying the original, which is a good practice to avoid unexpected side effects.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_05_Lists/lists.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_05.py\n</code></pre></li> </ol>"},{"location":"lessons/day-05-lists/#exercises-day-5","title":"\ud83d\udcbb Exercises: Day 5","text":"<ol> <li> <p>Manage a Product List:</p> </li> <li> <p>In a new script (<code>my_solutions_05.py</code>), create a list of product names: <code>[\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\"]</code>.</p> </li> <li>A new product, \"Webcam\", is now in stock. Call the <code>add_product</code> function (you can import it) to get a new list with the webcam.</li> <li>The \"Mouse\" is sold out. Call the <code>remove_product</code> function on your new list.</li> <li> <p>Print the final list of available products.</p> </li> <li> <p>Analyze Monthly Expenses:</p> </li> <li> <p>Create a function <code>analyze_expenses(expenses)</code> that takes a list of numbers.</p> </li> <li>The function should return a dictionary containing the <code>total_expenses</code>, <code>highest_expense</code>, and <code>lowest_expense</code>.</li> <li> <p>Call the function with a list like <code>[2200, 2350, 2600, 2130, 2190]</code> and print the results.</p> </li> <li> <p>Select Top Sales Performers:</p> </li> <li> <p>The <code>analyze_team_sales</code> function in <code>lists.py</code> already does this.</p> </li> <li>Import it into your script: <code>from Day_05_Lists.lists import analyze_team_sales</code>.</li> <li>Call the function with a list of sales figures: <code>[5000, 8000, 4500, 12000, 6000, 11000]</code>.</li> <li>Print the <code>top_3_sales</code> from the dictionary that the function returns.</li> </ol> <p>\ud83c\udf89 Great job! Lists are the workhorse for storing collections of data in Python. Understanding how to manage and analyze data within lists is a fundamental skill for any data analyst.</p>"},{"location":"lessons/day-05-lists/#additional-materials","title":"Additional Materials","text":"<ul> <li>lists.ipynb</li> <li>solutions.ipynb</li> </ul> lists.py <p>View on GitHub</p> lists.py<pre><code>\"\"\"\nDay 5: Managing and Analyzing Business Data with Lists (Refactored)\n\nThis script demonstrates how to create, access, modify, and analyze\nlists containing business-related data. This version is refactored\ninto functions for better organization and testability.\n\"\"\"\n\n\ndef get_list_element(data_list, index):\n    \"\"\"Safely gets an element from a list by its index.\"\"\"\n    if -len(data_list) &lt;= index &lt; len(data_list):\n        return data_list[index]\n    return None\n\n\ndef get_first_half_sales(sales_list):\n    \"\"\"Returns the first half of a list of sales.\"\"\"\n    midpoint = len(sales_list) // 2\n    return sales_list[:midpoint]\n\n\ndef add_product(product_list, new_product):\n    \"\"\"Adds a new product to a list of products.\"\"\"\n    new_list = product_list.copy()\n    new_list.append(new_product)\n    return new_list\n\n\ndef remove_product(product_list, product_to_remove):\n    \"\"\"Removes a product from a list if it exists.\"\"\"\n    new_list = product_list.copy()\n    if product_to_remove in new_list:\n        new_list.remove(product_to_remove)\n    return new_list\n\n\ndef analyze_team_sales(sales_figures):\n    \"\"\"Sorts sales, finds top performers, and returns an analysis.\"\"\"\n    if not sales_figures:\n        return None\n\n    sorted_sales = sorted(sales_figures, reverse=True)\n    top_3_sales = sorted_sales[:3]\n    total_top_sales = sum(top_3_sales)\n\n    return {\n        \"sorted_sales\": sorted_sales,\n        \"top_3_sales\": top_3_sales,\n        \"total_top_sales\": total_top_sales,\n    }\n\n\nif __name__ == \"__main__\":\n    # --- Initializing Lists with Business Data ---\n    print(\"--- Initializing Business Lists ---\")\n    departments_list = [\"Sales\", \"Marketing\", \"Human Resources\", \"Engineering\"]\n    quarterly_sales_figures = [120000.50, 135000.75, 110000.00, 145000.25]\n    print(f\"Company Departments: {departments_list}\")\n    print(f\"Quarterly Sales: {quarterly_sales_figures}\")\n    print(\"-\" * 20)\n\n    # --- Accessing and Slicing List Data ---\n    print(\"--- Accessing Specific Data ---\")\n    marketing_department = get_list_element(departments_list, 1)\n    print(f\"The second department is: {marketing_department}\")\n\n    last_sales = get_list_element(quarterly_sales_figures, -1)\n    print(f\"Sales for the last quarter: ${last_sales}\")\n\n    first_half_figures = get_first_half_sales(quarterly_sales_figures)\n    print(f\"First half sales: {first_half_figures}\")\n    print(\"-\" * 20)\n\n    # --- Modifying Lists ---\n    print(\"--- Modifying a Product List ---\")\n    initial_products = [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\"]\n    print(f\"Original product list: {initial_products}\")\n\n    products_after_add = add_product(initial_products, \"Webcam\")\n    print(f\"After adding 'Webcam': {products_after_add}\")\n\n    products_after_remove = remove_product(products_after_add, \"Mouse\")\n    print(f\"After removing 'Mouse': {products_after_remove}\")\n    print(\"-\" * 20)\n\n    # --- Analyzing List Data ---\n    print(\"--- Analyzing Sales Performance ---\")\n    team_sales_figures = [5000, 8000, 4500, 12000, 6000, 11000]\n    print(f\"Sales figures for the team: {team_sales_figures}\")\n\n    sales_analysis = analyze_team_sales(team_sales_figures)\n    if sales_analysis:\n        print(f\"Sales sorted from highest to lowest: {sales_analysis['sorted_sales']}\")\n        print(f\"Top 3 sales figures: {sales_analysis['top_3_sales']}\")\n        print(\n            f\"Total sales from top 3 performers: ${sales_analysis['total_top_sales']}\"\n        )\n    print(\"-\" * 20)\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 5: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Manage a Product List ---\nprint(\"--- Solution to Exercise 1 ---\")\nproducts = [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\"]\nprint(f\"Initial product list: {products}\")\n\n# Add \"Webcam\" to the end of the list\nproducts.append(\"Webcam\")\nprint(f\"After adding 'Webcam': {products}\")\n\n# Remove \"Mouse\" from the list\nproducts.remove(\"Mouse\")\nprint(f\"After removing 'Mouse': {products}\")\nprint(f\"Final available products: {products}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Analyze Monthly Expenses ---\nprint(\"--- Solution to Exercise 2 ---\")\nmonthly_expenses = [2200, 2350, 2600, 2130, 2190]\nprint(f\"Initial monthly expenses: {monthly_expenses}\")\n\n# Find total, min, and max expenses\ntotal_expenses = sum(monthly_expenses)\nhighest_expense = max(monthly_expenses)\nlowest_expense = min(monthly_expenses)\n\nprint(f\"Total expenses: ${total_expenses}\")\nprint(f\"Highest monthly expense: ${highest_expense}\")\nprint(f\"Lowest monthly expense: ${lowest_expense}\")\n\n# Add a new expense and print the updated total\nnew_expense = 2400\nmonthly_expenses.append(new_expense)\nupdated_total_expenses = sum(monthly_expenses)\nprint(\n    f\"After adding a new expense of ${new_expense}, the new total is: ${updated_total_expenses}\"\n)\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Select Top Sales Performers ---\nprint(\"--- Solution to Exercise 3 ---\")\nsales_figures = [5000, 8000, 4500, 12000, 6000, 11000]\nprint(f\"Original sales figures: {sales_figures}\")\n\n# Sort the list in descending order (highest to lowest)\nsales_figures.sort(reverse=True)\nprint(f\"Sorted sales figures: {sales_figures}\")\n\n# \"Slice\" the list to get the top 3\ntop_3_sales = sales_figures[0:3]\n\nprint(f\"The top 3 sales figures are: {top_3_sales}\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-06-tuples/","title":"\ud83d\udcd8 Day 6: Tuples - Storing Immutable Business Data","text":"<p>While lists are great for data that changes, sometimes you need to store data that shouldn't change. For this, Python provides the tuple.</p>"},{"location":"lessons/day-06-tuples/#what-is-a-tuple","title":"What is a Tuple?","text":"<p>A tuple is an ordered, immutable collection of items. \"Immutable\" means once a tuple is created, it cannot be changed. This makes tuples perfect for protecting the integrity of fixed data records.</p> <pre><code># A tuple for a transaction record (ID, Date, Amount)\ntransaction = (1001, \"2024-03-15\", 499.99)\n</code></pre>"},{"location":"lessons/day-06-tuples/#why-use-a-tuple","title":"Why Use a Tuple?","text":"<ol> <li>Data Integrity: Prevents accidental modification of data that should be constant.</li> <li>Performance: Tuples are slightly more memory-efficient and faster than lists.</li> <li>Dictionary Keys: Tuples can be used as keys in dictionaries, whereas lists cannot.</li> </ol>"},{"location":"lessons/day-06-tuples/#unpacking-tuples","title":"Unpacking Tuples","text":"<p>A very common and elegant feature is \"unpacking,\" which lets you assign the items of a tuple to multiple variables at once.</p> <pre><code>trans_id, date, amount = transaction\n</code></pre>"},{"location":"lessons/day-06-tuples/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-06-tuples/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>tuples.py</code>, has been refactored to separate the logic for handling tuples into testable functions.</p> <ol> <li>Review the Code: Open <code>Day_06_Tuples/tuples.py</code>. Notice the functions <code>get_location_coordinates()</code> and <code>unpack_transaction()</code> that now contain the core logic.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_06_Tuples/tuples.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_06.py\n</code></pre></li> </ol>"},{"location":"lessons/day-06-tuples/#exercises-day-6","title":"\ud83d\udcbb Exercises: Day 6","text":"<ol> <li> <p>Store Geographic Coordinates:</p> </li> <li> <p>In a new script (<code>my_solutions_06.py</code>), a company's headquarters is located at latitude <code>40.7128</code> and longitude <code>-74.0060</code>.</p> </li> <li>Store these in a tuple called <code>hq_location</code>.</li> <li> <p>\"Unpack\" the tuple into <code>latitude</code> and <code>longitude</code> variables and print them.</p> </li> <li> <p>Define Product Dimensions:</p> </li> <li> <p>Create a function <code>format_dimensions(dims_tuple)</code> that takes a tuple of three numbers (length, width, height).</p> </li> <li>The function should return a formatted string like <code>\"Dimensions (LxWxH): 25cm x 15cm x 10cm\"</code>.</li> <li> <p>Call the function with a tuple like <code>(25, 15, 10)</code> and print the result.</p> </li> <li> <p>List vs. Tuple - The Right Tool for the Job:</p> </li> <li> <p>For each scenario below, decide if a list or a tuple is more appropriate and write a comment in your script explaining why.</p> <ul> <li>Scenario A: Storing the monthly sales figures for the past year.</li> <li>Scenario B: Storing the RGB color code for your company's logo.</li> <li>Scenario C: Storing the names of employees in a department.</li> </ul> </li> </ol> <p>\ud83c\udf89 Excellent! You've learned about immutability and how to use tuples to ensure your data remains constant. Knowing when to use a tuple versus a list is a sign of a thoughtful analyst.</p>"},{"location":"lessons/day-06-tuples/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> <li>tuples.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 6: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Store Geographic Coordinates ---\nprint(\"--- Solution to Exercise 1 ---\")\n# A tuple is perfect here because these coordinates are fixed.\nhq_location = (40.7128, -74.0060)\n\n# \"Unpacking\" the tuple into separate variables\nlatitude, longitude = hq_location\n\nprint(f\"Headquarters Location: {hq_location}\")\nprint(f\"Latitude: {latitude}\")\nprint(f\"Longitude: {longitude}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Define Product Dimensions ---\nprint(\"--- Solution to Exercise 2 ---\")\n# Dimensions are fixed, so a tuple is the right choice.\npackage_dimensions = (25, 15, 10)\n\n# Accessing the tuple elements by their index for the print statement.\nprint(\n    f\"Package Dimensions (LxWxH): {package_dimensions[0]}cm x {package_dimensions[1]}cm x {package_dimensions[2]}cm\"\n)\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: List vs. Tuple - The Right Tool for the Job ---\nprint(\"--- Solution to Exercise 3 ---\")\n\n# Scenario A: Storing the monthly sales figures for the past year.\n# Choice: List. Sales figures might need to be corrected or updated. A list is mutable.\nprint(\"Scenario A (Monthly Sales): Use a LIST because the data may need to be changed.\")\n\n# Scenario B: Storing the RGB color code for your company's official logo.\n# Choice: Tuple. A brand color is a constant and should not be accidentally changed. A tuple is immutable.\nprint(\n    \"Scenario B (Brand Color): Use a TUPLE because the data is constant and should not change.\"\n)\n\n# Scenario C: Storing the names of employees in a department.\n# Choice: List. The roster of employees in a department changes frequently. A list is mutable.\nprint(\n    \"Scenario C (Employee Roster): Use a LIST because the roster of employees changes over time.\"\n)\n\n# Scenario D: Storing the name, founding year, and stock ticker symbol for a company.\n# Choice: Tuple. This is core, identifying information that is fixed and should not change. A tuple is immutable.\nprint(\n    \"Scenario D (Company Profile): Use a TUPLE because this core information is fixed.\"\n)\nprint(\"-\" * 20)\n</code></pre> tuples.py <p>View on GitHub</p> tuples.py<pre><code>\"\"\"\nDay 6: Using Tuples for Immutable Business Data (Refactored)\n\nThis script demonstrates the creation and use of tuples to store\ndata that should not be changed, such as transaction records or\nfixed coordinates. This version is refactored into functions for\nbetter organization and testability.\n\"\"\"\n\n\ndef get_location_coordinates(location_tuple):\n    \"\"\"\n    Returns the latitude and longitude from a location tuple.\n    Assumes the tuple is in the format (latitude, longitude).\n    \"\"\"\n    if isinstance(location_tuple, tuple) and len(location_tuple) == 2:\n        return location_tuple[0], location_tuple[1]\n    return None, None\n\n\ndef unpack_transaction(transaction_tuple):\n    \"\"\"\n    Unpacks a transaction tuple into a dictionary.\n    Assumes tuple format is (id, date, amount).\n    \"\"\"\n    if isinstance(transaction_tuple, tuple) and len(transaction_tuple) == 3:\n        trans_id, date, amount = transaction_tuple\n        return {\"id\": trans_id, \"date\": date, \"amount\": amount}\n    return None\n\n\ndef demonstrate_list_vs_tuple():\n    \"\"\"\n    Prints scenarios demonstrating when to use a list vs. a tuple.\n    \"\"\"\n    print(\"--- Choosing Between a List and a Tuple ---\")\n\n    # Scenario A: Storing the monthly sales figures for the past year.\n    # Choice: List. Sales data is likely to be updated or amended.\n    monthly_sales = [45000, 52000, 48000, 55000]\n    print(\n        f\"Scenario A (Monthly Sales): Use a list. Data might change. Example: {monthly_sales}\"\n    )\n\n    # Scenario B: Storing the RGB color code for your company's official logo.\n    # Choice: Tuple. The brand color is a fixed constant and should not change.\n    brand_color_rgb = (45, 85, 150)\n    print(\n        f\"Scenario B (Brand Color): Use a tuple. Data is constant. Example: {brand_color_rgb}\"\n    )\n\n    # Scenario C: Storing the names of employees in a department.\n    # Choice: List. Employees can be added or removed from the department.\n    marketing_team = [\"Alice\", \"Bob\", \"Charlie\"]\n    print(\n        f\"Scenario C (Team Roster): Use a list. Roster changes. Example: {marketing_team}\"\n    )\n\n    # Scenario D: Storing the name, founding year, and stock ticker symbol for a company.\n    # Choice: Tuple. This core identifying information for a company is fixed.\n    company_profile = (\"InnovateCorp\", 2015, \"INVC\")\n    print(\n        f\"Scenario D (Company Profile): Use a tuple. Core info is fixed. Example: {company_profile}\"\n    )\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    # --- Using a Tuple for Fixed Data ---\n    print(\"--- Storing Fixed Location Data ---\")\n    hq_coords = (40.7128, -74.0060)\n    lat, lon = get_location_coordinates(hq_coords)\n    if lat is not None:\n        print(f\"Headquarters Latitude: {lat}\")\n        print(f\"Headquarters Longitude: {lon}\")\n    print()\n\n    # --- Unpacking Tuples for Readability ---\n    print(\"--- Unpacking a Transaction Record ---\")\n    transaction_data = (1001, \"2024-03-15\", 499.99)\n    unpacked_data = unpack_transaction(transaction_data)\n    if unpacked_data:\n        print(f\"Transaction ID: {unpacked_data['id']}\")\n        print(f\"Date: {unpacked_data['date']}\")\n        print(f\"Amount: ${unpacked_data['amount']}\")\n    print(\"-\" * 20)\n\n    # --- List vs. Tuple Demonstration ---\n    demonstrate_list_vs_tuple()\n</code></pre>"},{"location":"lessons/day-07-sets/","title":"\ud83d\udcd8 Day 7: Sets - Managing Unique Business Data","text":"<p>We've seen lists for ordered data and tuples for immutable data. Now we'll learn about sets, which are powerful for two main business reasons: ensuring uniqueness and performing membership analysis.</p>"},{"location":"lessons/day-07-sets/#what-is-a-set","title":"What is a Set?","text":"<p>A set is an unordered collection of unique items.</p> <ul> <li>Unordered: Items have no defined order.</li> <li>Unique: A set cannot contain duplicate items.</li> </ul> <p>This de-duplication feature is one of the most common uses for sets in data analysis.</p>"},{"location":"lessons/day-07-sets/#set-operations-the-foundation-of-segmentation","title":"Set Operations: The Foundation of Segmentation","text":"<p>The true power of sets comes from their mathematical operations, which are invaluable for customer segmentation and cohort analysis.</p> Operation Python Operator Business Question Answered Union <code>A | B</code> What is the total unique audience for two groups? Intersection <code>A &amp; B</code> Which customers are in both Group A and Group B? Difference <code>A - B</code> Which customers are in Group A but not in Group B?"},{"location":"lessons/day-07-sets/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-07-sets/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>sets.py</code>, has been refactored into functions to make the logic for de-duplication and segmentation reusable and testable.</p> <ol> <li>Review the Code: Open <code>Day_07_Sets/sets.py</code>. Notice the functions <code>get_unique_items()</code>, <code>analyze_visitor_segments()</code>, and <code>upgrade_plan_features()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_07_Sets/sets.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_07.py\n</code></pre></li> </ol>"},{"location":"lessons/day-07-sets/#exercises-day-7","title":"\ud83d\udcbb Exercises: Day 7","text":"<ol> <li> <p>Find Unique Customer Cities:</p> </li> <li> <p>In a new script (<code>my_solutions_07.py</code>), you have a list of cities: <code>order_cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"New York\", \"Boston\", \"Los Angeles\"]</code>.</p> </li> <li>Import the <code>get_unique_items</code> function from the lesson script.</li> <li> <p>Call the function with your list to get a set of unique cities and print the result.</p> </li> <li> <p>Analyze Website Visitor Activity:</p> </li> <li> <p>You have two sets of user IDs:</p> <ul> <li><code>pricing_visitors = {\"user1\", \"user3\", \"user5\", \"user7\"}</code></li> <li><code>contact_visitors = {\"user2\", \"user3\", \"user4\", \"user5\"}</code></li> </ul> </li> <li>Import the <code>analyze_visitor_segments</code> function.</li> <li>Call the function with these two sets.</li> <li> <p>Print the <code>intersection</code> and <code>difference_a_b</code> from the returned dictionary to find highly engaged users and users who only viewed pricing.</p> </li> <li> <p>Manage Product Features:</p> </li> <li> <p>Your \"Standard Plan\" has a set of features: <code>standard_features = {\"reporting\", \"data_export\", \"basic_support\"}</code>.</p> </li> <li>You want to add <code>[\"api_access\", \"priority_support\"]</code> for the \"Pro Plan\".</li> <li>Import and use the <code>upgrade_plan_features</code> function to create the new feature set for the Pro Plan.</li> <li>Print the resulting Pro Plan feature set.</li> </ol> <p>\ud83c\udf89 Well done! Sets are a specialized but incredibly efficient tool. When you need to de-duplicate a list or analyze the overlap between two groups, sets are the best tool for the job.</p>"},{"location":"lessons/day-07-sets/#additional-materials","title":"Additional Materials","text":"<ul> <li>sets.ipynb</li> <li>solutions.ipynb</li> </ul> sets.py <p>View on GitHub</p> sets.py<pre><code>\"\"\"\nDay 7: Using Sets for Unique Data and Segmentation (Refactored)\n\nThis script demonstrates how to use sets to de-duplicate data\nand perform segmentation analysis on business data. This version\nis refactored into functions for better organization and testability.\n\"\"\"\n\n\ndef get_unique_items(items_list):\n    \"\"\"Converts a list to a set to get unique items.\"\"\"\n    return set(items_list)\n\n\ndef analyze_visitor_segments(set_a, set_b):\n    \"\"\"\n    Performs intersection, difference, and union operations on two sets.\n    Returns a dictionary with the results.\n    \"\"\"\n    intersection = set_a.intersection(set_b)\n    difference = set_a.difference(set_b)\n    union = set_a.union(set_b)\n\n    return {\"intersection\": intersection, \"difference_a_b\": difference, \"union\": union}\n\n\ndef upgrade_plan_features(base_features, new_features_list):\n    \"\"\"\n    Adds new features to a base set of features.\n    \"\"\"\n    upgraded_plan = base_features.copy()\n    upgraded_plan.update(new_features_list)\n    return upgraded_plan\n\n\nif __name__ == \"__main__\":\n    # --- Using a Set to Find Unique Items ---\n    print(\"--- Finding Unique Customer Cities ---\")\n    order_cities_list = [\n        \"New York\",\n        \"Los Angeles\",\n        \"Chicago\",\n        \"New York\",\n        \"Boston\",\n        \"Los Angeles\",\n        \"Chicago\",\n    ]\n    print(f\"Original list of cities: {order_cities_list}\")\n    unique_cities_set = get_unique_items(order_cities_list)\n    print(f\"Unique cities set: {unique_cities_set}\")\n    print(f\"Number of unique cities: {len(unique_cities_set)}\")\n    print(\"-\" * 20)\n\n    # --- Using Set Operations for Customer Segmentation ---\n    print(\"--- Analyzing Website Visitor Segments ---\")\n    pricing_page_visitors = {\"user1\", \"user3\", \"user5\", \"user7\", \"user8\"}\n    contact_page_visitors = {\"user2\", \"user3\", \"user4\", \"user5\", \"user9\"}\n\n    segment_analysis = analyze_visitor_segments(\n        pricing_page_visitors, contact_page_visitors\n    )\n\n    print(\n        f\"Users who visited Pricing AND Contact pages: {segment_analysis['intersection']}\"\n    )\n    print(\n        f\"Users who only visited the Pricing page: {segment_analysis['difference_a_b']}\"\n    )\n    print(f\"All unique visitors to either page: {segment_analysis['union']}\")\n    print(\"-\" * 20)\n\n    # --- Modifying Sets to Manage Product Plans ---\n    print(\"--- Managing Product Plan Features ---\")\n    standard_plan_features = {\"reporting\", \"data_export\", \"basic_support\"}\n    print(f\"Standard Plan Features: {standard_plan_features}\")\n\n    features_to_add_for_pro = [\"api_access\", \"priority_support\", \"24/7_monitoring\"]\n    pro_plan_features = upgrade_plan_features(\n        standard_plan_features, features_to_add_for_pro\n    )\n\n    print(f\"Pro Plan Features: {pro_plan_features}\")\n\n    pro_only = pro_plan_features.difference(standard_plan_features)\n    print(f\"Features unique to the Pro Plan: {pro_only}\")\n    print(\"-\" * 20)\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 7: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Find Unique Customer Cities ---\nprint(\"--- Solution to Exercise 1 ---\")\norder_cities = [\n    \"New York\",\n    \"Los Angeles\",\n    \"Chicago\",\n    \"New York\",\n    \"Boston\",\n    \"Los Angeles\",\n]\nprint(f\"Original list of cities: {order_cities}\")\n\n# Converting the list to a set automatically removes duplicates\nunique_cities = set(order_cities)\n\nprint(f\"Set of unique cities: {unique_cities}\")\nprint(f\"Number of unique cities where orders were placed: {len(unique_cities)}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Analyze Website Visitor Activity ---\nprint(\"--- Solution to Exercise 2 ---\")\npricing_visitors = {\"user1\", \"user3\", \"user5\", \"user7\"}\ncontact_visitors = {\"user2\", \"user3\", \"user4\", \"user5\"}\n\nprint(f\"Pricing Page Visitors: {pricing_visitors}\")\nprint(f\"Contact Page Visitors: {contact_visitors}\")\n\n# Intersection: users who did both\nboth_pages_visitors = pricing_visitors.intersection(contact_visitors)\nprint(f\"Users who visited BOTH pages: {both_pages_visitors}\")\n\n# Difference: users who visited pricing but not contact\npricing_only_visitors = pricing_visitors.difference(contact_visitors)\nprint(f\"Users who visited Pricing but NOT Contact: {pricing_only_visitors}\")\n\n# Union: all unique users who visited either page\nall_visitors = pricing_visitors.union(contact_visitors)\nprint(f\"All unique visitors to either page: {all_visitors}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Manage Product Features ---\nprint(\"--- Solution to Exercise 3 ---\")\nstandard_features = {\"reporting\", \"data_export\", \"basic_support\"}\nprint(f\"Standard Plan Features: {standard_features}\")\n\n# Create a copy to avoid modifying the original set\npro_features = standard_features.copy()\n\n# New features to add\nnew_pro_features = [\"api_access\", \"priority_support\"]\npro_features.update(new_pro_features)\n\nprint(f\"Pro Plan Features after update: {pro_features}\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-08-dictionaries/","title":"\ud83d\udcd8 Day 8: Dictionaries - Structuring Complex Business Data","text":"<p>Real-world business data is structured. A customer has a name, an email, and a location. A product has a price, an SKU, and an inventory count. For this, we need dictionaries. A dictionary is the most important data structure for handling structured data in Python.</p>"},{"location":"lessons/day-08-dictionaries/#what-is-a-dictionary","title":"What is a Dictionary?","text":"<p>A dictionary is an unordered collection of key-value pairs.</p> <ul> <li>Key: A unique identifier for a piece of data (e.g., <code>\"first_name\"</code>).</li> <li>Value: The data itself (e.g., <code>\"John\"</code>).</li> </ul> <pre><code>customer = {\n    \"customer_id\": \"CUST-001\",\n    \"first_name\": \"John\",\n    \"last_name\": \"Doe\",\n}\n</code></pre>"},{"location":"lessons/day-08-dictionaries/#key-dictionary-operations","title":"Key Dictionary Operations","text":"<ul> <li>Accessing Data: Use the key in square brackets (<code>customer[\"first_name\"]</code>). For safer access, use the <code>.get()</code> method (<code>customer.get(\"phone\", \"N/A\")</code>), which returns a default value if the key doesn't exist.</li> <li>Adding/Modifying: Assign a value to a key (<code>customer[\"phone\"] = \"555-123-4567\"</code>).</li> <li>Removing: Use the <code>del</code> keyword (<code>del customer[\"company\"]</code>).</li> <li>Nesting: Dictionaries can contain other dictionaries or lists, allowing you to model complex structures like an employee profile with nested contact info and a list of projects.</li> </ul>"},{"location":"lessons/day-08-dictionaries/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-08-dictionaries/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>dictionaries.py</code>, has been refactored to encapsulate dictionary operations into testable functions.</p> <ol> <li>Review the Code: Open <code>Day_08_Dictionaries/dictionaries.py</code>. Notice the functions like <code>create_customer_profile()</code>, <code>update_customer_record()</code>, and <code>add_project_to_employee()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_08_Dictionaries/dictionaries.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_08.py\n</code></pre></li> </ol>"},{"location":"lessons/day-08-dictionaries/#exercises-day-8","title":"\ud83d\udcbb Exercises: Day 8","text":"<ol> <li> <p>Create a Product Dictionary:</p> </li> <li> <p>In a new script (<code>my_solutions_08.py</code>), create a function <code>create_product(product_id, name, price, in_stock, tags)</code>.</p> </li> <li>The function should accept these arguments and return a dictionary representing a product.</li> <li> <p>Call the function with sample data (e.g., <code>PROD-123</code>, <code>SuperWidget</code>, <code>199.99</code>, <code>True</code>, <code>[\"electronics\", \"gadget\"]</code>) and print the resulting dictionary.</p> </li> <li> <p>Modify Employee Information:</p> </li> <li> <p>Start with the <code>employee_record</code> dictionary from the lesson's main block.</p> </li> <li>Import the <code>add_project_to_employee</code> function.</li> <li>Call the function to add a new project, \"2025 Strategy,\" to the employee's project list.</li> <li>Separately, update the <code>department</code> key of your new dictionary to \"Marketing Director\".</li> <li> <p>Print the final, updated employee dictionary.</p> </li> <li> <p>Access Nested Data:</p> </li> <li> <p>Create a dictionary for a <code>company</code> with keys <code>company_name</code> and <code>headquarters</code>.</p> </li> <li>The value for <code>headquarters</code> should be another dictionary with keys for <code>city</code>, <code>state</code>, and <code>country</code>.</li> <li>Write a script that creates this dictionary and then prints a sentence like: <code>\"[Company Name] is headquartered in [City], [State].\"</code>, accessing the nested values to build the string.</li> </ol> <p>\ud83c\udf89 Amazing work! Dictionaries are the cornerstone of handling structured data in Python. Almost every time you get data from an API or a database, it will be in the form of dictionaries. Mastering them is a huge step forward.</p>"},{"location":"lessons/day-08-dictionaries/#additional-materials","title":"Additional Materials","text":"<ul> <li>dictionaries.ipynb</li> <li>solutions.ipynb</li> </ul> dictionaries.py <p>View on GitHub</p> dictionaries.py<pre><code>\"\"\"\nDay 8: Structuring Business Data with Dictionaries (Refactored)\n\nThis script demonstrates how to create, access, and modify\ndictionaries that represent complex business records. This version\nis refactored into functions for better organization and testability.\n\"\"\"\n\n\ndef create_customer_profile(cust_id, first, last, email, company, is_premium, spent):\n    \"\"\"Creates a dictionary representing a customer profile.\"\"\"\n    return {\n        \"customer_id\": cust_id,\n        \"first_name\": first,\n        \"last_name\": last,\n        \"email\": email,\n        \"company\": company,\n        \"is_premium_member\": is_premium,\n        \"total_spent\": spent,\n    }\n\n\ndef get_customer_attribute(customer_profile, attribute, default_value=\"N/A\"):\n    \"\"\"Safely gets an attribute from a customer profile, returning a default if not found.\"\"\"\n    return customer_profile.get(attribute, default_value)\n\n\ndef update_customer_record(customer_profile, key, value, is_new=False):\n    \"\"\"Updates or adds a key-value pair to a customer profile.\"\"\"\n    profile_copy = customer_profile.copy()\n    if is_new:\n        profile_copy[key] = value\n    else:\n        # Assumes the key exists and we are modifying it (e.g., incrementing)\n        if key in profile_copy:\n            profile_copy[key] += value\n    return profile_copy\n\n\ndef remove_customer_attribute(customer_profile, attribute):\n    \"\"\"Removes an attribute from a customer profile if it exists.\"\"\"\n    profile_copy = customer_profile.copy()\n    if attribute in profile_copy:\n        del profile_copy[attribute]\n    return profile_copy\n\n\ndef add_project_to_employee(employee_profile, new_project):\n    \"\"\"Adds a new project to an employee's project list.\"\"\"\n    profile_copy = employee_profile.copy()\n    # Ensure the 'projects' key exists and is a list\n    if \"projects\" in profile_copy and isinstance(profile_copy[\"projects\"], list):\n        # To avoid modifying a list within the original dict, we copy it too\n        profile_copy[\"projects\"] = profile_copy[\"projects\"].copy()\n        profile_copy[\"projects\"].append(new_project)\n    return profile_copy\n\n\nif __name__ == \"__main__\":\n    # --- Creating a Dictionary for a Customer Profile ---\n    print(\"--- Customer Profile Dictionary ---\")\n    customer_record = create_customer_profile(\n        \"CUST-001\", \"John\", \"Doe\", \"john.doe@example.com\", \"InnovateCorp\", True, 2500.75\n    )\n    print(f\"Original customer record: {customer_record}\")\n\n    # Accessing data using keys\n    email_address = get_customer_attribute(customer_record, \"email\")\n    print(f\"Customer's email: {email_address}\")\n\n    # Safer access for a key that might not exist\n    phone_num = get_customer_attribute(customer_record, \"phone\")\n    print(f\"Customer's phone: {phone_num}\")\n    print()\n\n    # --- Modifying a Dictionary ---\n    print(\"--- Modifying Customer Record ---\")\n    customer_record_with_phone = update_customer_record(\n        customer_record, \"phone\", \"555-123-4567\", is_new=True\n    )\n    print(\n        f\"Added phone number: {get_customer_attribute(customer_record_with_phone, 'phone')}\"\n    )\n\n    customer_record_updated_spent = update_customer_record(\n        customer_record_with_phone, \"total_spent\", 500.25\n    )\n    print(f\"Updated total spent: ${customer_record_updated_spent['total_spent']:.2f}\")\n\n    customer_record_no_premium = remove_customer_attribute(\n        customer_record_updated_spent, \"is_premium_member\"\n    )\n    print(f\"Record after removing premium status: {customer_record_no_premium}\")\n    print(\"-\" * 20)\n\n    # --- Nested Dictionaries for Complex Structures ---\n    print(\"--- Nested Dictionary for an Employee Profile ---\")\n    employee_record = {\n        \"employee_id\": \"EMP-042\",\n        \"name\": \"Jane Smith\",\n        \"department\": \"Marketing\",\n        \"contact_info\": {\"email\": \"jane.smith@example.com\", \"phone_ext\": 112},\n        \"projects\": [\"Q1 Campaign\", \"Website Redesign\"],\n    }\n\n    employee_email_address = employee_record.get(\"contact_info\", {}).get(\"email\")\n    print(f\"Employee Email: {employee_email_address}\")\n\n    first_proj = employee_record.get(\"projects\", [\"N/A\"])[0]\n    print(f\"First project assigned: {first_proj}\")\n\n    employee_with_new_project = add_project_to_employee(\n        employee_record, \"2025 Strategy\"\n    )\n    print(f\"Updated projects: {employee_with_new_project['projects']}\")\n    print(\"-\" * 20)\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 8: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Create a Product Dictionary ---\nprint(\"--- Solution to Exercise 1 ---\")\nproduct = {\n    \"product_id\": \"PROD-123\",\n    \"product_name\": \"SuperWidget\",\n    \"price\": 199.99,\n    \"in_stock\": True,\n    \"tags\": [\"electronics\", \"gadget\"],\n}\n\n# Accessing the values using their keys\nprint(f\"Product Name: {product['product_name']}\")\nprint(f\"Price: ${product['price']}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Modify Employee Information ---\nprint(\"--- Solution to Exercise 2 ---\")\nemployee = {\n    \"employee_id\": \"EMP-042\",\n    \"name\": \"Jane Smith\",\n    \"department\": \"Marketing\",\n    \"contact_info\": {\"email\": \"jane.smith@example.com\", \"phone_ext\": 112},\n    \"projects\": [\"Q1 Campaign\", \"Website Redesign\"],\n}\nprint(f\"Original Employee Record: {employee}\")\n\n# Update the department\nemployee[\"department\"] = \"Marketing Director\"\n\n# Add a new project to the list of projects\nemployee[\"projects\"].append(\"2025 Strategy\")\n\nprint(f\"Updated Employee Record: {employee}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Access Nested Data ---\nprint(\"--- Solution to Exercise 3 ---\")\ncompany = {\n    \"company_name\": \"InnovateCorp\",\n    \"year_founded\": 2015,\n    \"headquarters\": {\"city\": \"New York\", \"state\": \"NY\", \"country\": \"USA\"},\n}\n\n# Accessing the nested dictionary and then the values within it\ncompany_name = company[\"company_name\"]\nhq_city = company[\"headquarters\"][\"city\"]\nhq_state = company[\"headquarters\"][\"state\"]\n\nprint(f\"{company_name} is headquartered in {hq_city}, {hq_state}.\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-09-conditionals/","title":"\ud83d\udcd8 Day 9: Conditionals - Implementing Business Logic","text":"<p>Business is full of rules and decisions. \"If a customer spends over $100, they get a 10% discount.\" \"If inventory is below 50 units, flag it for reorder.\" In Python, we implement this decision-making process using conditional statements.</p>"},{"location":"lessons/day-09-conditionals/#key-conditional-statements","title":"Key Conditional Statements","text":"<ul> <li><code>if</code>: Executes a block of code only if a certain condition is <code>True</code>.</li> <li><code>else</code>: Provides an alternative block of code to execute if the <code>if</code> condition is <code>False</code>.</li> <li><code>elif</code> (\"else if\"): Lets you check for multiple, sequential conditions. Python executes the first block where the condition is <code>True</code> and then skips the rest.</li> </ul> <pre><code># Classifying a customer based on their total spending\nif total_spent &gt; 1000:\n    customer_tier = \"Gold\"\nelif total_spent &gt; 500:\n    customer_tier = \"Silver\"\nelse:\n    customer_tier = \"Standard\"\n</code></pre>"},{"location":"lessons/day-09-conditionals/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-09-conditionals/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>conditionals.py</code>, has been refactored to encapsulate each business rule into a separate, testable function.</p> <ol> <li>Review the Code: Open <code>Day_09_Conditionals/conditionals.py</code>. Notice the functions <code>calculate_discount_percent()</code>, <code>calculate_shipping_cost()</code>, and <code>calculate_employee_bonus()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_09_Conditionals/conditionals.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function under various conditions:    <pre><code>pytest tests/test_day_09.py\n</code></pre></li> </ol>"},{"location":"lessons/day-09-conditionals/#exercises-day-9","title":"\ud83d\udcbb Exercises: Day 9","text":"<ol> <li> <p>Discount Policy Automation:</p> </li> <li> <p>In a new script (<code>my_solutions_09.py</code>), import the <code>calculate_discount_percent</code> function.</p> </li> <li>Define a <code>purchase_amount</code> variable.</li> <li>Call the function to get the discount rate, then calculate the final price (<code>purchase_amount * (1 - discount_rate)</code>).</li> <li> <p>Print the original amount, the discount rate, and the final price. Test it with amounts like <code>120</code>, <code>75</code>, and <code>40</code>.</p> </li> <li> <p>Shipping Cost Calculator:</p> </li> <li> <p>Import the <code>calculate_shipping_cost</code> function.</p> </li> <li> <p>Call the function with different combinations of countries (<code>\"USA\"</code>, <code>\"Canada\"</code>, <code>\"Mexico\"</code>) and weights (<code>40</code>, <code>60</code>) to see the results. Print a user-friendly message for each case.</p> </li> <li> <p>Employee Bonus Calculation:</p> </li> <li> <p>Import the <code>calculate_employee_bonus</code> function.</p> </li> <li>Test the function by calling it with different scenarios and printing the result:<ul> <li>A \"Sales\" employee with a <code>performance_rating</code> of 5.</li> <li>An \"Engineering\" employee with a <code>performance_rating</code> of 4.</li> <li>Any employee with a rating of 2.</li> </ul> </li> </ol> <p>\ud83c\udf89 Fantastic progress! You can now translate complex business rules into code that makes decisions automatically. This is a fundamental skill for automating reports and building analytical models.</p>"},{"location":"lessons/day-09-conditionals/#additional-materials","title":"Additional Materials","text":"<ul> <li>conditionals.ipynb</li> <li>solutions.ipynb</li> </ul> conditionals.py <p>View on GitHub</p> conditionals.py<pre><code>\"\"\"\nDay 9: Implementing Business Logic with Conditionals (Refactored)\n\nThis script demonstrates how to use if, elif, and else statements\nto create business rules and make decisions in code. This version is\nrefactored into functions for better organization and testability.\n\"\"\"\n\n\ndef calculate_discount_percent(purchase_amount):\n    \"\"\"Calculates a discount percentage based on the purchase amount.\"\"\"\n    if not isinstance(purchase_amount, (int, float)) or purchase_amount &lt; 0:\n        return 0.0\n\n    if purchase_amount &gt; 100.00:\n        return 0.10  # 10% discount\n    elif purchase_amount &gt; 50.00:\n        return 0.05  # 5% discount\n    else:\n        return 0.00  # 0% discount\n\n\ndef calculate_shipping_cost(country, order_weight_kg):\n    \"\"\"Calculates shipping cost based on destination and weight.\"\"\"\n    if country == \"USA\":\n        if order_weight_kg &gt; 50:\n            return 75\n        else:\n            return 50\n    elif country == \"Canada\":\n        if order_weight_kg &gt; 50:\n            return 100\n        else:\n            return 65\n    else:\n        return -1  # Using -1 to indicate not available\n\n\ndef calculate_employee_bonus(performance_rating, department, salary):\n    \"\"\"Calculates an employee's bonus based on performance and department.\"\"\"\n    if performance_rating &gt;= 4:\n        if department == \"Sales\":\n            return salary * 0.15\n        else:\n            return salary * 0.10\n    elif performance_rating == 3:\n        return salary * 0.05\n    else:\n        return 0.0\n\n\nif __name__ == \"__main__\":\n    # --- Example 1: Customer Discount Policy ---\n    print(\"--- Customer Discount Calculator ---\")\n    customer_purchase = 125.50\n    discount_rate = calculate_discount_percent(customer_purchase)\n    discount = customer_purchase * discount_rate\n    final = customer_purchase - discount\n\n    print(f\"Original Price: ${customer_purchase:.2f}\")\n    print(f\"Discount ({discount_rate * 100}%): ${discount:.2f}\")\n    print(f\"Final Price: ${final:.2f}\")\n    print(\"-\" * 20)\n\n    # --- Example 2: Nested Conditionals for Shipping Costs ---\n    print(\"--- Shipping Cost Calculator ---\")\n    shipping_country = \"Canada\"\n    weight = 60\n    cost = calculate_shipping_cost(shipping_country, weight)\n\n    if cost != -1:\n        print(f\"Shipping to {shipping_country} for a {weight}kg package costs: ${cost}\")\n    else:\n        print(f\"Sorry, shipping to {shipping_country} is not available.\")\n    print(\"-\" * 20)\n\n    # --- Example 3: Complex Bonus Calculation ---\n    print(\"--- Employee Bonus Calculator ---\")\n    emp_rating = 5\n    emp_dept = \"Sales\"\n    emp_salary = 80000\n    bonus_amount = calculate_employee_bonus(emp_rating, emp_dept, emp_salary)\n\n    print(\n        f\"Employee in {emp_dept} with rating {emp_rating} gets a bonus of: ${bonus_amount:.2f}\"\n    )\n    print(\"-\" * 20)\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 9: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Discount Policy Automation ---\nprint(\"--- Solution to Exercise 1 ---\")\npurchase_amount = 120\n\nif purchase_amount &gt; 100:\n    discount = 0.10\nelif purchase_amount &gt; 50:\n    discount = 0.05\nelse:\n    discount = 0.0\n\nfinal_price = purchase_amount * (1 - discount)\n\nprint(f\"Original Amount: ${purchase_amount:.2f}\")\nprint(f\"Discount Rate: {discount * 100}%\")\nprint(f\"Final Price: ${final_price:.2f}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Shipping Cost Calculator ---\nprint(\"--- Solution to Exercise 2 ---\")\n\n\n# We can wrap this in a function to easily test different scenarios\ndef get_shipping_cost(country, order_weight_kg):\n    cost = 0\n    if country == \"USA\":\n        if order_weight_kg &gt; 50:\n            cost = 75\n        else:\n            cost = 50\n    elif country == \"Canada\":\n        if order_weight_kg &gt; 50:\n            cost = 100\n        else:\n            cost = 65\n    else:\n        return \"Shipping not available.\"\n\n    return f\"Shipping cost: ${cost}\"\n\n\n# Test cases\nprint(f\"USA, 60kg -&gt; {get_shipping_cost('USA', 60)}\")\nprint(f\"Canada, 40kg -&gt; {get_shipping_cost('Canada', 40)}\")\nprint(f\"Mexico, 30kg -&gt; {get_shipping_cost('Mexico', 30)}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Employee Bonus Calculation ---\nprint(\"--- Solution to Exercise 3 ---\")\n\n\ndef calculate_bonus(rating, department, salary):\n    bonus_rate = 0\n    if rating &gt;= 4:\n        if department == \"Sales\":\n            bonus_rate = 0.15\n        else:\n            bonus_rate = 0.10\n    elif rating == 3:\n        bonus_rate = 0.05\n    # No need for an else for rating 1 or 2, as bonus_rate is already 0\n\n    bonus_amount = salary * bonus_rate\n    return bonus_amount\n\n\n# Test cases\nsalary = 90000\nprint(\n    f\"Sales employee with rating 5 gets bonus: ${calculate_bonus(5, 'Sales', salary):,.2f}\"\n)\nprint(\n    f\"Engineering employee with rating 4 gets bonus: ${calculate_bonus(4, 'Engineering', salary):,.2f}\"\n)\nprint(\n    f\"Sales employee with rating 3 gets bonus: ${calculate_bonus(3, 'Sales', salary):,.2f}\"\n)\nprint(f\"HR employee with rating 2 gets bonus: ${calculate_bonus(2, 'HR', salary):,.2f}\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-10-loops/","title":"\ud83d\udcd8 Day 10: Loops - Automating Repetitive Business Tasks","text":"<p>What if you have a list of 10,000 sales transactions? You won't write code for each one. This is where loops come in. Loops allow you to perform the same action on every item in a collection, automating what would otherwise be an impossibly tedious task.</p>"},{"location":"lessons/day-10-loops/#key-loop-types","title":"Key Loop Types","text":"<ul> <li><code>for</code> loop: The most common loop. It iterates over a sequence (like a list or dictionary) and executes a block of code for each item.   <pre><code># Calculate total revenue from a list of sales\ntotal_revenue = 0\nfor sale in monthly_sales:\n    total_revenue += sale\n</code></pre></li> <li><code>while</code> loop: Runs as long as a certain condition is <code>True</code>. It's used when you don't know beforehand how many times you need to loop, such as in a simulation.</li> </ul>"},{"location":"lessons/day-10-loops/#combining-loops-and-conditionals","title":"Combining Loops and Conditionals","text":"<p>The real power comes from combining loops with <code>if</code> statements. This allows you to perform an action on only the items that meet a certain criteria, which is the foundation of filtering and transforming data.</p> <pre><code># Find all transactions over a certain amount\nlarge_transactions = []\nfor amount in transactions:\n    if amount &gt; 500:\n        large_transactions.append(amount)\n</code></pre>"},{"location":"lessons/day-10-loops/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-10-loops/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>loops.py</code>, has been refactored to place each looping task into its own testable function.</p> <ol> <li>Review the Code: Open <code>Day_10_Loops/loops.py</code>. Notice the functions <code>calculate_total_from_list()</code>, <code>filter_high_value_customers()</code>, and <code>simulate_investment_growth()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_10_Loops/loops.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_10.py\n</code></pre></li> </ol>"},{"location":"lessons/day-10-loops/#exercises-day-10","title":"\ud83d\udcbb Exercises: Day 10","text":"<ol> <li> <p>Calculate Average Employee Age:</p> </li> <li> <p>In a new script (<code>my_solutions_10.py</code>), create a list of employee dictionaries: <code>employees = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 45}, {\"name\": \"Charlie\", \"age\": 35}]</code>.</p> </li> <li>Create a function <code>calculate_average_age(employee_list)</code> that takes a list of employee dictionaries.</li> <li>Inside the function, use a <code>for</code> loop to get the sum of all ages.</li> <li>Return the average age (<code>total_age / number_of_employees</code>).</li> <li> <p>Call your function and print the result.</p> </li> <li> <p>Filter High-Priority Customers:</p> </li> <li> <p>You have a list of customer dictionaries (see <code>loops.py</code> for an example).</p> </li> <li>Import the <code>filter_high_value_customers</code> function from the lesson script.</li> <li> <p>Call the function with your list of customers and print the result.</p> </li> <li> <p>Inventory Stock Alert:</p> </li> <li> <p>You have a dictionary of product inventory: <code>inventory = {\"Laptops\": 15, \"Mice\": 150, \"Keyboards\": 45}</code>.</p> </li> <li>Import the <code>check_inventory_levels</code> function.</li> <li>Call the function to get a list of low-stock items.</li> <li>Loop through the returned list and print an alert for each item.</li> </ol> <p>\ud83c\udf89 Incredible! The combination of loops and conditionals is the foundation of almost all data processing and analysis tasks. You've completed the core structures of Python!</p>"},{"location":"lessons/day-10-loops/#additional-materials","title":"Additional Materials","text":"<ul> <li>loops.ipynb</li> <li>solutions.ipynb</li> </ul> loops.py <p>View on GitHub</p> loops.py<pre><code>\"\"\"\nDay 10: Automating Tasks with Loops (Refactored)\n\nThis script demonstrates how to use for and while loops to\nprocess collections of business data automatically. This version\nis refactored into functions for better organization and testability.\n\"\"\"\n\n\ndef calculate_total_from_list(numbers_list):\n    \"\"\"Calculates the sum of all numbers in a list using a for loop.\"\"\"\n    total = 0\n    for number in numbers_list:\n        total += number\n    return total\n\n\ndef filter_high_value_customers(customers_list, threshold=2000):\n    \"\"\"\n    Filters a list of customer dictionaries to find those who spent above a threshold.\n    Returns a list of their names.\n    \"\"\"\n    high_priority = []\n    for customer in customers_list:\n        if customer.get(\"total_spent\", 0) &gt; threshold:\n            high_priority.append(customer.get(\"name\"))\n    return high_priority\n\n\ndef check_inventory_levels(inventory_dict, threshold=50):\n    \"\"\"\n    Checks an inventory dictionary and returns a list of products\n    that are below a specified stock threshold.\n    \"\"\"\n    low_stock_alerts = []\n    for product, count in inventory_dict.items():\n        if count &lt; threshold:\n            low_stock_alerts.append(product)\n    return low_stock_alerts\n\n\ndef simulate_investment_growth(initial_investment, target_amount, interest_rate):\n    \"\"\"\n    Simulates the number of years it takes for an investment to reach a target.\n    Returns the number of years.\n    \"\"\"\n    if initial_investment &lt;= 0 or interest_rate &lt;= 0:\n        return -1  # Indicate invalid input\n\n    investment = initial_investment\n    years = 0\n    while investment &lt; target_amount:\n        investment *= 1 + interest_rate\n        years += 1\n    return years\n\n\nif __name__ == \"__main__\":\n    # --- Using a for loop to aggregate data ---\n    print(\"--- Calculating Total Monthly Revenue ---\")\n    sales_data = [2340.50, 3100.25, 2900.00, 4500.75]\n    total_rev = calculate_total_from_list(sales_data)\n    print(f\"Total Revenue for the month: ${total_rev:.2f}\")\n    print(\"-\" * 20)\n\n    # --- Using a for loop with a conditional to filter data ---\n    print(\"--- Filtering High-Value Customers ---\")\n    customer_data = [\n        {\"name\": \"InnovateCorp\", \"total_spent\": 5500},\n        {\"name\": \"DataDriven Inc.\", \"total_spent\": 1200},\n        {\"name\": \"Analytics LLC\", \"total_spent\": 2100},\n        {\"name\": \"Global Solutions\", \"total_spent\": 850},\n    ]\n    priority_customers = filter_high_value_customers(customer_data)\n    print(f\"High-priority customers to contact: {priority_customers}\")\n    print(\"-\" * 20)\n\n    # --- Looping through a dictionary for inventory alerts ---\n    print(\"--- Inventory Stock Level Alerts ---\")\n    inventory_levels = {\"Laptops\": 15, \"Mice\": 150, \"Keyboards\": 45, \"Monitors\": 25}\n    low_stock_items = check_inventory_levels(inventory_levels)\n    for item in low_stock_items:\n        print(\n            f\"ALERT: {item} are low on stock ({inventory_levels[item]} units remaining).\"\n        )\n    print(\"-\" * 20)\n\n    # --- Using a while loop for financial simulation ---\n    print(\"--- Investment Growth Simulation ---\")\n    initial = 10000\n    target_val = 20000\n    rate = 0.07\n    years_to_double = simulate_investment_growth(initial, target_val, rate)\n    print(\n        f\"It will take {years_to_double} years for the initial investment of ${initial} to double at a {rate * 100}% interest rate.\"\n    )\n    print(\"-\" * 20)\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 10: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Calculate Average Employee Age ---\nprint(\"--- Solution to Exercise 1 ---\")\nemployees = [\n    {\"name\": \"Alice\", \"age\": 30},\n    {\"name\": \"Bob\", \"age\": 45},\n    {\"name\": \"Charlie\", \"age\": 35},\n    {\"name\": \"Diana\", \"age\": 28},\n]\n\ntotal_age = 0\nfor employee in employees:\n    total_age += employee[\"age\"]\n\n# Ensure we don't divide by zero if the list is empty\nif len(employees) &gt; 0:\n    average_age = total_age / len(employees)\n    print(f\"The average age of employees is: {average_age:.1f} years\")\nelse:\n    print(\"No employees in the list.\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Filter High-Priority Customers ---\nprint(\"--- Solution to Exercise 2 ---\")\ncustomers = [\n    {\"name\": \"InnovateCorp\", \"total_spent\": 5500},\n    {\"name\": \"DataDriven Inc.\", \"total_spent\": 1200},\n    {\"name\": \"Analytics LLC\", \"total_spent\": 2100},\n    {\"name\": \"Key Insights\", \"total_spent\": 1800},\n]\n\nhigh_priority_customers = []\nfor customer in customers:\n    if customer[\"total_spent\"] &gt; 2000:\n        high_priority_customers.append(customer[\"name\"])\n\nprint(f\"High-priority customers (spent &gt; $2000): {high_priority_customers}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Inventory Stock Alert ---\nprint(\"--- Solution to Exercise 3 ---\")\ninventory = {\"Laptops\": 15, \"Mice\": 150, \"Keyboards\": 45, \"Monitors\": 25}\nlow_stock_threshold = 50\n\nprint(\"Checking inventory levels...\")\nfor product, count in inventory.items():\n    if count &lt; low_stock_threshold:\n        print(f\"  - ALERT: {product} are low on stock ({count} units remaining).\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-11-functions/","title":"\ud83d\udcd8 Day 11: Functions - Creating Reusable Business Tools","text":"<p>As you perform more complex analysis, you'll write the same code repeatedly. Functions are named, reusable blocks of code that perform a specific task, helping you avoid repetition and write cleaner, more maintainable code.</p>"},{"location":"lessons/day-11-functions/#key-function-concepts","title":"Key Function Concepts","text":"<ul> <li>Definition (<code>def</code>): You define a function using the <code>def</code> keyword.</li> <li>Parameters: Inputs to your function that allow you to pass data to it (e.g., <code>revenue</code>, <code>expenses</code>).</li> <li><code>return</code> Statement: Sends a value back from the function, so you can use the result in other parts of your code.</li> <li>Type Hinting: A modern Python feature that lets you specify the expected data types for parameters and the return value (e.g., <code>def get_net_profit(revenue: float) -&gt; float:</code>). This improves code clarity and helps prevent bugs.</li> </ul>"},{"location":"lessons/day-11-functions/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-11-functions/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>functions.py</code>, is already well-structured, with each piece of business logic encapsulated in its own function. We've made a minor improvement by wrapping the example usage in a <code>main()</code> function, which is a standard convention.</p> <ol> <li>Review the Code: Open <code>Day_11_Functions/functions.py</code>. Examine the different functions like <code>get_net_profit()</code>, <code>calculate_commission()</code>, and <code>format_currency()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_11_Functions/functions.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_11.py\n</code></pre></li> </ol>"},{"location":"lessons/day-11-functions/#exercises-day-11","title":"\ud83d\udcbb Exercises: Day 11","text":"<ol> <li> <p>Commission Calculator Function:</p> </li> <li> <p>The <code>calculate_commission</code> function is already created in <code>functions.py</code>.</p> </li> <li>In a new script (<code>my_solutions_11.py</code>), import this function: <code>from Day_11_Functions.functions import calculate_commission</code>.</li> <li> <p>Call the function with a sample sales amount and print the returned commission.</p> </li> <li> <p>Employee Bonus Eligibility Function:</p> </li> <li> <p>Import the <code>is_eligible_for_bonus</code> function.</p> </li> <li> <p>Call the function with a few different scenarios for <code>performance_rating</code> and <code>years_of_service</code> and print the <code>True</code>/<code>False</code> results.</p> </li> <li> <p>Advanced Currency Formatter:</p> </li> <li> <p>Create a new function <code>format_currency_with_symbol(amount, symbol='$')</code>.</p> </li> <li>This function should take an amount and an optional <code>symbol</code> parameter that defaults to <code>'$'</code>.</li> <li>It should return a formatted string like <code>\u20ac1,250.50</code> if the symbol is <code>'\u20ac'</code>.</li> <li>Test your new function by calling it with and without the <code>symbol</code> parameter.</li> </ol> <p>\ud83c\udf89 Great work! Functions are the key to writing clean, organized, and professional code. By packaging your logic into reusable tools, you're moving from a simple scripter to a true programmer.</p>"},{"location":"lessons/day-11-functions/#additional-materials","title":"Additional Materials","text":"<ul> <li>functions.ipynb</li> <li>solutions.ipynb</li> </ul> functions.py <p>View on GitHub</p> functions.py<pre><code>\"\"\"\nDay 11: Building Reusable Business Tools with Functions (Refactored)\n\nThis script demonstrates how to define and call functions\nto perform repeatable business calculations.\n\"\"\"\n\n\n# Adding type hints (e.g., revenue: float) makes the function's expected inputs clear.\n# The \"-&gt; float\" indicates that the function is expected to return a float value.\ndef get_net_profit(revenue: float, expenses: float) -&gt; float:\n    \"\"\"Calculates the net profit from revenue and expenses.\"\"\"\n    net_profit = revenue - expenses\n    return net_profit\n\n\ndef calculate_commission(sales_amount: float) -&gt; float:\n    \"\"\"Calculates a 15% commission on a given sales amount.\"\"\"\n    commission_rate = 0.15\n    commission = sales_amount * commission_rate\n    return commission\n\n\ndef is_eligible_for_bonus(performance_rating: int, years_of_service: int) -&gt; bool:\n    \"\"\"\n    Checks if an employee is eligible for a bonus based on performance\n    and years of service.\n    \"\"\"\n    # Returns True if rating is 4 or 5 AND service is more than 2 years.\n    return performance_rating &gt;= 4 and years_of_service &gt; 2\n\n\ndef format_currency(amount: float) -&gt; str:\n    \"\"\"Formats a number into a currency string (e.g., $1,234.56).\"\"\"\n    return f\"${amount:,.2f}\"\n\n\ndef main():\n    \"\"\"Main function to demonstrate the use of the business functions.\"\"\"\n    # --- Using the Functions ---\n    print(\"--- Calculating Company-Wide Profits ---\")\n    # Let's use our functions to analyze two different products.\n    product_a_revenue = 500000\n    product_a_expenses = 400000\n    product_a_profit = get_net_profit(product_a_revenue, product_a_expenses)\n    print(f\"Product A Profit: {format_currency(product_a_profit)}\")\n\n    product_b_revenue = 250000\n    product_b_expenses = 210000\n    product_b_profit = get_net_profit(product_b_revenue, product_b_expenses)\n    print(f\"Product B Profit: {format_currency(product_b_profit)}\")\n\n    total_profit = product_a_profit + product_b_profit\n    print(f\"Total Company Profit: {format_currency(total_profit)}\")\n    print(\"-\" * 20)\n\n    print(\"--- Sales and Bonus Calculations ---\")\n    sales_figure = 12000\n    commission_earned = calculate_commission(sales_figure)\n    print(\n        f\"A sale of {format_currency(sales_figure)} earns a commission of {format_currency(commission_earned)}.\"\n    )\n    print()\n\n    # Test the bonus eligibility function with different scenarios\n    employee1_rating = 5\n    employee1_service = 3\n    eligibility1 = is_eligible_for_bonus(employee1_rating, employee1_service)\n    print(\n        f\"Employee 1 (Rating: {employee1_rating}, Service: {employee1_service} yrs) is eligible for bonus: {eligibility1}\"\n    )\n\n    employee2_rating = 4\n    employee2_service = 1\n    eligibility2 = is_eligible_for_bonus(employee2_rating, employee2_service)\n    print(\n        f\"Employee 2 (Rating: {employee2_rating}, Service: {employee2_service} yr) is eligible for bonus: {eligibility2}\"\n    )\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 11: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Commission Calculator Function ---\nprint(\"--- Solution to Exercise 1 ---\")\n\n\ndef calculate_commission(sales_amount: float) -&gt; float:\n    \"\"\"\n    Calculates a commission of 15% of the sales amount.\n    Takes sales_amount as a float, returns the commission as a float.\n    \"\"\"\n    commission_rate = 0.15\n    return sales_amount * commission_rate\n\n\n# Example usage:\nsample_sale = 5000.00\ncommission_earned = calculate_commission(sample_sale)\nprint(f\"A sale of ${sample_sale:,.2f} earns a commission of ${commission_earned:,.2f}.\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Employee Bonus Eligibility Function ---\nprint(\"--- Solution to Exercise 2 ---\")\n\n\ndef is_eligible_for_bonus(performance_rating: int, years_of_service: int) -&gt; bool:\n    \"\"\"\n    Returns True if rating is 4 or 5 AND service is more than 2 years.\n    \"\"\"\n    return performance_rating &gt;= 4 and years_of_service &gt; 2\n\n\n# Test cases:\nprint(\n    f\"Rating 5, 3 years service -&gt; Eligible? {is_eligible_for_bonus(5, 3)}\"\n)  # Expected: True\nprint(\n    f\"Rating 4, 3 years service -&gt; Eligible? {is_eligible_for_bonus(4, 3)}\"\n)  # Expected: True\nprint(\n    f\"Rating 5, 1 year service  -&gt; Eligible? {is_eligible_for_bonus(5, 1)}\"\n)  # Expected: False\nprint(\n    f\"Rating 3, 5 years service -&gt; Eligible? {is_eligible_for_bonus(3, 5)}\"\n)  # Expected: False\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Format Currency Function ---\nprint(\"--- Solution to Exercise 3 ---\")\n\n\ndef format_currency(number: float) -&gt; str:\n    \"\"\"\n    Returns a string formatted as currency with a dollar sign and commas.\n    Example: 1250.5 -&gt; \"$1,250.50\"\n    \"\"\"\n    return f\"${number:,.2f}\"\n\n\n# Example usage:\namount1 = 1250.5\namount2 = 500\nprint(f\"{amount1} -&gt; {format_currency(amount1)}\")\nprint(f\"{amount2} -&gt; {format_currency(amount2)}\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-12-list-comprehension/","title":"\ud83d\udcd8 Day 12: List Comprehension - Elegant Data Manipulation","text":"<p>In data analysis, you constantly create new lists by transforming or filtering existing ones. While <code>for</code> loops work, Python provides a more concise, powerful, and often faster way to do this: a list comprehension.</p>"},{"location":"lessons/day-12-list-comprehension/#the-syntax-of-list-comprehension","title":"The Syntax of List Comprehension","text":"<p>A list comprehension creates a new list in a single, readable line.</p> <p><code>new_list = [expression for item in iterable if condition]</code></p> <ol> <li><code>[expression ... ]</code>: The brackets that create the new list.</li> <li><code>expression</code>: What to do with each item (e.g., <code>item * 1.1</code>, <code>item[\"name\"]</code>).</li> <li><code>for item in iterable</code>: The loop part that iterates through the original list.</li> <li><code>if condition</code> (Optional): A filter that decides whether to include the item.</li> </ol> <pre><code># The for loop way\nlarge_sales = []\nfor sale in sales:\n    if sale &gt; 1000:\n        large_sales.append(sale)\n\n# The list comprehension way\nlarge_sales = [sale for sale in sales if sale &gt; 1000]\n</code></pre>"},{"location":"lessons/day-12-list-comprehension/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-12-list-comprehension/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>list_comprehension.py</code>, has been refactored to place each list comprehension task into its own testable function.</p> <ol> <li>Review the Code: Open <code>Day_12_List_Comprehension/list_comprehension.py</code>. Notice the functions <code>apply_price_increase()</code>, <code>filter_large_sales()</code>, and <code>get_top_sales_performers()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_12_List_Comprehension/list_comprehension.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_12.py\n</code></pre></li> </ol>"},{"location":"lessons/day-12-list-comprehension/#exercises-day-12","title":"\ud83d\udcbb Exercises: Day 12","text":"<ol> <li> <p>Calculate Sales Commissions:</p> </li> <li> <p>In a new script (<code>my_solutions_12.py</code>), create a list of sales figures: <code>sales = [2500, 8000, 12000, 5500]</code>.</p> </li> <li>Write a function <code>calculate_commissions(sales_list, commission_rate)</code> that takes a list of sales and a rate.</li> <li>Inside the function, use a list comprehension to return a new list of calculated commissions.</li> <li> <p>Call your function with a 15% commission rate (<code>0.15</code>) and print the result.</p> </li> <li> <p>Filter Products by Category:</p> </li> <li> <p>You have a list of product dictionaries (see below).</p> </li> <li>Create a function <code>filter_by_category(product_list, category)</code> that takes a list of products and a category name.</li> <li>The function should use a list comprehension to return a new list containing only the names of products in the specified category.</li> <li> <p>Call your function with the category <code>\"electronics\"</code> and print the result.      <pre><code>products = [\n    {\"name\": \"Laptop\", \"category\": \"electronics\"},\n    {\"name\": \"T-Shirt\", \"category\": \"apparel\"},\n    {\"name\": \"Keyboard\", \"category\": \"electronics\"}\n]\n</code></pre></p> </li> <li> <p>Format Prices for Display:</p> </li> <li> <p>You have a list of prices as floats: <code>prices = [49.99, 199.99, 19.95]</code>.</p> </li> <li>Use a list comprehension to create a new list called <code>display_prices</code>.</li> <li>Each item in the new list should be a string, formatted as currency (e.g., <code>\"$49.99\"</code>).</li> <li>Print the <code>display_prices</code> list.</li> </ol> <p>\ud83c\udf89 Excellent! List comprehensions are a powerful tool for writing clean, efficient, and professional Python code. They are heavily used in data analysis for quick and readable data transformations.</p>"},{"location":"lessons/day-12-list-comprehension/#additional-materials","title":"Additional Materials","text":"<ul> <li>list_comprehension.ipynb</li> <li>solutions.ipynb</li> </ul> list_comprehension.py <p>View on GitHub</p> list_comprehension.py<pre><code>\"\"\"\nDay 12: Elegant Data Manipulation with List Comprehensions (Refactored)\n\nThis script demonstrates how to use list comprehensions to\nefficiently transform and filter lists of business data. This version\nis refactored into functions for better organization and testability.\n\"\"\"\n\n\ndef apply_price_increase(prices, increase_percentage):\n    \"\"\"\n    Applies a percentage price increase to a list of prices\n    using a list comprehension.\n    \"\"\"\n    increase_multiplier = 1 + increase_percentage\n    # [expression for item in iterable]\n    return [price * increase_multiplier for price in prices]\n\n\ndef filter_large_sales(sales, threshold):\n    \"\"\"\n    Filters a list of sales to find those above a given threshold\n    using a list comprehension.\n    \"\"\"\n    # [expression for item in iterable if condition]\n    return [sale for sale in sales if sale &gt; threshold]\n\n\ndef get_top_sales_performers(employees, sales_target):\n    \"\"\"\n    Filters and transforms a list of employee dictionaries to get the\n    names of top-performing sales staff.\n    \"\"\"\n    # [expression for item in iterable if condition]\n    return [\n        employee[\"name\"]\n        for employee in employees\n        if employee.get(\"department\") == \"Sales\"\n        and employee.get(\"quarterly_sales\", 0) &gt; sales_target\n    ]\n\n\ndef main():\n    \"\"\"Main function to demonstrate list comprehensions.\"\"\"\n    # --- Example 1: Transforming Data ---\n    print(\"--- Applying a Price Increase ---\")\n    original_prices = [100.00, 150.50, 200.00, 80.25]\n    print(f\"Original prices: {original_prices}\")\n\n    increased_prices = apply_price_increase(original_prices, 0.10)  # 10% increase\n    print(f\"New prices (from comprehension): {[f'${p:.2f}' for p in increased_prices]}\")\n    print(\"-\" * 20)\n\n    # --- Example 2: Filtering Data ---\n    print(\"--- Filtering for Large Sales Transactions ---\")\n    sales_data = [500, 1200, 800, 1500, 300, 2500]\n    print(f\"All sales: {sales_data}\")\n\n    large_sales_data = filter_large_sales(sales_data, 1000)\n    print(f\"Large sales (from comprehension): {large_sales_data}\")\n    print(\"-\" * 20)\n\n    # --- Example 3: Filtering and Transforming ---\n    print(\"--- Extracting Names of High-Performing Sales Staff ---\")\n    employee_data = [\n        {\"name\": \"Alice\", \"department\": \"Sales\", \"quarterly_sales\": 12000},\n        {\"name\": \"Bob\", \"department\": \"Engineering\", \"quarterly_sales\": 0},\n        {\"name\": \"Charlie\", \"department\": \"Sales\", \"quarterly_sales\": 8000},\n        {\"name\": \"David\", \"department\": \"Sales\", \"quarterly_sales\": 15000},\n    ]\n    target = 10000\n    top_performers_list = get_top_sales_performers(employee_data, target)\n    print(f\"Top performing sales staff (sales &gt; ${target}): {top_performers_list}\")\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 12: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Calculate Sales Commissions ---\nprint(\"--- Solution to Exercise 1 ---\")\nsales = [2500, 8000, 12000, 5500]\ncommission_rate = 0.10\n\n# [expression for item in iterable]\n# The expression is the calculation for each sale.\ncommissions = [sale * commission_rate for sale in sales]\n\nprint(f\"Original sales: {sales}\")\nprint(f\"Calculated commissions (10%): {commissions}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Filter Products by Category ---\nprint(\"--- Solution to Exercise 2 ---\")\nproducts = [\n    {\"name\": \"Laptop\", \"category\": \"electronics\"},\n    {\"name\": \"T-Shirt\", \"category\": \"apparel\"},\n    {\"name\": \"Keyboard\", \"category\": \"electronics\"},\n    {\"name\": \"Coffee Mug\", \"category\": \"homeware\"},\n    {\"name\": \"Webcam\", \"category\": \"electronics\"},\n]\n\n# [expression for item in iterable if condition]\n# The expression is the product's name.\n# The condition checks if the product's category is 'electronics'.\nelectronic_products = [\n    product[\"name\"] for product in products if product[\"category\"] == \"electronics\"\n]\n\nprint(f\"All products: {products}\")\nprint(f\"Electronic products only: {electronic_products}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Format Prices for Display ---\nprint(\"--- Solution to Exercise 3 ---\")\nprices = [49.99, 199.99, 19.95, 24.50, 12.00]\n\n# The expression is an f-string that formats each price.\ndisplay_prices = [f\"${price:,.2f}\" for price in prices]\n\nprint(f\"Original prices (float): {prices}\")\nprint(f\"Display prices (string): {display_prices}\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-13-higher-order-functions/","title":"\ud83d\udcd8 Day 13: Higher-Order Functions & Lambda","text":"<p>A higher-order function is a function that takes another function as an argument or returns a function as its result. This is a powerful concept used heavily in data analysis for its conciseness and flexibility.</p>"},{"location":"lessons/day-13-higher-order-functions/#key-higher-order-functions","title":"Key Higher-Order Functions","text":"<ul> <li><code>map(function, iterable)</code>: Applies a function to every item in an iterable (e.g., a list).</li> <li><code>filter(function, iterable)</code>: Creates a new iterable containing only the items for which the function returns <code>True</code>.</li> <li><code>sorted(iterable, key=function)</code>: Sorts an iterable. The <code>key</code> argument takes a function that specifies what to sort by.</li> </ul>"},{"location":"lessons/day-13-higher-order-functions/#lambda-functions-quick-anonymous-functions","title":"Lambda Functions: Quick, Anonymous Functions","text":"<p>A lambda function is a small, anonymous function defined with the <code>lambda</code> keyword. They are perfect for use with higher-order functions when you need a simple, one-off function.</p> <p><code>lambda arguments: expression</code></p> <pre><code># The 'for' loop way\nnew_salaries = []\nfor s in salaries:\n    new_salaries.append(s * 1.10)\n\n# The map and lambda way\nnew_salaries = list(map(lambda s: s * 1.10, salaries))\n</code></pre>"},{"location":"lessons/day-13-higher-order-functions/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-13-higher-order-functions/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>HOF.py</code>, has been refactored to place each higher-order function task into its own testable function.</p> <ol> <li>Review the Code: Open <code>Day_13_Higher_Order_Functions/HOF.py</code>. Notice the functions <code>apply_bonus_to_salaries()</code>, <code>filter_high_yield_projects()</code>, and <code>sort_products_by_attribute()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_13_Higher_Order_Functions/HOF.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_13.py\n</code></pre></li> </ol>"},{"location":"lessons/day-13-higher-order-functions/#exercises-day-13","title":"\ud83d\udcbb Exercises: Day 13","text":"<ol> <li> <p>Standardize Department Names:</p> </li> <li> <p>In a new script (<code>my_solutions_13.py</code>), create a list of department names: <code>departments = [\"Sales\", \" marketing \", \"  Engineering\", \"HR  \"]</code>.</p> </li> <li>Write a function that takes this list and uses <code>map</code> with a <code>lambda</code> function to return a new list where each name is cleaned (whitespace stripped) and converted to lowercase.</li> <li> <p>Call your function and print the result.</p> </li> <li> <p>Filter Active Subscriptions:</p> </li> <li> <p>You have a list of customer dictionaries (see below).</p> </li> <li>Import the <code>get_active_customer_names</code> function from the lesson script.</li> <li> <p>Call the function with the customer list and print the names of the active customers.      <pre><code>customers = [\n    {\"name\": \"Cust A\", \"subscription_status\": \"active\"},\n    {\"name\": \"Cust B\", \"subscription_status\": \"inactive\"},\n    {\"name\": \"Cust C\", \"subscription_status\": \"active\"}\n]\n</code></pre></p> </li> <li> <p>Sort Complex Data:</p> </li> <li> <p>You have a list of product dictionaries.</p> </li> <li>Import the <code>sort_products_by_attribute</code> function.</li> <li>Call the function twice: once to sort the products by <code>\"price\"</code> and once to sort them by <code>\"name\"</code>. Print both sorted lists.      <pre><code>products = [\n    {\"name\": \"Laptop\", \"price\": 1200},\n    {\"name\": \"Mouse\", \"price\": 25},\n    {\"name\": \"Keyboard\", \"price\": 75}\n]\n</code></pre></li> </ol> <p>\ud83c\udf89 Congratulations! Higher-order functions and lambdas are a gateway to a more powerful style of programming that you will see everywhere in the world of data science.</p>"},{"location":"lessons/day-13-higher-order-functions/#additional-materials","title":"Additional Materials","text":"<ul> <li>HOF.ipynb</li> <li>solutions.ipynb</li> </ul> HOF.py <p>View on GitHub</p> HOF.py<pre><code>\"\"\"\nDay 13: Advanced Data Processing with Higher-Order Functions (Refactored)\n\nThis script demonstrates using map, filter, and sorted with lambda functions\nfor concise and powerful data manipulation. This version is refactored\ninto functions for better organization and testability.\n\"\"\"\n\n\ndef apply_bonus_to_salaries(salaries, bonus_percentage):\n    \"\"\"\n    Applies a percentage bonus to a list of salaries using map().\n    \"\"\"\n    bonus_multiplier = 1 + bonus_percentage\n    return list(map(lambda s: s * bonus_multiplier, salaries))\n\n\ndef filter_high_yield_projects(projects, roi_threshold):\n    \"\"\"\n    Filters a list of projects to find those with an ROI above a threshold.\n    Assumes projects is a list of tuples: (project_name, roi_percentage).\n    \"\"\"\n    return list(filter(lambda p: p[1] &gt; roi_threshold, projects))\n\n\ndef get_active_customer_names(customers):\n    \"\"\"\n    Filters a list of customer dictionaries for active customers and returns their names.\n    \"\"\"\n    active_customers = filter(\n        lambda c: c.get(\"subscription_status\") == \"active\", customers\n    )\n    return list(map(lambda c: c.get(\"name\"), active_customers))\n\n\ndef sort_products_by_attribute(products, attribute_name):\n    \"\"\"\n    Sorts a list of product dictionaries by a specified attribute (e.g., 'price').\n    \"\"\"\n    return sorted(products, key=lambda p: p.get(attribute_name, 0))\n\n\ndef main():\n    \"\"\"Main function to demonstrate higher-order functions.\"\"\"\n    # --- Using map() to transform a list ---\n    print(\"--- Applying a Bonus to All Salaries ---\")\n    salaries_list = [50000, 80000, 120000, 65000]\n    print(f\"Original salaries: {salaries_list}\")\n\n    new_salaries_list = apply_bonus_to_salaries(salaries_list, 0.10)  # 10% bonus\n    print(f\"Salaries after 10% bonus: {new_salaries_list}\")\n    print(\"-\" * 20)\n\n    # --- Using filter() to select data ---\n    print(\"--- Filtering for High-Yield Projects ---\")\n    projects_list = [\n        (\"Project A\", 12),\n        (\"Project B\", 20),\n        (\"Project C\", 8),\n        (\"Project D\", 25),\n    ]\n    print(f\"All projects: {projects_list}\")\n\n    high_yield_list = filter_high_yield_projects(projects_list, 15)\n    print(f\"High-yield projects (ROI &gt; 15%): {high_yield_list}\")\n    print(\"-\" * 20)\n\n    # --- Combining map() and filter() ---\n    print(\"--- Analyzing High-Value Customer Data ---\")\n    customers_list = [\n        {\"name\": \"InnovateCorp\", \"subscription_status\": \"active\", \"monthly_spend\": 550},\n        {\n            \"name\": \"DataDriven Inc.\",\n            \"subscription_status\": \"inactive\",\n            \"monthly_spend\": 120,\n        },\n        {\n            \"name\": \"Analytics LLC\",\n            \"subscription_status\": \"active\",\n            \"monthly_spend\": 210,\n        },\n    ]\n    print(f\"Original customer data: {customers_list}\")\n\n    active_names = get_active_customer_names(customers_list)\n    print(f\"Names of active customers: {active_names}\")\n    print(\"-\" * 20)\n\n    # --- Using sorted() with a lambda key ---\n    print(\"--- Sorting Products by Price ---\")\n    products_list = [\n        {\"name\": \"Laptop\", \"price\": 1200},\n        {\"name\": \"Mouse\", \"price\": 25},\n        {\"name\": \"Keyboard\", \"price\": 75},\n        {\"name\": \"Monitor\", \"price\": 300},\n    ]\n    print(f\"Original product list: {products_list}\")\n\n    sorted_products = sort_products_by_attribute(products_list, \"price\")\n    print(f\"Products sorted by price: {sorted_products}\")\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 13: Solutions to Exercises\n\"\"\"\n\n# --- Exercise 1: Standardize Department Names ---\nprint(\"--- Solution to Exercise 1 ---\")\ndepartments = [\"Sales\", \" marketing \", \"  Engineering\", \"HR  \"]\nprint(f\"Original list: {departments}\")\n\n# The lambda function x.strip().lower() is applied to each item in the list.\n# .strip() removes whitespace, .lower() converts to lowercase.\ncleaned_departments = list(map(lambda x: x.strip().lower(), departments))\n\nprint(f\"Cleaned list: {cleaned_departments}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Filter Active Subscriptions ---\nprint(\"--- Solution to Exercise 2 ---\")\ncustomers = [\n    {\"id\": 1, \"status\": \"active\"},\n    {\"id\": 2, \"status\": \"inactive\"},\n    {\"id\": 3, \"status\": \"active\"},\n    {\"id\": 4, \"status\": \"cancelled\"},\n]\nprint(f\"Original customers: {customers}\")\n\n# The lambda function returns True only if the customer's status is 'active'.\n# filter() keeps only the items for which the lambda returns True.\nactive_customers = list(filter(lambda c: c[\"status\"] == \"active\", customers))\n\nprint(f\"Active customers only: {active_customers}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Sort Complex Data ---\nprint(\"--- Solution to Exercise 3 ---\")\nproducts = [\n    {\"name\": \"Laptop\", \"price\": 1200},\n    {\"name\": \"Mouse\", \"price\": 25},\n    {\"name\": \"Keyboard\", \"price\": 75},\n    {\"name\": \"Monitor\", \"price\": 300},\n]\nprint(f\"Original product list: {products}\")\n\n# The 'key' argument of sorted() tells it what to base the sort on.\n# The lambda function tells sorted() to look at the value associated\n# with the 'price' key in each dictionary.\nsorted_products = sorted(products, key=lambda p: p[\"price\"])\n\nprint(f\"Products sorted by price: {sorted_products}\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-14-modules/","title":"\ud83d\udcd8 Day 14: Modules - Organizing Your Business Logic","text":"<p>As your projects grow, you need a way to organize your code. In Python, we do this with modules. A module is simply a Python file (<code>.py</code>) containing functions and variables. By grouping related functions into modules, you create a clean, maintainable, and scalable project.</p>"},{"location":"lessons/day-14-modules/#why-use-modules","title":"Why Use Modules?","text":"<ul> <li>Organization: Keeps related code together (e.g., all financial calculations in <code>financials.py</code>).</li> <li>Reusability: Import your custom modules into any project.</li> <li>Readability: Makes your main script shorter and easier to understand.</li> </ul>"},{"location":"lessons/day-14-modules/#how-to-import-modules","title":"How to Import Modules","text":"<p>1. Import the entire module (recommended): This is the most common way. You access functions using the <code>module_name.function_name</code> syntax.</p> <pre><code>import business_logic as bl\nroi = bl.calculate_roi(50000, 10000)\n</code></pre> <p>2. Import a specific function: If you only need one or two functions from a large module.</p> <pre><code>from business_logic import calculate_roi\nroi = calculate_roi(50000, 10000)\n</code></pre>"},{"location":"lessons/day-14-modules/#pythons-built-in-modules","title":"Python's Built-in Modules","text":"<p>Python comes with a huge Standard Library of built-in modules, including:</p> <ul> <li><code>math</code>: For advanced mathematical functions (<code>math.sqrt()</code>).</li> <li><code>datetime</code>: Essential for working with dates and times.</li> <li><code>random</code>: For generating random numbers (useful for sampling).</li> </ul>"},{"location":"lessons/day-14-modules/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-14-modules/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The code for this lesson is split into two files:</p> <ul> <li><code>business_logic.py</code>: A module containing reusable functions.</li> <li><code>modules.py</code>: The main script that imports and uses the <code>business_logic</code> module.</li> </ul> <p>We have refactored <code>modules.py</code> to wrap its logic in functions and deleted a redundant, unused module file.</p> <ol> <li>Review the Code: Open both <code>Day_14_Modules/business_logic.py</code> and <code>Day_14_Modules/modules.py</code>. See how the main script imports and uses the functions defined in the module.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the main script:    <pre><code>python Day_14_Modules/modules.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of the functions in the <code>business_logic</code> module:    <pre><code>pytest tests/test_day_14.py\n</code></pre></li> </ol>"},{"location":"lessons/day-14-modules/#exercises-day-14","title":"\ud83d\udcbb Exercises: Day 14","text":"<ol> <li> <p>Create Your Own Module:</p> </li> <li> <p>Create a new file named <code>text_tools.py</code>.</p> </li> <li>Inside this file, define a function <code>count_characters(text)</code> that returns the length of a string.</li> <li>Create a second file, <code>main_report.py</code>.</li> <li> <p>In <code>main_report.py</code>, import your <code>text_tools</code> module and use your function to count the characters in a sample sentence.</p> </li> <li> <p>Use the <code>datetime</code> Module:</p> </li> <li> <p>In a new script, <code>import</code> the <code>datetime</code> module.</p> </li> <li>Get the current date and time using <code>datetime.datetime.now()</code>.</li> <li>Print the current date.</li> <li> <p>Print just the current year from the date object (e.g., <code>my_date.year</code>).</p> </li> <li> <p>Use the <code>math</code> Module:</p> </li> <li> <p>A company's sales are growing by the square root of its marketing budget.</p> </li> <li><code>import</code> the <code>math</code> module.</li> <li>Create a variable <code>marketing_budget = 100000</code>.</li> <li>Use <code>math.sqrt()</code> to calculate the growth factor and print the result.</li> </ol> <p>\ud83c\udf89 Well done! You've learned how to organize your code into modules. This is a critical skill for building any project that's more than a few dozen lines long.</p>"},{"location":"lessons/day-14-modules/#additional-materials","title":"Additional Materials","text":"<ul> <li>business_logic.ipynb</li> <li>modules.ipynb</li> <li>solutions.ipynb</li> </ul> business_logic.py <p>View on GitHub</p> business_logic.py<pre><code>\"\"\"\nDay 14: Business Logic Module\n\nThis file acts as a module containing various reusable functions\nfor common business calculations and validations.\n\"\"\"\n\n\ndef calculate_roi(investment: float, profit: float) -&gt; float:\n    \"\"\"Calculates the Return on Investment (ROI).\"\"\"\n    if investment == 0:\n        return 0\n    return (profit / investment) * 100\n\n\ndef calculate_future_value(principal: float, rate: float, years: int) -&gt; float:\n    \"\"\"Calculates the future value of an investment with annual compounding.\"\"\"\n    return principal * ((1 + rate) ** years)\n\n\ndef is_eligible_for_promotion(years_of_service: int, performance_rating: int) -&gt; bool:\n    \"\"\"Checks if an employee is eligible for promotion.\"\"\"\n    # Rule: Must have &gt; 3 years of service and a performance rating of 4 or 5.\n    return years_of_service &gt; 3 and performance_rating &gt;= 4\n\n\ndef format_as_currency(amount: float) -&gt; str:\n    \"\"\"Formats a number into a standard currency string.\"\"\"\n    return f\"${amount:,.2f}\"\n</code></pre> modules.py <p>View on GitHub</p> modules.py<pre><code>\"\"\"\nDay 14: Using Modules to Organize Code (Refactored)\n\nThis script demonstrates how to import and use functions from\nboth custom-built modules and Python's built-in modules.\n\"\"\"\n\n# We are importing the functions we created in the 'business_logic.py' file.\n# We use an alias 'bl' to keep our code clean and concise.\nimport datetime\nimport math\n\nimport business_logic as bl\n\n\ndef demonstrate_custom_module():\n    \"\"\"Demonstrates using functions from the custom business_logic module.\"\"\"\n    print(\"--- Using Custom Business Logic Module ---\")\n    investment_amount = 50000\n    profit_amount = 12000\n    roi = bl.calculate_roi(investment_amount, profit_amount)\n    print(\n        f\"A project with an investment of {bl.format_as_currency(investment_amount)} and profit of {bl.format_as_currency(profit_amount)} has an ROI of {roi:.2f}%.\"\n    )\n\n    years = 4\n    rating = 5\n    is_eligible = bl.is_eligible_for_promotion(years, rating)\n    print(\n        f\"An employee with {years} years of service and a rating of {rating} is eligible for promotion: {is_eligible}\"\n    )\n    print(\"-\" * 20)\n\n\ndef demonstrate_builtin_modules():\n    \"\"\"Demonstrates using Python's built-in datetime and math modules.\"\"\"\n    print(\"--- Using Built-in 'datetime' and 'math' Modules ---\")\n    current_datetime = datetime.datetime.now()\n    print(f\"Current Date and Time: {current_datetime}\")\n    print(f\"Current Year: {current_datetime.year}\")\n    print()\n\n    marketing_budget = 100000\n    growth_factor = math.sqrt(marketing_budget)\n    print(\n        f\"The growth factor for a marketing budget of {bl.format_as_currency(marketing_budget)} is {growth_factor:.2f}.\"\n    )\n    print(\"-\" * 20)\n\n\ndef main():\n    \"\"\"Main function to demonstrate module usage.\"\"\"\n    demonstrate_custom_module()\n    demonstrate_builtin_modules()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Day 14: Solutions to Exercises.\"\"\"\n\nimport datetime\nimport math\n\nimport finance_tools as ft\n\n# --- Exercise 1: Create a Finance Module ---\nprint(\"--- Solution to Exercise 1 ---\")\n\n# --- Call functions from the module ---\ninvestment = 150000\nprofit = 25000\nroi = ft.calculate_roi(investment, profit)\nprint(\n    f\"ROI for a ${investment:,.2f} investment with ${profit:,.2f} profit is: {roi:.2f}%\"\n)\n\nprincipal = 10000\nrate = 0.05\nyears = 10\nfuture_value = ft.calculate_future_value(principal, rate, years)\nprint(\n    f\"Future value of ${principal:,.2f} after {years} years at a {rate * 100}% rate is: ${future_value:,.2f}\"\n)\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Use the `datetime` Module ---\nprint(\"--- Solution to Exercise 2 ---\")\n# import the module\n\n# Get the current date and time\nnow = datetime.datetime.now()\n\nprint(f\"Current full date and time: {now}\")\nprint(f\"Current date only: {now.date()}\")\nprint(f\"Current year: {now.year}\")\nprint(f\"Current month: {now.month}\")\nprint(f\"Current day: {now.day}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Use the `math` Module ---\nprint(\"--- Solution to Exercise 3 ---\")\n# import the module\n\nmarketing_budget = 100000\n# Use the sqrt function from the math module\ngrowth_factor = math.sqrt(marketing_budget)\n\nprint(\n    f\"The growth factor for a marketing budget of ${marketing_budget:,.2f} is {growth_factor:.2f}.\"\n)\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-15-exception-handling/","title":"\ud83d\udcd8 Day 15: Exception Handling - Building Robust Business Logic","text":"<p>In the real world, data is messy and operations can fail. A file might be missing, a user might enter text instead of a number, or you might try to divide by zero when calculating a financial ratio. Without a safety net, these errors\u2014called exceptions\u2014will crash your script.</p> <p>Exception handling is the process of catching these errors and handling them gracefully so your program can continue running or fail in a predictable way.</p>"},{"location":"lessons/day-15-exception-handling/#key-concepts-try-and-except","title":"Key Concepts: <code>try</code> and <code>except</code>","text":"<p>The core of exception handling is the <code>try...except</code> block.</p> <ul> <li><code>try</code> block: You place the code that might cause an error inside the <code>try</code> block.</li> <li><code>except</code> block: If an error occurs in the <code>try</code> block, the code inside the <code>except</code> block is executed, and the program does not crash.</li> </ul> <pre><code># A common business scenario: calculating profit margin\ntry:\n    # This might cause a ZeroDivisionError if revenue is 0\n    margin = (profit / revenue) * 100\n    print(f\"Profit margin is {margin:.2f}%\")\nexcept ZeroDivisionError:\n    print(\"Cannot calculate margin: revenue is zero.\")\n</code></pre> <p>You can also catch specific error types like <code>ValueError</code> (e.g., trying to convert \"abc\" to a number) or <code>FileNotFoundError</code>.</p>"},{"location":"lessons/day-15-exception-handling/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-15-exception-handling/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>exception.py</code>, has been refactored to place the logic into testable functions that include exception handling.</p> <ol> <li>Review the Code: Open <code>Day_15_Exception_Handling/exception.py</code>.</li> <li>The <code>unpack_country_list()</code> function now includes a check to prevent errors with small lists.</li> <li>The new <code>calculate_profit_margin()</code> function demonstrates how to handle a <code>ZeroDivisionError</code> and <code>TypeError</code> in a practical business calculation.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions handle both successful cases and errors gracefully:    <pre><code>python Day_15_Exception_Handling/exception.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify that the functions behave correctly, both with valid input and when exceptions are expected:    <pre><code>pytest tests/test_day_15.py\n</code></pre></li> </ol>"},{"location":"lessons/day-15-exception-handling/#exercises-day-15","title":"\ud83d\udcbb Exercises: Day 15","text":"<ol> <li> <p>Safe Division Function:</p> </li> <li> <p>In a new script (<code>my_solutions_15.py</code>), create a function <code>safe_divide(numerator, denominator)</code>.</p> </li> <li>Inside the function, use a <code>try...except</code> block to handle a potential <code>ZeroDivisionError</code>.</li> <li>If division is successful, return the result.</li> <li>If a <code>ZeroDivisionError</code> occurs, print an error message and return <code>0</code>.</li> <li> <p>Test your function by calling it with valid numbers (e.g., <code>10, 2</code>) and with a zero denominator (e.g., <code>10, 0</code>).</p> </li> <li> <p>User Input with Validation:</p> </li> <li> <p>Create a function <code>get_user_age()</code> that prompts the user to enter their age.</p> </li> <li>Use a <code>try...except</code> block to handle the <code>ValueError</code> that occurs if the user enters text instead of a number.</li> <li>If the input is invalid, print an error message and return <code>None</code>.</li> <li> <p>If the input is valid, convert it to an integer and return it.</p> </li> <li> <p>File Reading with Error Handling:</p> </li> <li> <p>Create a function <code>read_file_contents(filepath)</code>.</p> </li> <li>Use a <code>try...except</code> block to handle a <code>FileNotFoundError</code>.</li> <li>If the file is found, the function should read its contents and return them.</li> <li>If the file is not found, it should print an error message and return <code>None</code>.</li> <li>Test your function with a real file path and a fake one.</li> </ol> <p>\ud83c\udf89 Congratulations! You've learned how to make your Python scripts more robust and reliable. Exception handling is a critical skill for any data analyst or developer working with real-world data.</p>"},{"location":"lessons/day-15-exception-handling/#additional-materials","title":"Additional Materials","text":"<ul> <li>exception.ipynb</li> <li>solutions.ipynb</li> </ul> exception.py <p>View on GitHub</p> exception.py<pre><code>\"\"\"\nDay 15: Handling Exceptions in Business Logic (Refactored)\n\nThis script demonstrates exception handling and iterable unpacking\nwith more practical, testable functions.\n\"\"\"\n\n\ndef unpack_country_list(countries):\n    \"\"\"\n    Unpacks a list of countries into nordic countries, Estonia, and Russia.\n    Uses a try-except block to handle cases where the list is too short.\n    \"\"\"\n    if not isinstance(countries, list) or len(countries) &lt; 3:\n        return None, None, None\n\n    try:\n        # Extended iterable unpacking\n        *nordic, estonia, russia = countries\n        return nordic, estonia, russia\n    except ValueError:\n        # This would catch errors if the list had fewer than 2 items,\n        # but the initial check makes it mostly for demonstration.\n        return None, None, None\n\n\ndef calculate_profit_margin(revenue, profit):\n    \"\"\"\n    Calculates the profit margin and handles the case of zero revenue\n    to avoid a ZeroDivisionError.\n    \"\"\"\n    try:\n        margin = (profit / revenue) * 100\n        return margin\n    except ZeroDivisionError:\n        print(\"Error: Revenue is zero, cannot calculate profit margin.\")\n        return 0.0  # Return a sensible default\n    except TypeError:\n        print(\"Error: Invalid input, revenue and profit must be numbers.\")\n        return None\n\n\ndef main():\n    \"\"\"Main function to demonstrate exception handling and unpacking.\"\"\"\n    # --- Example 1: Extended Iterable Unpacking ---\n    print(\"--- Unpacking a List of Countries ---\")\n    country_names = [\n        \"Finland\",\n        \"Sweden\",\n        \"Norway\",\n        \"Denmark\",\n        \"Iceland\",\n        \"Estonia\",\n        \"Russia\",\n    ]\n\n    nordic_list, estonia_country, russia_country = unpack_country_list(country_names)\n\n    if nordic_list is not None:\n        print(\"Nordic Countries:\", nordic_list)\n        print(\"Estonia:\", estonia_country)\n        print(\"Russia:\", russia_country)\n    print(\"-\" * 20)\n\n    # --- Example 2: Handling a ZeroDivisionError ---\n    print(\"--- Calculating Profit Margin (with Error Handling) ---\")\n\n    # Successful case\n    revenue1 = 500000\n    profit1 = 75000\n    margin1 = calculate_profit_margin(revenue1, profit1)\n    print(f\"Revenue: ${revenue1}, Profit: ${profit1} -&gt; Margin: {margin1:.2f}%\")\n\n    # Error case\n    revenue2 = 0\n    profit2 = -10000  # A loss\n    margin2 = calculate_profit_margin(revenue2, profit2)\n    print(f\"Revenue: ${revenue2}, Profit: ${profit2} -&gt; Margin: {margin2:.2f}%\")\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code># Day 15: Exception Handling - Solutions\n\n## Exercise 1: Handling a `ValueError`\n\n\ntry:\n    age = int(input(\"Enter your age: \"))\n    print(f\"You are {age} years old.\")\nexcept ValueError:\n    print(\"Invalid input. Please enter a numeric value for your age.\")\n\n\n## Exercise 2: Handling a `ZeroDivisionError`\n\n\ntry:\n    num1 = float(input(\"Enter the first number: \"))\n    num2 = float(input(\"Enter the second number: \"))\n    result = num1 / num2\n    print(f\"The result of the division is: {result}\")\nexcept ZeroDivisionError:\n    print(\"Error: Cannot divide by zero.\")\nexcept ValueError:\n    print(\"Invalid input. Please enter numeric values.\")\n\n\n## Exercise 3: Refactor `exception.py`\n\n\ncountry_names = [\"Finland\"]\n\ntry:\n    *nordic_countries, estonia, russia = country_names\n\n    print(\"Nordic Countries:\", nordic_countries)\n    print(\"Estonia:\", estonia)\n    print(\"Russia:\", russia)\n\nexcept ValueError as e:\n    if \"not enough values to unpack\" in str(e):\n        print(\n            f\"Error: The list of countries must have at least two elements. Details: {e}\"\n        )\n    else:\n        print(f\"A ValueError occurred: {e}\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"lessons/day-16-file-handling/","title":"\ud83d\udcd8 Day 16: File Handling for Business Analytics","text":"<p>A huge part of data analysis involves reading data from files and writing results to them. Whether you're processing a sales report, a customer list, or log files, you need to interact with the file system. Python makes this easy.</p>"},{"location":"lessons/day-16-file-handling/#key-concepts","title":"Key Concepts","text":"<ul> <li>Opening Files: Use the <code>open()</code> function to open a file. It's best practice to use it with a <code>with</code> statement, which automatically closes the file for you, even if errors occur.   <pre><code>with open('my_report.txt', 'r') as file:\n    content = file.read()\n</code></pre></li> <li>File Modes:</li> <li><code>'r'</code>: Read (default). Throws an error if the file doesn't exist.</li> <li><code>'w'</code>: Write. Creates a new file or overwrites an existing one.</li> <li><code>'a'</code>: Append. Adds content to the end of an existing file.</li> <li>Exception Handling: When working with files, it's crucial to wrap your code in a <code>try...except FileNotFoundError</code> block to handle cases where a file might be missing.</li> </ul>"},{"location":"lessons/day-16-file-handling/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-16-file-handling/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>fh.py</code>, has been refactored to provide several powerful, reusable functions for common business file-handling tasks.</p> <ol> <li>Review the Code: Open <code>Day_16_File_Handling/fh.py</code>. Examine functions like <code>count_words_and_lines()</code>, <code>find_most_common_words()</code>, <code>extract_emails_from_file()</code>, and <code>analyze_sales_csv()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script. It will create a few temporary demo files, run the analysis functions on them, print the results, and then clean up the files.    <pre><code>python Day_16_File_Handling/fh.py\n</code></pre></li> <li>Run the Tests: The tests for this lesson are more advanced. They create temporary files in memory to test the functions without needing actual files on your disk.    <pre><code>pytest tests/test_day_16.py\n</code></pre></li> </ol>"},{"location":"lessons/day-16-file-handling/#exercises-day-16","title":"\ud83d\udcbb Exercises: Day 16","text":"<ol> <li> <p>Analyze a Text File:</p> </li> <li> <p>In a new script (<code>my_solutions_16.py</code>), create a simple text file named <code>my_memo.txt</code> and write a few sentences into it.</p> </li> <li>Import the <code>count_words_and_lines</code> and <code>find_most_common_words</code> functions from the lesson script.</li> <li> <p>Call these functions with your new file's path and print the results.</p> </li> <li> <p>Process a Simple CSV:</p> </li> <li> <p>Create a function <code>create_sales_data(filepath, sales_data)</code> that takes a list of lists and writes it to a CSV file.</p> </li> <li>Your <code>sales_data</code> could be <code>[['Product', 'Price', 'Quantity'], ['Widget A', '10.00', '50'], ['Widget B', '15.50', '30']]</code>.</li> <li>Import and use the <code>analyze_sales_csv</code> function from the lesson to read your new CSV and print the total revenue and average transaction value.</li> </ol> <p>\ud83c\udf89 Excellent! You can now programmatically read from and write to the most common file types. This is a fundamental skill for automating data intake, processing reports, and saving your analysis.</p>"},{"location":"lessons/day-16-file-handling/#additional-materials","title":"Additional Materials","text":"<ul> <li>fh.ipynb</li> <li>solutions.ipynb</li> <li>stop_words.ipynb</li> </ul> fh.py <p>View on GitHub</p> fh.py<pre><code>\"\"\"Day 16: File Handling for Business Analytics (Refactored)\n\nThis module demonstrates various file handling operations commonly used in business.\n\"\"\"\n\nimport csv\nimport os\nimport re\nimport string\nfrom collections import Counter\nfrom typing import Dict, List, Optional, Tuple\n\n# Import stop words from the local file\nfrom .stop_words import stop_words as sw\n\n\ndef count_words_and_lines(fname: str) -&gt; Tuple[int, int]:\n    \"\"\"Count words and lines in a text file.\"\"\"\n    num_words, num_lines = 0, 0\n    try:\n        with open(fname, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                wordslist = line.split()\n                num_lines += 1\n                num_words += len(wordslist)\n        return num_words, num_lines\n    except FileNotFoundError:\n        print(f\"\u274c Error: File '{fname}' not found\")\n        return 0, 0\n    except IOError as e:\n        print(f\"\u274c Error reading file '{fname}': {e}\")\n        return 0, 0\n\n\ndef find_most_common_words(fname: str, top_n: int) -&gt; List[Tuple[str, int]]:\n    \"\"\"Find the most frequently used words in a text file, ignoring stop words.\"\"\"\n    try:\n        with open(fname, \"r\", encoding=\"utf-8\") as f:\n            text = f.read().lower()\n\n        # Remove punctuation\n        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n        words = text.split()\n\n        # Filter out stop words\n        filtered_words = [word for word in words if word not in sw]\n\n        # Count and return the most common\n        counts = Counter(filtered_words)\n        return counts.most_common(top_n)\n\n    except FileNotFoundError:\n        print(f\"\u274c Error: File '{fname}' not found\")\n        return []\n    except Exception as e:\n        print(f\"\u274c An unexpected error occurred: {e}\")\n        return []\n\n\ndef extract_emails_from_file(fname: str) -&gt; List[str]:\n    \"\"\"Extract all unique email addresses from a text file.\"\"\"\n    email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n    try:\n        with open(fname, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n\n        emails = re.findall(email_pattern, text)\n        return sorted(list(set(emails)))  # Return unique emails, sorted\n\n    except FileNotFoundError:\n        print(f\"\u274c Error: File '{fname}' not found\")\n        return []\n    except Exception as e:\n        print(f\"\u274c An unexpected error occurred: {e}\")\n        return []\n\n\ndef analyze_sales_csv(fname: str) -&gt; Optional[Dict[str, float]]:\n    \"\"\"\n    Reads a sales CSV and calculates total revenue and average transaction value.\n    Assumes CSV format: Product,Price,Quantity\n    \"\"\"\n    total_revenue = 0.0\n    transaction_count = 0\n\n    try:\n        with open(fname, mode=\"r\", encoding=\"utf-8\") as file:\n            csv_reader = csv.reader(file)\n            _ = next(csv_reader)  # Skip header row\n\n            for row in csv_reader:\n                try:\n                    price = float(row[1])\n                    quantity = int(row[2])\n                    total_revenue += price * quantity\n                    transaction_count += 1\n                except (ValueError, IndexError):\n                    # Skip rows with malformed data\n                    continue\n\n        if transaction_count == 0:\n            return None\n\n        average_transaction = total_revenue / transaction_count\n        return {\n            \"total_revenue\": total_revenue,\n            \"average_transaction\": average_transaction,\n        }\n\n    except FileNotFoundError:\n        print(f\"\u274c Error: File '{fname}' not found\")\n        return None\n    except Exception as e:\n        print(f\"\u274c An unexpected error occurred: {e}\")\n        return None\n\n\ndef main():\n    \"\"\"Main function to demonstrate file handling capabilities.\"\"\"\n    print(\"\ud83d\uddc2\ufe0f  Day 16: File Handling for Business Analytics\")\n    print(\"=\" * 60)\n\n    # Create dummy files for demonstration\n    # In a real scenario, these files would already exist.\n    demo_text_content = \"This is a sample business report. The report details sales and profits. Contact support@example.com for details.\"\n    demo_csv_content = \"Product,Price,Quantity\\nLaptop,1200.00,5\\nMouse,25.50,10\"\n\n    demo_text_file = \"demo_report.txt\"\n    demo_csv_file = \"demo_sales.csv\"\n\n    with open(demo_text_file, \"w\") as f:\n        f.write(demo_text_content)\n    with open(demo_csv_file, \"w\") as f:\n        f.write(demo_csv_content)\n\n    # 1. Analyze document word counts\n    print(\"\\n\ud83d\udcc4 Document Analysis Example:\")\n    words, lines = count_words_and_lines(demo_text_file)\n    print(f\"\u2705 Document '{demo_text_file}' has {words} words and {lines} lines.\")\n\n    # 2. Find most common words\n    print(\"\\n\ud83d\udcca Most Common Words Example:\")\n    common_words = find_most_common_words(demo_text_file, 3)\n    print(f\"\u2705 Top 3 most common words: {common_words}\")\n\n    # 3. Extract emails\n    print(\"\\n\ud83d\udce7 Email Extraction Example:\")\n    emails = extract_emails_from_file(demo_text_file)\n    print(f\"\u2705 Found emails: {emails}\")\n\n    # 4. Analyze CSV data\n    print(\"\\n\ud83d\udcc8 CSV Sales Analysis Example:\")\n    sales_analysis = analyze_sales_csv(demo_csv_file)\n    if sales_analysis:\n        print(f\"\u2705 Total Revenue: ${sales_analysis['total_revenue']:.2f}\")\n        print(f\"\u2705 Average Transaction: ${sales_analysis['average_transaction']:.2f}\")\n\n    # Clean up dummy files\n    os.remove(demo_text_file)\n    os.remove(demo_csv_file)\n\n    print(\"\\n\u2728 Demonstration complete!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 16: File Handling - Solutions\n\nThis file contains comprehensive solutions to all Day 16 exercises,\ndemonstrating advanced file handling techniques for business analytics.\n\nAuthor: 50 Days of Python Course\nPurpose: Educational solutions for MBA students\n\"\"\"\n\nimport csv\nimport glob\nimport os\nfrom typing import Dict, List\n\nfrom fh import (\n    check_email,\n    counter,\n    extract_emails,\n    find_most_common_words,\n)\n\n\ndef exercise_1_document_analyzer():\n    \"\"\"\n    Exercise 1: Document Statistics Analyzer\n\n    Creates and analyzes a sample story file, then demonstrates\n    batch processing of multiple business documents.\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"\ud83d\udcca EXERCISE 1: Document Statistics Analyzer\")\n    print(\"=\" * 60)\n\n    # Create sample story file\n    story_content = \"\"\"\n    Once upon a time in a bustling corporate office, there lived a data analyst named Sarah.\n    Sarah worked tirelessly to transform raw business data into meaningful insights.\n    Every morning, she would arrive early to review the previous day's sales reports.\n\n    The company had been struggling with declining market share.\n    Sarah believed that data-driven decisions could turn the tide.\n    She spent hours analyzing customer feedback, sales trends, and market research.\n\n    One day, Sarah discovered a pattern in the data that nobody had noticed before.\n    Customer satisfaction was directly correlated with response time to support tickets.\n    This insight led to a complete overhaul of the customer service process.\n\n    Within six months, the company's customer retention improved by 35%.\n    Sarah's analytical skills had literally saved the company millions of dollars.\n    Her story became legend in the data analytics community.\n    \"\"\"\n\n    # Write story to file\n    story_file = \"my_story.txt\"\n    try:\n        with open(story_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(story_content.strip())\n        print(f\"\u2705 Created story file: {story_file}\")\n    except Exception as e:\n        print(f\"\u274c Error creating story file: {e}\")\n        return\n\n    # Analyze the story\n    try:\n        words, lines = counter(story_file)\n        print(\"\\n\ud83d\udcc8 Story Analysis Results:\")\n        print(f\"   \ud83d\udcdd Total words: {words}\")\n        print(f\"   \ud83d\udcc4 Total lines: {lines}\")\n        print(f\"   \ud83d\udcca Average words per line: {words / lines:.2f}\")\n\n        # Find most common words in the story\n        common_words = find_most_common_words(story_file, 5)\n        print(\"\\n\ud83d\udd24 Most Common Words:\")\n        for i, (freq, word) in enumerate(common_words, 1):\n            print(f\"   {i}. '{word}': {freq} times\")\n\n    except Exception as e:\n        print(f\"\u274c Error analyzing story: {e}\")\n\n    # Demonstrate batch document analysis\n    print(\"\\n\ud83d\udcda Batch Document Analysis:\")\n    batch_analyze_business_documents()\n\n    # Clean up\n    if os.path.exists(story_file):\n        os.remove(story_file)\n        print(f\"\ud83e\uddf9 Cleaned up: {story_file}\")\n\n\ndef batch_analyze_business_documents():\n    \"\"\"\n    Advanced function for analyzing multiple business documents\n    \"\"\"\n    # Look for sample files in the data directory\n    data_dir = os.path.join(\"..\", \"data\")\n\n    # Common business document patterns\n    text_patterns = [\"*.txt\"]\n    sample_files = []\n\n    for pattern in text_patterns:\n        files = glob.glob(os.path.join(data_dir, pattern))\n        sample_files.extend(files)\n\n    if not sample_files:\n        print(\"   \u2139\ufe0f  No sample text files found for batch analysis\")\n        return\n\n    print(f\"   \ud83d\udcc1 Found {len(sample_files)} document(s) for analysis\")\n\n    total_words = 0\n    total_lines = 0\n    all_themes = {}\n\n    for file_path in sample_files[:3]:  # Limit to first 3 files\n        try:\n            print(f\"   \ud83d\udd0d Analyzing: {os.path.basename(file_path)}\")\n            words, lines = counter(file_path)\n            total_words += words\n            total_lines += lines\n\n            # Extract themes\n            themes = find_most_common_words(file_path, 3)\n            for freq, word in themes:\n                if len(word) &gt; 3:  # Filter short words\n                    all_themes[word] = all_themes.get(word, 0) + freq\n\n        except Exception as e:\n            print(f\"   \u274c Error processing {file_path}: {e}\")\n\n    if total_words &gt; 0:\n        print(\"\\n   \ud83d\udcca Batch Analysis Summary:\")\n        print(f\"      \ud83d\udcdd Total words across documents: {total_words:,}\")\n        print(f\"      \ud83d\udcc4 Total lines across documents: {total_lines:,}\")\n\n        # Top themes across all documents\n        if all_themes:\n            top_themes = sorted(all_themes.items(), key=lambda x: x[1], reverse=True)[\n                :5\n            ]\n            print(\"      \ud83c\udfaf Common themes across documents:\")\n            for i, (word, freq) in enumerate(top_themes, 1):\n                print(f\"         {i}. '{word}': {freq} mentions\")\n\n\ndef exercise_2_contact_management():\n    \"\"\"\n    Exercise 2: Customer Contact Management System\n\n    Demonstrates email extraction and contact database creation.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83d\udce7 EXERCISE 2: Customer Contact Management System\")\n    print(\"=\" * 60)\n\n    # Look for email files in data directory\n    data_dir = os.path.join(\"..\", \"data\")\n    email_files = [\n        os.path.join(data_dir, \"email_exchanges.txt\"),\n        os.path.join(data_dir, \"email_exchanges_big.txt\"),\n    ]\n\n    all_emails = []\n    contact_database = {}\n    processed_files = []\n\n    for file_path in email_files:\n        if os.path.exists(file_path):\n            try:\n                print(f\"\ud83d\udd0d Processing: {os.path.basename(file_path)}\")\n                emails = extract_emails(file_path)\n\n                valid_emails = []\n                for email in emails:\n                    if check_email(email):\n                        valid_emails.append(email)\n\n                        # Organize by domain\n                        domain = email.split(\"@\")[1].lower()\n                        if domain not in contact_database:\n                            contact_database[domain] = []\n                        contact_database[domain].append(email)\n                        all_emails.append(email)\n\n                processed_files.append(file_path)\n                print(f\"   \u2705 Found {len(valid_emails)} valid email addresses\")\n\n            except Exception as e:\n                print(f\"   \u274c Error processing {file_path}: {e}\")\n        else:\n            print(f\"   \u2139\ufe0f  File not found: {os.path.basename(file_path)}\")\n\n    if all_emails:\n        # Remove duplicates\n        unique_emails = list(set(all_emails))\n\n        print(\"\\n\ud83d\udcca Contact Management Summary:\")\n        print(f\"   \ud83d\udce7 Total email addresses found: {len(all_emails)}\")\n        print(f\"   \ud83d\udd04 Unique email addresses: {len(unique_emails)}\")\n        print(f\"   \ud83c\udfe2 Companies/domains: {len(contact_database)}\")\n\n        print(\"\\n\ud83c\udfe2 Contact Database by Domain:\")\n        for domain, emails in sorted(contact_database.items()):\n            unique_domain_emails = list(set(emails))\n            print(f\"   \ud83d\udccd {domain}: {len(unique_domain_emails)} contacts\")\n            for email in unique_domain_emails[:3]:  # Show first 3\n                print(f\"      \u2022 {email}\")\n            if len(unique_domain_emails) &gt; 3:\n                print(f\"      ... and {len(unique_domain_emails) - 3} more\")\n\n        # Export contacts to CSV\n        export_contacts_to_csv(contact_database)\n    else:\n        print(\"   \u2139\ufe0f  No email addresses found in available files\")\n\n\ndef export_contacts_to_csv(contact_database: Dict[str, List[str]]):\n    \"\"\"\n    Export contact database to CSV format for business use.\n    \"\"\"\n    csv_file = \"customer_contacts.csv\"\n    try:\n        with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n            writer = csv.writer(f)\n            writer.writerow([\"Email\", \"Domain\", \"Company\"])\n\n            for domain, emails in contact_database.items():\n                unique_emails = list(set(emails))\n                for email in unique_emails:\n                    # Extract company name from domain (remove .com, .org, etc.)\n                    company = domain.split(\".\")[0].title()\n                    writer.writerow([email, domain, company])\n\n        print(f\"   \ud83d\udcbe Exported contacts to: {csv_file}\")\n\n        # Clean up demonstration file\n        if os.path.exists(csv_file):\n            os.remove(csv_file)\n            print(\"   \ud83e\uddf9 Cleaned up demonstration file\")\n\n    except Exception as e:\n        print(f\"   \u274c Error exporting contacts: {e}\")\n\n\ndef main():\n    \"\"\"\n    Main function to run all Day 16 solutions and demonstrations.\n    \"\"\"\n    print(\"\ud83d\udc0d Day 16: File Handling for Business Analytics - Solutions\")\n    print(\"\ud83c\udf93 50 Days of Python for MBA Program\")\n    print(\"\ud83d\udcda Comprehensive demonstrations of file processing techniques\")\n\n    try:\n        # Run key exercises\n        exercise_1_document_analyzer()\n        exercise_2_contact_management()\n\n        print(\"\\n\" + \"=\" * 60)\n        print(\"\ud83c\udf89 All File Handling Exercises Completed Successfully!\")\n        print(\"\ud83d\udca1 Key Skills Demonstrated:\")\n        print(\"   \ud83d\udcc4 Text file processing and analysis\")\n        print(\"   \ud83d\udcca Data extraction from multiple formats\")\n        print(\"   \ud83d\udce7 Email validation and contact management\")\n        print(\"   \ud83d\udd0d Document similarity and competitive analysis\")\n        print(\"   \ud83d\udcc8 Technology trend tracking\")\n        print(\"   \ud83c\udfd7\ufe0f  Integrated business intelligence systems\")\n        print(\"=\" * 60)\n\n    except Exception as e:\n        print(f\"\u274c Error in main execution: {e}\")\n        print(\"\ud83d\udca1 This may be due to missing sample files in the data directory\")\n\n\nif __name__ == \"__main__\":\n    main()\n# extract_unique_emails('../data/email_exchanges.txt')\n</code></pre> stop_words.py <p>View on GitHub</p> stop_words.py<pre><code>\"\"\"\nEnglish Stop Words Collection for Text Analytics\n\nThis module provides a comprehensive list of common English stop words\nused in business text analysis, sentiment analysis, and natural language\nprocessing tasks. Stop words are frequently used words that are typically\nfiltered out during text preprocessing to focus on meaningful content.\n\nBusiness Use Cases:\n- Customer review sentiment analysis\n- Social media monitoring and brand sentiment\n- Content analysis of business documents\n- Market research text mining\n- Competitor analysis from web content\n\nUsage:\n    from data.stop_words import stop_words\n\n    # Filter out stop words from business text\n    business_text = \"The company has great customer service\"\n    filtered_words = [word for word in business_text.lower().split()\n                      if word not in stop_words]\n    print(filtered_words)  # ['company', 'great', 'customer', 'service']\n\"\"\"\n\nfrom typing import List\n\n# Comprehensive English stop words list for business text analysis\nstop_words: List[str] = [\n    \"i\",\n    \"me\",\n    \"my\",\n    \"myself\",\n    \"we\",\n    \"our\",\n    \"ours\",\n    \"ourselves\",\n    \"you\",\n    \"you're\",\n    \"you've\",\n    \"you'll\",\n    \"you'd\",\n    \"your\",\n    \"yours\",\n    \"yourself\",\n    \"yourselves\",\n    \"he\",\n    \"him\",\n    \"his\",\n    \"himself\",\n    \"she\",\n    \"she's\",\n    \"her\",\n    \"hers\",\n    \"herself\",\n    \"it\",\n    \"it's\",\n    \"its\",\n    \"itself\",\n    \"they\",\n    \"them\",\n    \"their\",\n    \"theirs\",\n    \"themselves\",\n    \"what\",\n    \"which\",\n    \"who\",\n    \"whom\",\n    \"this\",\n    \"that\",\n    \"that'll\",\n    \"these\",\n    \"those\",\n    \"am\",\n    \"is\",\n    \"are\",\n    \"was\",\n    \"were\",\n    \"be\",\n    \"been\",\n    \"being\",\n    \"have\",\n    \"has\",\n    \"had\",\n    \"having\",\n    \"do\",\n    \"does\",\n    \"did\",\n    \"doing\",\n    \"a\",\n    \"an\",\n    \"the\",\n    \"and\",\n    \"but\",\n    \"if\",\n    \"or\",\n    \"because\",\n    \"as\",\n    \"until\",\n    \"while\",\n    \"of\",\n    \"at\",\n    \"by\",\n    \"for\",\n    \"with\",\n    \"about\",\n    \"against\",\n    \"between\",\n    \"into\",\n    \"through\",\n    \"during\",\n    \"before\",\n    \"after\",\n    \"above\",\n    \"below\",\n    \"to\",\n    \"from\",\n    \"up\",\n    \"down\",\n    \"in\",\n    \"out\",\n    \"on\",\n    \"off\",\n    \"over\",\n    \"under\",\n    \"again\",\n    \"further\",\n    \"then\",\n    \"once\",\n    \"here\",\n    \"there\",\n    \"when\",\n    \"where\",\n    \"why\",\n    \"how\",\n    \"all\",\n    \"any\",\n    \"both\",\n    \"each\",\n    \"few\",\n    \"more\",\n    \"most\",\n    \"other\",\n    \"some\",\n    \"such\",\n    \"no\",\n    \"nor\",\n    \"not\",\n    \"only\",\n    \"own\",\n    \"same\",\n    \"so\",\n    \"than\",\n    \"too\",\n    \"very\",\n    \"s\",\n    \"t\",\n    \"can\",\n    \"will\",\n    \"just\",\n    \"don\",\n    \"don't\",\n    \"should\",\n    \"should've\",\n    \"now\",\n    \"d\",\n    \"ll\",\n    \"m\",\n    \"o\",\n    \"re\",\n    \"ve\",\n    \"y\",\n    \"ain\",\n    \"aren\",\n    \"aren't\",\n    \"couldn\",\n    \"couldn't\",\n    \"didn\",\n    \"didn't\",\n    \"doesn\",\n    \"doesn't\",\n    \"hadn\",\n    \"hadn't\",\n    \"hasn\",\n    \"hasn't\",\n    \"haven\",\n    \"haven't\",\n    \"isn\",\n    \"isn't\",\n    \"ma\",\n    \"mightn\",\n    \"mightn't\",\n    \"mustn\",\n    \"mustn't\",\n    \"needn\",\n    \"needn't\",\n    \"shan\",\n    \"shan't\",\n    \"shouldn\",\n    \"shouldn't\",\n    \"wasn\",\n    \"wasn't\",\n    \"weren\",\n    \"weren't\",\n    \"won\",\n    \"won't\",\n    \"wouldn\",\n    \"wouldn't\",\n]\n</code></pre>"},{"location":"lessons/day-17-regular-expressions/","title":"\ud83d\udcd8 Day 17: Regular Expressions for Text Pattern Matching","text":"<p>Often, the text data you need to analyze isn't perfectly structured. You might need to find all the invoice numbers in a document, extract all email addresses from a messy text block, or validate that a product code follows a specific format. For these tasks, we use regular expressions (regex).</p>"},{"location":"lessons/day-17-regular-expressions/#what-is-a-regular-expression","title":"What is a Regular Expression?","text":"<p>A regular expression is a special sequence of characters that defines a search pattern. It's like a mini-language for finding and manipulating text. While it can look intimidating at first, a few key patterns can solve most common business problems.</p> <ul> <li><code>\\d+</code>: Matches one or more digits (e.g., <code>123</code>).</li> <li><code>[a-z]+</code>: Matches one or more lowercase letters.</li> <li><code>\\s</code>: Matches a whitespace character.</li> <li><code>\\b</code>: Matches a word boundary.</li> <li><code>[]</code>: Defines a character set (e.g., <code>[abc]</code> matches 'a', 'b', or 'c').</li> <li><code>[^...]</code>: Matches anything not in the character set.</li> </ul>"},{"location":"lessons/day-17-regular-expressions/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-17-regular-expressions/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>regex.py</code>, has been refactored to place each regex task into its own testable function.</p> <ol> <li>Review the Code: Open <code>Day_17_Regular_Expressions/regex.py</code>. Examine functions like <code>find_most_common_words()</code>, <code>extract_and_analyze_numbers()</code>, <code>is_valid_python_variable()</code>, and <code>clean_text_advanced()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_17_Regular_Expressions/regex.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each regex function:    <pre><code>pytest tests/test_day_17.py\n</code></pre></li> </ol>"},{"location":"lessons/day-17-regular-expressions/#exercises-day-17","title":"\ud83d\udcbb Exercises: Day 17","text":"<ol> <li> <p>Extract All Numbers from a String:</p> </li> <li> <p>In a new script (<code>my_solutions_17.py</code>), you have a string: <code>text = \"Order #123 was placed for $49.99. The order contains 3 items.\"</code></p> </li> <li>Import the <code>extract_and_analyze_numbers</code> function from the lesson script. Note that it only extracts integers. Can you modify the regex inside it to extract floating-point numbers as well?</li> <li> <p>Call the function and print the extracted numbers.</p> </li> <li> <p>Validate a Product Code:</p> </li> <li> <p>A valid product code must follow the format <code>PROD-XXXX</code>, where <code>X</code> is a digit.</p> </li> <li>Create a function <code>is_valid_product_code(code)</code> that returns <code>True</code> if the code is valid and <code>False</code> otherwise.</li> <li>Use the <code>re.fullmatch(pattern, string)</code> function. Your pattern should look something like <code>r'PROD-\\d{4}'</code>. (<code>\\d{4}</code> means exactly four digits).</li> <li> <p>Test your function with <code>\"PROD-1234\"</code> (valid) and <code>\"PROD-123\"</code> (invalid).</p> </li> <li> <p>Clean Up a Messy Sentence:</p> </li> <li> <p>You have a sentence with extra symbols: <code>sentence = \"Contact us... at (support@example.com)!\"</code></p> </li> <li>Import and use the <code>clean_text_advanced</code> function from the lesson to remove the punctuation and symbols.</li> <li>Print the cleaned sentence.</li> </ol> <p>\ud83c\udf89 Excellent! Regular expressions are a fundamental tool for any data analyst who works with text. They provide a powerful and efficient way to clean, validate, and extract information from unstructured data.</p>"},{"location":"lessons/day-17-regular-expressions/#additional-materials","title":"Additional Materials","text":"<ul> <li>regex.ipynb</li> <li>solutions.ipynb</li> </ul> regex.py <p>View on GitHub</p> regex.py<pre><code>\"\"\"\nDay 17: Using Regular Expressions for Text Pattern Matching (Refactored)\n\nThis script demonstrates how to use the 're' module for common\nbusiness text processing tasks like finding patterns, validating text,\nand cleaning data. This version is refactored into testable functions.\n\"\"\"\n\nimport re\nfrom collections import Counter\nfrom typing import List, Tuple\n\n\ndef find_most_common_words(text: str, top_n: int) -&gt; List[Tuple[str, int]]:\n    \"\"\"\n    Finds the most common words in a given text string.\n    The text is converted to lowercase and punctuation is removed before counting.\n    \"\"\"\n    # Use regex to find all words (sequences of alphabetic characters)\n    words = re.findall(r\"\\b[a-z]+\\b\", text.lower())\n    return Counter(words).most_common(top_n)\n\n\ndef extract_and_analyze_numbers(text: str) -&gt; dict:\n    \"\"\"\n    Extracts all integer numbers from a text and returns the sorted list\n    and the distance between the maximum and minimum numbers.\n    \"\"\"\n    # Regex to find all integers, including negative ones\n    numbers = [int(n) for n in re.findall(r\"-?\\d+\", text)]\n    if not numbers:\n        return {\"sorted_numbers\": [], \"distance\": 0}\n\n    numbers.sort()\n    distance = numbers[-1] - numbers[0]\n    return {\"sorted_numbers\": numbers, \"distance\": distance}\n\n\ndef is_valid_python_variable(name: str) -&gt; bool:\n    \"\"\"\n    Checks if a string is a valid Python variable name using regex.\n    Valid: starts with a letter or underscore, followed by letters, numbers, or underscores.\n    \"\"\"\n    # ^[a-zA-Z_] matches the start of the string with a letter or underscore.\n    # \\w* matches zero or more \"word\" characters (letters, numbers, underscore).\n    # $ matches the end of the string.\n    return bool(re.fullmatch(r\"[a-zA-Z_]\\w*\", name))\n\n\ndef clean_text_advanced(text: str) -&gt; str:\n    \"\"\"\n    Cleans a text string by removing all non-alphanumeric characters\n    (except spaces) and converting to lowercase.\n    \"\"\"\n    # [^a-z0-9\\s] is a character set that matches anything that is NOT\n    # a lowercase letter, a digit, or a whitespace character.\n    return re.sub(r\"[^a-z0-9\\s]\", \"\", text.lower())\n\n\ndef main():\n    \"\"\"Main function to demonstrate regex capabilities.\"\"\"\n    print(\"--- Finding Most Common Words ---\")\n    paragraph = (\n        \"I love teaching. If you do not love teaching what else can you love. I love Python if you do not love \"\n        \"something which can give you all the capabilities to develop an application what else can you love.\"\n    )\n    top_words = find_most_common_words(paragraph, 5)\n    print(f\"Original Text: '{paragraph[:50]}...'\")\n    print(f\"Top 5 most common words: {top_words}\")\n    print(\"-\" * 20)\n\n    print(\"--- Extracting and Analyzing Numbers ---\")\n    para_with_nums = (\n        \"The position of some particles on the x-axis are -12, -4, -3, -1, 0, 4, and 8.\"\n    )\n    analysis = extract_and_analyze_numbers(para_with_nums)\n    print(f\"Original Text: '{para_with_nums}'\")\n    print(f\"Extracted and sorted numbers: {analysis['sorted_numbers']}\")\n    print(f\"Distance between max and min: {analysis['distance']}\")\n    print(\"-\" * 20)\n\n    print(\"--- Validating Python Variable Names ---\")\n    print(\n        f\"'first_name' is a valid variable name: {is_valid_python_variable('first_name')}\"\n    )\n    print(\n        f\"'first-name' is a valid variable name: {is_valid_python_variable('first-name')}\"\n    )\n    print(\n        f\"'2nd_place' is a valid variable name: {is_valid_python_variable('2nd_place')}\"\n    )\n    print(\"-\" * 20)\n\n    print(\"--- Advanced Text Cleaning ---\")\n    messy_sentence = \"\"\"%I $am@% a %tea@cher%, &amp;and&amp; I lo%#ve %tea@ching%;.\"\"\"\n    cleaned = clean_text_advanced(messy_sentence)\n    print(f\"Original: '{messy_sentence}'\")\n    print(f\"Cleaned: '{cleaned}'\")\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code># Day 17: Regular Expressions - Solutions\n\nimport re\nimport string\nfrom collections import Counter\n\n## Exercise 1: Find all numbers in a string\n\npara = \"\"\"The position of some particles on the horizontal x-axis are -12, -4, -3 and -1 in the negative direction,\n0 at origin, 4 and 8 in the positive direction. \"\"\"\n\nnum_list = list(map(int, re.findall(r\"[-+]?[.]?[\\d]+\", para)))\nprint(\"Numbers found in the string:\", num_list)\n\n\n## Exercise 2: Validate a variable name\n\n\ndef is_valid_variable(potential_variable):\n    if re.search(r\"^[a-zA-Z_]\\w*$\", potential_variable):\n        return True\n    else:\n        return False\n\n\nprint(\"Is '_name' a valid variable?\", is_valid_variable(\"_name\"))  # Expected: True\nprint(\n    \"Is 'first_name' a valid variable?\", is_valid_variable(\"first_name\")\n)  # Expected: True\nprint(\"Is '1name' a valid variable?\", is_valid_variable(\"1name\"))  # Expected: False\nprint(\"Is 'name-1' a valid variable?\", is_valid_variable(\"name-1\"))  # Expected: False\n\n\n## Exercise 3: Clean up a messy sentence\n\nsentence = \"\"\"%I $am@% a %tea@cher%, &amp;and&amp; I lo%#ve %tea@ching%;. There $is nothing; &amp;as&amp; mo@re rewarding as\neduca@ting &amp;and&amp; @emp%o@wering peo@ple. ;I found tea@ching m%o@re interesting tha@n any other %jo@bs. %Do@es thi%s\nmo@tivate yo@u to be a tea@cher!? \"\"\"\n\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"\\[.*?]\", \"\", text)\n    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n    text = re.sub(r\"&lt;.*?&gt;+\", \"\", text)\n    text = re.sub(r\"[%s]\" % re.escape(string.punctuation), \"\", text)\n    text = re.sub(r\"\\n\", \"\", text)\n    text = re.sub(r\"\\w*\\d\\w*\", \"\", text)\n    return text\n\n\ndef most_common_words(text, n):\n    split_it = text.split()\n    return Counter(split_it).most_common(n)\n\n\ncleaned_sentence = clean_text(sentence)\nprint(\"Cleaned sentence:\", cleaned_sentence)\nprint(\"Top 3 most common words:\", most_common_words(cleaned_sentence, 3))\n</code></pre>"},{"location":"lessons/day-18-classes-and-objects/","title":"\ud83d\udcd8 Day 18: Classes and Objects - Modeling Business Concepts","text":"<p>So far, we've organized our code with functions. But what if you want to model a real-world concept, like a \"Customer,\" that has both data (name, email) and actions (calculate total spending)? For this, we use Object-Oriented Programming (OOP), and its building blocks: classes and objects.</p>"},{"location":"lessons/day-18-classes-and-objects/#what-are-classes-and-objects","title":"What are Classes and Objects?","text":"<ul> <li>A Class is a blueprint for creating an object. It defines the data (attributes) and actions (methods) that the object will have. Think of <code>Car</code> as a class.</li> <li>An Object is an instance of a class. It's a concrete thing created from the blueprint. Your specific <code>toyota_camry</code> would be an object of the <code>Car</code> class.</li> </ul> <p>This approach is powerful because it lets you bundle data and the functions that operate on that data together in a clean, logical way.</p>"},{"location":"lessons/day-18-classes-and-objects/#key-class-concepts","title":"Key Class Concepts","text":"<ul> <li><code>__init__(self, ...)</code>: The \"initializer\" or \"constructor\" method. It runs automatically when you create a new object and is used to set up its initial attributes.</li> <li><code>self</code>: Represents the instance of the class itself. Inside a method, you use <code>self</code> to access the object's own attributes (e.g., <code>self.firstname</code>).</li> <li>Attributes: Variables that belong to an object (e.g., <code>person.firstname</code>).</li> <li>Methods: Functions that belong to an object (e.g., <code>person.account_balance()</code>).</li> </ul>"},{"location":"lessons/day-18-classes-and-objects/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-18-classes-and-objects/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>CaO.py</code>, has been refactored for clarity and robustness.</p> <ol> <li>Review the Code: Open <code>Day_18_Classes_and_Objects/CaO.py</code>.</li> <li>The <code>Statistics</code> class now gracefully handles empty data and potential errors when calculating the mode.</li> <li>The <code>PersonAccount</code> class has been streamlined to add incomes and expenses via methods, which is a more standard object-oriented approach.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the classes instantiated and their methods called:    <pre><code>python Day_18_Classes_and_Objects/CaO.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each class and its methods under various conditions:    <pre><code>pytest tests/test_day_18.py\n</code></pre></li> </ol>"},{"location":"lessons/day-18-classes-and-objects/#exercises-day-18","title":"\ud83d\udcbb Exercises: Day 18","text":"<ol> <li> <p>Create a <code>Product</code> Class:</p> </li> <li> <p>In a new script (<code>my_solutions_18.py</code>), create a class named <code>Product</code>.</p> </li> <li>The <code>__init__</code> method should take <code>name</code>, <code>price</code>, and <code>initial_quantity</code> as arguments and store them as attributes.</li> <li>Add a method <code>get_inventory_value()</code> that returns the total value of the product's inventory (<code>price * quantity</code>).</li> <li>Add a method <code>restock(amount)</code> that increases the product's quantity.</li> <li> <p>Create an instance of your <code>Product</code> class, check its initial inventory value, restock it, and then check the new value.</p> </li> <li> <p>Extend the <code>PersonAccount</code> Class:</p> </li> <li> <p>Import the <code>PersonAccount</code> class from the lesson script.</p> </li> <li>Create an instance of the class for a new person.</li> <li>Add several income and expense items using the <code>.add_income()</code> and <code>.add_expense()</code> methods.</li> <li>Print the final account summary using the <code>.account_info()</code> method.</li> </ol> <p>\ud83c\udf89 Congratulations! You've learned the basics of object-oriented programming. This will enable you to write more organized, powerful, and scalable analytical scripts that model real-world business concepts.</p>"},{"location":"lessons/day-18-classes-and-objects/#additional-materials","title":"Additional Materials","text":"<ul> <li>CaO.ipynb</li> <li>solutions.ipynb</li> </ul> CaO.py <p>View on GitHub</p> CaO.py<pre><code>\"\"\"\nDay 18: Classes and Objects for Structured Business Data (Refactored)\n\nThis module demonstrates how to use classes and objects to model\nreal-world business concepts like a statistics calculator and a personal account.\n\"\"\"\n\nimport statistics as stat\nfrom collections import Counter\nfrom typing import Any, Dict, List, Sequence, Tuple, Union\n\n\nclass Statistics:\n    \"\"\"\n    A class to perform basic statistical calculations on a list of numbers.\n    \"\"\"\n\n    def __init__(self, data: Sequence[Union[int, float]]):\n        \"\"\"Initializes the Statistics class with a sequence of numbers.\"\"\"\n        if not data:\n            # Handle empty data case gracefully\n            self.data = []\n        else:\n            self.data = data\n\n    def count(self) -&gt; int:\n        \"\"\"Calculates the number of elements.\"\"\"\n        return len(self.data)\n\n    def sum(self) -&gt; Union[int, float]:\n        \"\"\"Calculates the sum of the numbers.\"\"\"\n        return sum(self.data) if self.data else 0\n\n    def min(self) -&gt; Union[int, float, None]:\n        \"\"\"Finds the minimum value.\"\"\"\n        return min(self.data) if self.data else None\n\n    def max(self) -&gt; Union[int, float, None]:\n        \"\"\"Finds the maximum value.\"\"\"\n        return max(self.data) if self.data else None\n\n    def range(self) -&gt; Union[int, float]:\n        \"\"\"Calculates the range of the data.\"\"\"\n        return self.max() - self.min() if self.data else 0\n\n    def mean(self) -&gt; float:\n        \"\"\"Calculates the mean.\"\"\"\n        return stat.mean(self.data) if self.data else 0.0\n\n    def median(self) -&gt; Union[int, float]:\n        \"\"\"Calculates the median.\"\"\"\n        return stat.median(self.data) if self.data else 0.0\n\n    def mode(self) -&gt; Dict[str, Any]:\n        \"\"\"Calculates the mode, handling cases with no unique mode.\"\"\"\n        if not self.data:\n            return {\"mode\": \"No data\", \"count\": 0}\n\n        counts = Counter(self.data)\n        most_common = counts.most_common(2)  # Get up to two most common\n\n        # If there's only one item or the most common is more frequent than the second most common\n        if len(most_common) == 1 or most_common[0][1] &gt; most_common[1][1]:\n            mode_value = most_common[0][0]\n            return {\"mode\": mode_value, \"count\": most_common[0][1]}\n        else:\n            return {\"mode\": \"No unique mode\", \"count\": 0}\n\n    def std(self) -&gt; float:\n        \"\"\"Calculates the standard deviation.\"\"\"\n        return stat.stdev(self.data) if self.count() &gt; 1 else 0.0\n\n    def var(self) -&gt; float:\n        \"\"\"Calculates the variance.\"\"\"\n        return stat.variance(self.data) if self.count() &gt; 1 else 0.0\n\n    def freq_dist(self) -&gt; List[Tuple[float, Union[int, float]]]:\n        \"\"\"Calculates the frequency distribution as percentages.\"\"\"\n        if not self.data:\n            return []\n        total = self.count()\n        counts = Counter(self.data)\n        return sorted(\n            [(count / total * 100, item) for item, count in counts.items()],\n            reverse=True,\n        )\n\n    def describe(self) -&gt; str:\n        \"\"\"Provides a descriptive summary of the data.\"\"\"\n        if not self.data:\n            return \"No data to describe.\"\n\n        desc = (\n            f\"Count: {self.count()}\\n\"\n            f\"Sum: {self.sum()}\\n\"\n            f\"Min: {self.min()}\\n\"\n            f\"Max: {self.max()}\\n\"\n            f\"Range: {self.range()}\\n\"\n            f\"Mean: {self.mean():.2f}\\n\"\n            f\"Median: {self.median()}\\n\"\n            f\"Mode: {self.mode()['mode']} (Count: {self.mode()['count']})\\n\"\n            f\"Standard Deviation: {self.std():.2f}\\n\"\n            f\"Variance: {self.var():.2f}\"\n        )\n        return desc\n\n\nclass PersonAccount:\n    \"\"\"A class to manage a person's income and expenses.\"\"\"\n\n    def __init__(self, firstname: str, lastname: str):\n        self.firstname = firstname\n        self.lastname = lastname\n        self.incomes: Dict[str, Union[int, float]] = {}\n        self.expenses: Dict[str, Union[int, float]] = {}\n\n    def add_income(self, source: str, amount: Union[int, float]):\n        \"\"\"Adds an income source and amount.\"\"\"\n        self.incomes[source] = self.incomes.get(source, 0) + amount\n\n    def add_expense(self, category: str, amount: Union[int, float]):\n        \"\"\"Adds an expense category and amount.\"\"\"\n        self.expenses[category] = self.expenses.get(category, 0) + amount\n\n    def total_income(self) -&gt; Union[int, float]:\n        \"\"\"Calculates the total income.\"\"\"\n        return sum(self.incomes.values())\n\n    def total_expense(self) -&gt; Union[int, float]:\n        \"\"\"Calculates the total expense.\"\"\"\n        return sum(self.expenses.values())\n\n    def account_balance(self) -&gt; Union[int, float]:\n        \"\"\"Calculates the account balance.\"\"\"\n        return self.total_income() - self.total_expense()\n\n    def account_info(self) -&gt; str:\n        \"\"\"Provides information about the person's account.\"\"\"\n        return f\"{self.firstname} {self.lastname}'s account:\\n  Total Income: ${self.total_income():,.2f}\\n  Total Expense: ${self.total_expense():,.2f}\\n  Balance: ${self.account_balance():,.2f}\"\n\n\ndef main():\n    \"\"\"Main function to demonstrate class usage.\"\"\"\n    print(\"--- Statistics Class Demonstration ---\")\n    ages = [\n        31,\n        26,\n        34,\n        37,\n        27,\n        26,\n        32,\n        32,\n        26,\n        27,\n        27,\n        24,\n        32,\n        33,\n        27,\n        25,\n        26,\n        38,\n        37,\n        31,\n        34,\n        24,\n        33,\n        29,\n        26,\n    ]\n    stats_data = Statistics(ages)\n    print(stats_data.describe())\n    print(\"-\" * 20)\n\n    print(\"\\n--- PersonAccount Class Demonstration ---\")\n    person_account = PersonAccount(\"Rishabh\", \"Agrawal\")\n    person_account.add_income(\"Salary\", 150000)\n    person_account.add_income(\"Bonus\", 5500)\n    person_account.add_expense(\"Rent\", 20000)\n    person_account.add_expense(\"General\", 4500)\n\n    print(person_account.account_info())\n\n    person_account.add_income(\"Diwali\", 2500)\n    person_account.add_expense(\"Fuel\", 5000)\n    print(\"\\nAfter adding more income and expenses:\")\n    print(person_account.account_info())\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code># Day 18: Classes and Objects - Solutions\n\nfrom datetime import datetime\nfrom typing import List\n\nfrom CaO import PersonAccount\n\n## Exercise 1 &amp; 2: Create a `Car` class with a `get_age` method\n\n\nclass Car:\n    def __init__(self, make: str, model: str, year: int):\n        self.make = make\n        self.model = model\n        self.year = year\n\n    def display_info(self) -&gt; None:\n        print(f\"Car Information: {self.year} {self.make} {self.model}\")\n\n    def get_age(self) -&gt; int:\n        current_year = datetime.now().year\n        return current_year - self.year\n\n\n# Example usage:\nmy_car = Car(\"Toyota\", \"Camry\", 2020)\nmy_car.display_info()\nprint(f\"The car is {my_car.get_age()} years old.\")\n\n## Exercise 3: Extend the `PersonAccount` class\n\n# Create an enhanced PersonAccount instance with additional methods\n# The PersonAccount class is imported from CaO.py\n\n# Example usage:\nperson = PersonAccount(\n    \"John\", \"Doe\", {\"Salary\": 5000, \"Bonus\": 1000}, {\"Rent\": 1500, \"Food\": 500}\n)\n\n\n# Additional methods that could be added to PersonAccount class:\ndef get_income_sources(account: PersonAccount) -&gt; List[str]:\n    \"\"\"Get all income source names for a PersonAccount.\"\"\"\n    return list(account.incomes.keys())\n\n\ndef get_expense_sources(account: PersonAccount) -&gt; List[str]:\n    \"\"\"Get all expense source names for a PersonAccount.\"\"\"\n    return list(account.expenses.keys())\n\n\nprint(f\"Income sources: {get_income_sources(person)}\")\nprint(f\"Expense sources: {get_expense_sources(person)}\")\nprint(f\"Total income: {person.total_income()}\")\nprint(f\"Total expenses: {person.total_expense()}\")\nprint(f\"Account balance: {person.account_balance()}\")\n</code></pre>"},{"location":"lessons/day-19-python-date-time/","title":"\ud83d\udcd8 Day 19: Working with Dates and Times","text":"<p>Time-series analysis is at the heart of business analytics. Whether you're tracking daily sales, monthly user growth, or quarterly financial results, you need to work with dates and times. Python's built-in <code>datetime</code> module is the standard tool for these tasks.</p>"},{"location":"lessons/day-19-python-date-time/#key-datetime-concepts","title":"Key <code>datetime</code> Concepts","text":"<ul> <li><code>datetime.now()</code>: Gets the current date and time as a <code>datetime</code> object.</li> <li><code>strftime(format_code)</code>: Str**ing **F**rom **Time. Formats a <code>datetime</code> object into a string according to a specific format code (e.g., <code>\"%Y-%m-%d\"</code>).</li> <li><code>strptime(string, format_code)</code>: Str**ing **P**arse **Time. Parses a string into a <code>datetime</code> object based on a specific format code. This is crucial for converting text-based dates into a usable format.</li> <li><code>timedelta</code>: Represents the difference between two dates or times.</li> </ul>"},{"location":"lessons/day-19-python-date-time/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-19-python-date-time/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>date_time.py</code>, has been refactored to place each date/time operation into its own testable function.</p> <ol> <li>Review the Code: Open <code>Day_19_Python_Date_Time/date_time.py</code>. Examine functions like <code>get_current_datetime_components()</code>, <code>format_datetime_to_string()</code>, <code>parse_string_to_datetime()</code>, and <code>calculate_date_difference()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_19_Python_Date_Time/date_time.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_19.py\n</code></pre></li> </ol>"},{"location":"lessons/day-19-python-date-time/#exercises-day-19","title":"\ud83d\udcbb Exercises: Day 19","text":"<ol> <li> <p>Format the Current Date:</p> </li> <li> <p>In a new script (<code>my_solutions_19.py</code>), import the <code>datetime</code> object from the <code>datetime</code> module.</p> </li> <li>Get the current date and time.</li> <li>Use the <code>strftime</code> method to format it into a string like <code>\"Today is 24 September, 2025\"</code>.</li> <li> <p>Print the result.</p> </li> <li> <p>Calculate Days Until a Deadline:</p> </li> <li> <p>Create a function <code>days_until(deadline_str)</code> that takes a date string in <code>\"YYYY-MM-DD\"</code> format.</p> </li> <li>Inside the function, get today's date (<code>date.today()</code>) and parse the deadline string into a <code>date</code> object.</li> <li>Calculate the difference and return the number of days.</li> <li> <p>Call your function with a future date and print the result.</p> </li> <li> <p>Parse a Log File Timestamp:</p> </li> <li> <p>You have a timestamp from a log file as a string: <code>\"Log entry: 2023-03-15 10:30:00\"</code>.</p> </li> <li>Import the <code>parse_string_to_datetime</code> function from the lesson.</li> <li>The timestamp part starts at index 13. Use string slicing to extract just the date/time part (<code>\"2023-03-15 10:30:00\"</code>).</li> <li>Call the function with the extracted string and the correct format code (<code>\"%Y-%m-%d %H:%M:%S\"</code>) and print the resulting <code>datetime</code> object.</li> </ol> <p>\ud83c\udf89 Congratulations! You've learned how to work with dates and times in Python. You're now ready to tackle time-series analysis and other time-based calculations.</p>"},{"location":"lessons/day-19-python-date-time/#additional-materials","title":"Additional Materials","text":"<ul> <li>date_time.ipynb</li> <li>solutions.ipynb</li> </ul> date_time.py <p>View on GitHub</p> date_time.py<pre><code>\"\"\"\nDay 19: Working with Dates and Times in Python (Refactored)\n\nThis script demonstrates common date and time operations using the\ndatetime module, refactored into testable functions.\n\"\"\"\n\nfrom datetime import date, datetime, timedelta\n\n\ndef get_current_datetime_components() -&gt; dict:\n    \"\"\"Gets the current date and time and returns its components as a dictionary.\"\"\"\n    now = datetime.now()\n    return {\n        \"day\": now.day,\n        \"month\": now.month,\n        \"year\": now.year,\n        \"hour\": now.hour,\n        \"minute\": now.minute,\n        \"timestamp\": now.timestamp(),\n    }\n\n\ndef format_datetime_to_string(dt_object: datetime, format_str: str) -&gt; str:\n    \"\"\"Formats a datetime object into a string according to a format code.\"\"\"\n    return dt_object.strftime(format_str)\n\n\ndef parse_string_to_datetime(date_string: str, format_str: str) -&gt; datetime:\n    \"\"\"Parses a string into a datetime object based on a format code.\"\"\"\n    try:\n        return datetime.strptime(date_string, format_str)\n    except ValueError:\n        return None\n\n\ndef calculate_date_difference(date1: date, date2: date) -&gt; timedelta:\n    \"\"\"Calculates the difference between two date objects.\"\"\"\n    return date1 - date2\n\n\ndef main():\n    \"\"\"Main function to demonstrate datetime operations.\"\"\"\n    print(\"--- Getting Current Date and Time Components ---\")\n    dt_components = get_current_datetime_components()\n    print(f\"Current Datetime Components: {dt_components}\")\n    print(\"-\" * 20)\n\n    print(\"--- Formatting a Datetime Object to a String ---\")\n    now_obj = datetime.now()\n    formatted_t = format_datetime_to_string(now_obj, \"%m/%d/%Y, %H:%M:%S\")\n    print(f\"Current datetime object: {now_obj}\")\n    print(f\"Formatted as a string: {formatted_t}\")\n    print(\"-\" * 20)\n\n    print(\"--- Parsing a String into a Datetime Object ---\")\n    date_str_to_parse = \"5 December, 2019\"\n    parsed_date_obj = parse_string_to_datetime(date_str_to_parse, \"%d %B, %Y\")\n    print(f\"Original string: '{date_str_to_parse}'\")\n    print(f\"Parsed into a datetime object: {parsed_date_obj}\")\n    print(\"-\" * 20)\n\n    print(\"--- Calculating the Difference Between Two Dates ---\")\n    new_year_2022 = date(year=2022, month=1, day=1)\n    today_fake = date(year=2021, month=9, day=20)\n    time_difference = calculate_date_difference(new_year_2022, today_fake)\n    print(\n        f\"The difference between {new_year_2022} and {today_fake} is {time_difference.days} days.\"\n    )\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code># Day 19: Python Date and Time - Solutions\n\nfrom datetime import date, datetime\n\n## Exercise 1: Format the current date\n\nnow = datetime.now()\nformatted_date = now.strftime(\"Today is %d %B, %Y\")\nprint(formatted_date)\n\n\n## Exercise 2: Calculate the number of days until your next birthday\n\n\ndef days_until_next_birthday(birthday_month: int, birthday_day: int) -&gt; int:\n    today = date.today()\n    next_birthday_year = today.year\n\n    # Check if the birthday has already passed this year\n    if today.month &gt; birthday_month or (\n        today.month == birthday_month and today.day &gt; birthday_day\n    ):\n        next_birthday_year += 1\n\n    next_birthday = date(next_birthday_year, birthday_month, birthday_day)\n    delta = next_birthday - today\n    return delta.days\n\n\n# Example usage (replace with your own birthday)\ndays_left = days_until_next_birthday(10, 25)\nprint(f\"There are {days_left} days until your next birthday.\")\n\n\n## Exercise 3: Parse a date string\n\ndate_string = \"2023-01-15\"\nparsed_date = datetime.strptime(date_string, \"%Y-%m-%d\")\nprint(f\"The parsed date is: {parsed_date}\")\n</code></pre>"},{"location":"lessons/day-20-python-package-manager/","title":"\ud83d\udcd8 Day 20: Python Package Manager (pip) & Third-Party Libraries","text":"<p>The real power of Python for data analysis comes from its vast ecosystem of third-party libraries. These are pre-written modules, created by the community, that provide powerful tools for specific tasks.</p> <ul> <li><code>requests</code>: For making HTTP requests to APIs and websites.</li> <li><code>numpy</code>: For numerical computing and advanced math.</li> <li><code>pandas</code>: For data manipulation and analysis (we'll dive deep into this soon).</li> <li><code>beautifulsoup4</code>: For web scraping.</li> </ul>"},{"location":"lessons/day-20-python-package-manager/#pip-and-requirementstxt","title":"<code>pip</code> and <code>requirements.txt</code>","text":"<ul> <li><code>pip</code> is the standard tool for installing these packages. You use it in your terminal (e.g., <code>pip install requests</code>).</li> <li>A <code>requirements.txt</code> file is the standard way to list all the third-party packages your project depends on. This allows anyone (including you, on a different computer) to install all the necessary libraries at once using a single command: <code>pip install -r requirements.txt</code>.</li> </ul>"},{"location":"lessons/day-20-python-package-manager/#environment-setup","title":"Environment Setup","text":"<p>This project now has a <code>requirements.txt</code> file at its root. Before you begin, ensure you have followed the setup instructions in the main README.md to create a virtual environment and run <code>pip install -r requirements.txt</code>.</p>"},{"location":"lessons/day-20-python-package-manager/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>url.py</code>, demonstrates how to use <code>requests</code> to fetch data from live APIs and <code>numpy</code> to perform calculations. The code has been refactored to separate the data fetching logic from the data analysis logic, which is a crucial best practice.</p> <ol> <li>Review the Code: Open <code>Day_20_Python_Package_Manager/url.py</code>. Notice the <code>fetch_api_data()</code> function, which is responsible for the network request, and the <code>analyze_...</code> functions, which take data as input and perform calculations.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script. It will make live calls to external APIs and print the analysis.    <pre><code>python Day_20_Python_Package_Manager/url.py\n</code></pre></li> <li>Run the Tests: The tests for this lesson demonstrate a key testing technique: mocking. We replace the live network call with a \"mock\" object that returns sample data. This makes our tests fast, reliable, and independent of the network.    <pre><code>pytest tests/test_day_20.py\n</code></pre></li> </ol>"},{"location":"lessons/day-20-python-package-manager/#exercises-day-20","title":"\ud83d\udcbb Exercises: Day 20","text":"<ol> <li> <p>Explore the <code>requests</code> Response:</p> </li> <li> <p>In a new script (<code>my_solutions_20.py</code>), import the <code>requests</code> library.</p> </li> <li>Make a <code>get</code> request to <code>\"https://api.thecatapi.com/v1/breeds\"</code>.</li> <li> <p>The object returned by <code>requests.get()</code> is a <code>Response</code> object. Print its <code>status_code</code> attribute (e.g., <code>response.status_code</code>) and its <code>headers</code> attribute.</p> </li> <li> <p>Analyze Different Metrics:</p> </li> <li> <p>Import the <code>fetch_api_data</code> and <code>analyze_breed_metrics</code> functions from the lesson script.</p> </li> <li>Fetch the cat breed data.</li> <li> <p>The <code>analyze_breed_metrics</code> function can analyze both <code>'weight'</code> and <code>'life_span'</code>. Call it for <code>'life_span'</code> and print the result.</p> </li> <li> <p>Find the Top 3 Origins:</p> </li> <li> <p>Import and use the <code>analyze_breed_origins</code> function, but modify the call so it returns the top 3 most common origins instead of the default 5.</p> </li> </ol> <p>\ud83c\udf89 Congratulations! You now understand how to leverage the vast Python ecosystem using <code>pip</code>. This skill unlocks a world of powerful tools for data analysis, machine learning, web development, and more.</p>"},{"location":"lessons/day-20-python-package-manager/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> <li>url.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 20: Python Package Manager (pip) - Solutions\n\nThis file contains solutions to the exercises for Day 20, demonstrating\nhow to use pip to install and work with third-party Python packages\nfor business analytics and data processing.\n\nAuthor: 50 Days of Python Course\n\"\"\"\n\nimport json\n\nimport requests\n\n## Exercise 1: Install a package\n\n\"\"\"\nSOLUTION TO EXERCISE 1: Install a package\n\nTo install pandas (a powerful data analysis library), follow these steps:\n\n1. Open your terminal or command prompt\n2. Run the following command:\n   pip install pandas\n\n3. To verify the installation was successful, run:\n   pip show pandas\n\n4. You can also check the version:\n   pip show pandas | findstr Version     (Windows)\n   pip show pandas | grep Version        (macOS/Linux)\n\nAlternative commands:\n- Install a specific version: pip install pandas==1.5.0\n- Upgrade to latest version: pip install --upgrade pandas\n- Install from requirements file: pip install -r requirements.txt\n\"\"\"\n\nprint(\"\ud83d\udce6 Exercise 1: Package Installation\")\nprint(\"\u2705 To install pandas, run: pip install pandas\")\nprint(\"\u2705 To verify installation, run: pip show pandas\")\nprint(\"=\" * 50)\n\n\n## Exercise 2: Explore a package\n\n\"\"\"\nSOLUTION TO EXERCISE 2: Explore the requests package\n\nThe requests library is one of the most popular Python packages for making HTTP requests.\nIt's commonly used in business applications for:\n- API integrations\n- Data collection from web services\n- Microservice communication\n- Web scraping (when appropriate)\n\nKey information about requests.get():\n- Documentation: https://requests.readthedocs.io/en/latest/\n- The get() function sends a GET request to a specified URL\n- Returns a Response object with status codes, headers, and content\n- Supports parameters, headers, authentication, and more\n\nExample usage:\n    import requests\n    response = requests.get('https://api.example.com/data')\n    if response.status_code == 200:\n        data = response.json()  # Parse JSON response\n        print(data)\n\"\"\"\n\nprint(\"\\\\n\ud83d\udd0d Exercise 2: Exploring the requests package\")\nprint(\"\ud83d\udcd6 Documentation: https://requests.readthedocs.io/\")\nprint(\"\ud83c\udfaf Purpose: HTTP requests made simple for Python\")\nprint(\"\ud83d\udcbc Business use: API integrations, data collection, web services\")\nprint(\"=\" * 50)\n\n\n## Exercise 3: Analyze country data\n\nprint(\"\\\\n\ud83c\udf0d Exercise 3: Analyzing World Countries Data\")\nprint(\"\ud83c\udfaf Objective: Find the top 5 largest countries by area\")\nprint(\"\ud83d\udcca Data Source: REST Countries API v3.1\")\n\n\ndef top_5_largest_countries_by_area():\n    \"\"\"\n    Fetch and display the top 5 largest countries by area using the REST Countries API v3.1.\n\n    This demonstrates how to work with modern APIs and handle data structure changes\n    that commonly occur in business applications.\n    \"\"\"\n    try:\n        # Try multiple approaches for better reliability\n        api_attempts = [\n            \"https://restcountries.com/v3.1/all?fields=name,area\",  # Optimized query\n            \"https://restcountries.com/v3.1/all\",  # Full data fallback\n        ]\n\n        countries = None\n        for i, url in enumerate(api_attempts, 1):\n            try:\n                print(f\"\ud83d\udce1 Attempt {i}: Fetching from {url}\")\n                response = requests.get(\n                    url,\n                    timeout=15,\n                    headers={\"User-Agent\": \"Business-Analytics-Course/1.0\"},\n                )\n                response.raise_for_status()\n                countries = response.json()\n                print(f\"\u2705 Success! Retrieved data for {len(countries)} countries\")\n                break\n            except requests.exceptions.RequestException as e:\n                print(f\"\u274c Attempt {i} failed: {e}\")\n                if i &lt; len(api_attempts):\n                    print(\"\ud83d\udd04 Trying alternative endpoint...\")\n\n        # If API completely fails, use fallback data\n        if not countries:\n            print(\"\\\\n\ud83d\udd04 All API endpoints failed. Using educational fallback data...\")\n            countries = [\n                {\"name\": {\"common\": \"Russia\"}, \"area\": 17098242},\n                {\"name\": {\"common\": \"Canada\"}, \"area\": 9984670},\n                {\"name\": {\"common\": \"United States\"}, \"area\": 9833517},\n                {\"name\": {\"common\": \"China\"}, \"area\": 9596961},\n                {\"name\": {\"common\": \"Brazil\"}, \"area\": 8514877},\n                {\"name\": {\"common\": \"Australia\"}, \"area\": 7692024},\n                {\"name\": {\"common\": \"India\"}, \"area\": 3287263},\n            ]\n            print(f\"\u2705 Using fallback dataset with {len(countries)} countries\")\n\n        # Filter out countries that don't have an 'area' key\n        countries_with_area = []\n        for country in countries:\n            if \"area\" in country and country[\"area\"] is not None:\n                # v3.1 API has a different structure for country names\n                name = country.get(\"name\", {}).get(\"common\", \"Unknown\")\n                area = country[\"area\"]\n                countries_with_area.append({\"name\": name, \"area\": area})\n\n        # Sort the countries by area in descending order\n        sorted_countries = sorted(\n            countries_with_area, key=lambda x: x[\"area\"], reverse=True\n        )\n\n        print(\"\\n\ud83c\udf0d Top 5 largest countries by area:\")\n        print(\"=\" * 45)\n        for i in range(min(5, len(sorted_countries))):\n            country = sorted_countries[i]\n            print(f\"{i + 1}. {country['name']:&lt;20}: {country['area']:&gt;15,.0f} sq. km\")\n\n        print(\"\\n\ud83d\udca1 Business Insight: These large countries represent significant\")\n        print(\"   market opportunities for international expansion!\")\n\n    except requests.exceptions.Timeout:\n        print(\"\u274c Error: Request timed out. The API might be slow to respond.\")\n    except requests.exceptions.RequestException as e:\n        print(f\"\u274c Error fetching data: {e}\")\n        print(\"\ud83d\udca1 Tip: Check your internet connection or try again later.\")\n    except json.JSONDecodeError as e:\n        print(f\"\u274c Error: Could not decode the JSON response - {e}\")\n    except KeyError as e:\n        print(f\"\u274c Error: Expected data field missing - {e}\")\n    except Exception as e:\n        print(f\"\u274c Unexpected error occurred: {e}\")\n\n\ndef demonstrate_additional_package_features():\n    \"\"\"\n    Bonus demonstration: Additional features you can explore with pip and packages.\n    \"\"\"\n    print(\"\\\\n\ud83c\udf81 BONUS: Additional Package Manager Features\")\n    print(\"=\" * 50)\n    print(\"\ud83d\udccb Useful pip commands for business projects:\")\n    print(\"   \u2022 pip list                    - Show installed packages\")\n    print(\"   \u2022 pip freeze &gt; requirements.txt - Save current environment\")\n    print(\"   \u2022 pip install -r requirements.txt - Install from requirements\")\n    print(\"   \u2022 pip uninstall package_name  - Remove a package\")\n    print(\"   \u2022 pip search keyword          - Search for packages\")\n    print(\"   \u2022 pip show --files package    - Show package files\")\n\n    print(\"\\\\n\ud83c\udfe2 Popular business analytics packages:\")\n    packages = [\n        (\"pandas\", \"Data manipulation and analysis\"),\n        (\"numpy\", \"Numerical computing\"),\n        (\"matplotlib\", \"Data visualization\"),\n        (\"requests\", \"HTTP library for APIs\"),\n        (\"openpyxl\", \"Excel file handling\"),\n        (\"sqlalchemy\", \"Database toolkit\"),\n        (\"scikit-learn\", \"Machine learning\"),\n        (\"plotly\", \"Interactive visualizations\"),\n    ]\n\n    for package, description in packages:\n        print(f\"   \u2022 {package:12s} - {description}\")\n\n\ndef main():\n    \"\"\"Main function to run all exercise solutions.\"\"\"\n    print(\"\ud83d\ude80 Day 20: Python Package Manager Solutions\")\n    print(\"\ud83c\udf93 Learning how to leverage the Python ecosystem for business\")\n    print(\"=\" * 60)\n\n    # Run Exercise 3 solution\n    top_5_largest_countries_by_area()\n\n    # Show additional features\n    demonstrate_additional_package_features()\n\n    print(\"\\\\n\u2728 Congratulations! You've learned how to:\")\n    print(\"   \u2705 Install Python packages with pip\")\n    print(\"   \u2705 Explore package documentation\")\n    print(\"   \u2705 Use APIs for real-world data analysis\")\n    print(\"   \u2705 Handle errors in data fetching operations\")\n    print(\"\\\\n\ud83c\udf1f Next steps: Explore more packages at https://pypi.org/\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> url.py <p>View on GitHub</p> url.py<pre><code>\"\"\"Day 20: Python Package Manager - Working with Third-Party Libraries (Refactored).\"\"\"\n\nimport json\nimport re\nfrom collections import Counter\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport numpy as np\nimport requests\n\n# Optional import for web scraping\ntry:\n    from bs4 import BeautifulSoup\n\n    BS4_AVAILABLE = True\nexcept ImportError:\n    BS4_AVAILABLE = False\n\n# --- Data Fetching Functions ---\n\n\ndef fetch_api_data(url: str) -&gt; Optional[List[Dict[str, Any]]]:\n    \"\"\"Fetches and parses JSON data from a given API endpoint.\"\"\"\n    try:\n        print(f\"\ud83d\udcda Fetching data from {url}...\")\n        response = requests.get(url, timeout=15)\n        response.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)\n        print(\"\u2705 Data downloaded successfully!\")\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"\u274c Error fetching data: {e}\")\n        return None\n    except json.JSONDecodeError:\n        print(\"\u274c Error: Failed to decode JSON response.\")\n        return None\n\n\n# --- Data Analysis Functions ---\n\n\ndef analyze_text_frequency(text: str, top_n: int = 5) -&gt; List[Tuple[str, int]]:\n    \"\"\"Analyzes text to find the most common words.\"\"\"\n    words = re.findall(r\"\\b[a-z]+\\b\", text.lower())\n    return Counter(words).most_common(top_n)\n\n\ndef parse_metric_range(metric_str: str) -&gt; float:\n    \"\"\"Parses a string like '3 - 7' and returns the average.\"\"\"\n    try:\n        parts = [float(p.strip()) for p in metric_str.split(\"-\")]\n        return sum(parts) / len(parts)\n    except (ValueError, IndexError):\n        return 0.0\n\n\ndef analyze_breed_metrics(\n    breeds_data: List[Dict[str, Any]], metric: str, unit: str\n) -&gt; Optional[Dict[str, float]]:\n    \"\"\"Analyzes a specific metric (e.g., 'weight', 'life_span') from breed data.\"\"\"\n    values = []\n    for breed in breeds_data:\n        if metric == \"weight\" and \"weight\" in breed and \"metric\" in breed[\"weight\"]:\n            avg_value = parse_metric_range(breed[\"weight\"][\"metric\"])\n        elif metric == \"life_span\" and \"life_span\" in breed:\n            avg_value = parse_metric_range(breed[\"life_span\"])\n        else:\n            continue\n\n        if avg_value &gt; 0:\n            values.append(avg_value)\n\n    if not values:\n        return None\n\n    return {\n        \"unit\": unit,\n        \"mean\": np.mean(values),\n        \"median\": np.median(values),\n        \"std_dev\": np.std(values),\n    }\n\n\ndef analyze_breed_origins(\n    breeds_data: List[Dict[str, Any]], top_n: int = 5\n) -&gt; List[Tuple[str, int]]:\n    \"\"\"Analyzes the geographic distribution of cat breed origins.\"\"\"\n    origins = [\n        breed[\"origin\"]\n        for breed in breeds_data\n        if \"origin\" in breed and breed[\"origin\"]\n    ]\n    return Counter(origins).most_common(top_n)\n\n\ndef main():\n    \"\"\"Main function to demonstrate package manager capabilities.\"\"\"\n    print(\"\ud83d\ude80 Day 20: Python Package Manager Demo\\n\")\n\n    # 1. Analyze Cat Breed Data from TheCatAPI\n    print(\"--- Analyzing Cat Breed Data ---\")\n    cat_breeds = fetch_api_data(\"https://api.thecatapi.com/v1/breeds\")\n    if cat_breeds:\n        weight_stats = analyze_breed_metrics(cat_breeds, \"weight\", \"kg\")\n        if weight_stats:\n            print(\n                f\"Average Cat Weight: {weight_stats['mean']:.2f} {weight_stats['unit']}\"\n            )\n\n        lifespan_stats = analyze_breed_metrics(cat_breeds, \"life_span\", \"years\")\n        if lifespan_stats:\n            print(\n                f\"Average Cat Lifespan: {lifespan_stats['mean']:.2f} {lifespan_stats['unit']}\"\n            )\n\n        top_origins = analyze_breed_origins(cat_breeds)\n        print(\"Top 5 Breed Origins:\", top_origins)\n\n    print(\"-\" * 20)\n\n    # 2. Web Scraping Demonstration (Optional)\n    if BS4_AVAILABLE:\n        print(\"\\n--- Web Scraping Demonstration ---\")\n        try:\n            response = requests.get(\"https://httpbin.org/html\", timeout=10)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.content, \"html.parser\")\n            title = soup.find(\"h1\")\n            print(f\"Scraped Page Title: {title.get_text() if title else 'Not Found'}\")\n        except Exception as e:\n            print(f\"\u274c Web scraping demo failed: {e}\")\n    else:\n        print(\"\\n--- Web Scraping Demonstration ---\")\n        print(\"BeautifulSoup4 not installed. Skipping.\")\n\n    print(\"\\n\u2705 Demo complete!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"lessons/day-21-virtual-environments/","title":"\ud83d\udcd8 Day 21: Virtual Environments - Professional Project Management","text":"<p>As you work on more complex projects, you'll find they have different requirements. Project A might need an older version of a library, while Project B needs the latest version. Installing everything globally on your computer leads to conflicts.</p> <p>The professional solution is to use virtual environments.</p>"},{"location":"lessons/day-21-virtual-environments/#what-is-a-virtual-environment","title":"What is a Virtual Environment?","text":"<p>A virtual environment is an isolated, self-contained directory that holds a specific version of Python plus all the specific packages and libraries required for a particular project. Think of it as a clean, separate workspace for each project.</p> <p>This is a critical best practice for any serious Python development because it makes your projects:</p> <ul> <li>Isolated: Prevents package versions from clashing between projects.</li> <li>Reproducible: Allows anyone to recreate the exact same environment your project needs to run correctly.</li> </ul>"},{"location":"lessons/day-21-virtual-environments/#how-it-works-venv-and-requirementstxt","title":"How it Works: <code>venv</code> and <code>requirements.txt</code>","text":"<p>The main <code>README.md</code> file for this entire repository now contains the standard setup instructions for this project, which includes creating a virtual environment and installing the project's dependencies.</p> <p>The key commands, which you should run from your terminal, are:</p> <ol> <li> <p>Create Environment: <code>python3 -m venv venv</code></p> </li> <li> <p>This creates a <code>venv</code> folder in your project directory.</p> </li> <li> <p>Activate Environment:</p> </li> <li> <p>macOS/Linux: <code>source venv/bin/activate</code></p> </li> <li>Windows: <code>venv\\\\Scripts\\\\activate</code></li> <li> <p>Your terminal prompt will change to show <code>(venv)</code>, indicating it's active.</p> </li> <li> <p>Install Packages: <code>pip install -r requirements.txt</code></p> </li> <li> <p>This command reads the <code>requirements.txt</code> file and installs the exact versions of all necessary packages into your active <code>venv</code>.</p> </li> <li> <p>Deactivate Environment: <code>deactivate</code></p> </li> <li> <p>When you're done, this command returns you to your normal shell.</p> </li> </ol>"},{"location":"lessons/day-21-virtual-environments/#exercises-day-21","title":"\ud83d\udcbb Exercises: Day 21","text":"<p>This lesson is about terminal commands, not Python scripts. The best way to learn is by doing.</p> <ol> <li> <p>Create a New Project:</p> </li> <li> <p>On your computer, create a new folder called <code>my_test_project</code>.</p> </li> <li> <p>Navigate into it using <code>cd my_test_project</code>.</p> </li> <li> <p>Initialize and Activate:</p> </li> <li> <p>Create a new virtual environment inside it: <code>python3 -m venv my_env</code>.</p> </li> <li> <p>Activate the new environment.</p> </li> <li> <p>Install and Freeze:</p> </li> <li> <p>Install a package that is not in our main project, for example: <code>pip install \"cowsay==5.0\"</code>.</p> </li> <li> <p>Generate a <code>requirements.txt</code> file for this new project using the <code>pip freeze</code> command. Open the file and see that <code>cowsay</code> is listed.</p> </li> <li> <p>Deactivate and Clean Up:</p> </li> <li> <p>Deactivate the environment. You can now delete the <code>my_test_project</code> folder.</p> </li> </ol> <p>\ud83c\udf89 Congratulations! You've practiced one of the most important skills for professional Python development. Using virtual environments will save you from countless headaches and make your projects more robust and shareable.</p>"},{"location":"lessons/day-21-virtual-environments/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> <li>virtual_environments.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 21: Solutions to Exercises (Command-Line Steps)\n\nThese exercises are performed in your terminal. This file describes\nthe steps you would take to complete them.\n\"\"\"\n\n# --- Exercise 1: Create a Project and Environment ---\nsolution_1 = \"\"\"\n--- Solution to Exercise 1 ---\n1. Open your terminal (Command Prompt, PowerShell, Terminal, etc.).\n2. Navigate to a place where you want to create your project (e.g., Desktop or Documents).\n   cd Desktop\n3. Create the project folder:\n   mkdir my_analytics_project\n4. Navigate into the new folder:\n   cd my_analytics_project\n5. Create the virtual environment (named 'venv'):\n   # On macOS/Linux:\n   python3 -m venv venv\n   # On Windows:\n   python -m venv venv\n\"\"\"\nprint(solution_1)\n\n\n# --- Exercise 2: Activate and Install ---\nsolution_2 = \"\"\"\n--- Solution to Exercise 2 ---\n1. Make sure you are inside the 'my_analytics_project' directory.\n2. Activate the environment:\n   # On macOS/Linux:\n   source venv/bin/activate\n   # On Windows Command Prompt:\n   venv\\\\Scripts\\\\activate.bat\n   # Your terminal prompt should now start with '(venv)'.\n\n3. Install the packages using pip:\n   pip install pandas scipy\n\n4. Verify the installation by listing the packages:\n   pip list\n   # You should see pandas, scipy, and their dependencies in the output.\n\"\"\"\nprint(solution_2)\n\n\n# --- Exercise 3: Create a Requirements File ---\nsolution_3 = \"\"\"\n--- Solution to Exercise 3 ---\n1. Ensure your virtual environment is still active.\n2. Run the 'pip freeze' command and redirect the output to a file:\n   pip freeze &gt; requirements.txt\n\n3. You can now see a 'requirements.txt' file in your project folder.\n   If you open it, it will contain lines like:\n   numpy==...\n   pandas==...\n   scipy==...\n   ...and other dependencies with their exact versions.\n\"\"\"\nprint(solution_3)\n\n\n# --- Exercise 4: Deactivate ---\nsolution_4 = \"\"\"\n--- Solution to Exercise 4 ---\n1. To exit the virtual environment, simply type:\n   deactivate\n2. Your terminal prompt will return to normal.\n\"\"\"\nprint(solution_4)\n</code></pre> virtual_environments.py <p>View on GitHub</p> virtual_environments.py<pre><code>\"\"\"\nDay 21: Virtual Environments\n\nThis lesson is primarily focused on commands you run in your terminal,\nnot in a Python script.\n\nPlease see the README.md file for the full lesson and exercises.\n\nThe key commands are:\n1. Create environment: python -m venv venv\n2. Activate (macOS/Linux): source venv/bin/activate\n3. Activate (Windows): venv\\\\Scripts\\\\activate.bat\n4. Install packages: pip install &lt;package_name&gt;\n5. Save packages: pip freeze &gt; requirements.txt\n6. Deactivate: deactivate\n\"\"\"\n\nprint(\"This Python script is a placeholder.\")\nprint(\n    \"Please follow the instructions in README.md to practice using virtual environments in your terminal.\"\n)\n</code></pre>"},{"location":"lessons/day-22-numpy/","title":"\ud83d\udcd8 Day 22: NumPy - The Foundation of Numerical Computing","text":"<p>While Python lists are flexible, they aren't efficient for large-scale numerical calculations. For this, we use NumPy (Numerical Python), the fundamental package for scientific and numerical computing in Python. It is the bedrock upon which almost all data science libraries, including Pandas, are built.</p>"},{"location":"lessons/day-22-numpy/#why-numpy-for-business","title":"Why NumPy for Business?","text":"<p>The core advantage of NumPy is vectorization. Instead of looping through 10,000 sales figures to apply a price increase, you can perform the operation on the entire dataset at once. This makes your code:</p> <ol> <li>Faster: NumPy operations are performed in highly optimized C code, making them orders of magnitude faster than Python loops.</li> <li>More Readable: <code>new_prices = prices * 1.05</code> is much clearer than a <code>for</code> loop.</li> </ol>"},{"location":"lessons/day-22-numpy/#key-numpy-concepts","title":"Key NumPy Concepts","text":"<ul> <li><code>ndarray</code>: The core data structure, a powerful N-dimensional array. It's like a Python list but faster and more memory-efficient.</li> <li>Vectorized Operations: Performing math on entire arrays at once (e.g., <code>revenue = prices_array * units_array</code>).</li> <li>Array Methods: NumPy arrays have built-in methods for fast calculations (e.g., <code>.sum()</code>, <code>.mean()</code>, <code>.max()</code>, <code>.std()</code>).</li> <li>Boolean Indexing: Using a conditional to create a boolean array (<code>True</code>/<code>False</code> values) and then using that array to filter your data.</li> </ul>"},{"location":"lessons/day-22-numpy/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries (including <code>numpy</code>).</p>"},{"location":"lessons/day-22-numpy/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>numpy_examples.py</code>, has been refactored to place each NumPy task into its own testable function.</p> <ol> <li>Review the Code: Open <code>Day_22_NumPy/numpy_examples.py</code>. Examine the functions <code>calculate_revenue_vectorized()</code>, <code>analyze_sales_data()</code>, and <code>filter_above_average()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_22_NumPy/numpy_examples.py\n</code></pre></li> <li>Run the Tests: You can run the tests for this lesson to verify the correctness of each function:    <pre><code>pytest tests/test_day_22.py\n</code></pre></li> </ol>"},{"location":"lessons/day-22-numpy/#exercises-day-22","title":"\ud83d\udcbb Exercises: Day 22","text":"<ol> <li> <p>Vectorized Revenue Calculation:</p> </li> <li> <p>In a new script (<code>my_solutions_22.py</code>), create two Python lists: <code>prices = [12.50, 15.00, 22.50]</code> and <code>units = [100, 85, 120]</code>.</p> </li> <li>Import the <code>calculate_revenue_vectorized</code> function from the lesson script.</li> <li> <p>Call the function with your lists and print the resulting NumPy array.</p> </li> <li> <p>Sales Data Analysis:</p> </li> <li> <p>You have a list of sales figures: <code>sales_data = [250, 300, 280, 450, 500, 220, 180]</code>.</p> </li> <li>Import and use the <code>analyze_sales_data</code> function to get a dictionary of statistics.</li> <li> <p>Print the total sales and the mean sales from the returned dictionary.</p> </li> <li> <p>Conditional Filtering with Arrays:</p> </li> <li> <p>Import the <code>filter_above_average</code> function and the <code>numpy</code> library (<code>import numpy as np</code>).</p> </li> <li>Create a NumPy array from your <code>sales_data</code> list from the previous exercise.</li> <li>Pass this array to the <code>filter_above_average</code> function to get a new array containing only the sales figures that were better than average.</li> <li>Print the resulting array.</li> </ol> <p>\ud83c\udf89 Fantastic start! NumPy is the essential first step into the world of high-performance data analysis in Python. Understanding vectorization will make all subsequent topics, especially Pandas, much easier to grasp.</p>"},{"location":"lessons/day-22-numpy/#additional-materials","title":"Additional Materials","text":"<ul> <li>numpy_examples.ipynb</li> <li>solutions.ipynb</li> </ul> numpy_examples.py <p>View on GitHub</p> numpy_examples.py<pre><code>\"\"\"\nDay 22: NumPy in Action for Business Analytics (Refactored)\n\nThis script demonstrates fundamental NumPy operations for\nefficient numerical analysis of business data. This version is\nrefactored into functions for better organization and testability.\n\"\"\"\n\nimport numpy as np\n\n\ndef calculate_revenue_vectorized(prices: list, units: list) -&gt; np.ndarray:\n    \"\"\"\n    Calculates revenue by performing a vectorized multiplication of prices and units.\n    \"\"\"\n    prices_array = np.array(prices)\n    units_array = np.array(units)\n    return prices_array * units_array\n\n\ndef analyze_sales_data(sales: list) -&gt; dict:\n    \"\"\"\n    Analyzes a list of sales data and returns a dictionary of key statistics.\n    \"\"\"\n    sales_array = np.array(sales)\n    if sales_array.size == 0:\n        return {}\n\n    return {\n        \"total\": sales_array.sum(),\n        \"mean\": sales_array.mean(),\n        \"max\": sales_array.max(),\n        \"min\": sales_array.min(),\n        \"std_dev\": sales_array.std(),\n    }\n\n\ndef filter_above_average(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Filters a NumPy array to return only the elements above its average.\n    \"\"\"\n    if data.size == 0:\n        return np.array([])\n\n    average = data.mean()\n    return data[data &gt; average]\n\n\ndef main():\n    \"\"\"Main function to demonstrate NumPy capabilities.\"\"\"\n    # --- Example 1: Vectorized Calculations ---\n    print(\"--- Calculating Revenue with Vectorization ---\")\n    prices_data = [12.50, 15.00, 22.50, 18.00, 19.99]\n    units_data = [100, 85, 120, 95, 110]\n\n    revenue_data = calculate_revenue_vectorized(prices_data, units_data)\n    print(f\"Prices array: {np.array(prices_data)}\")\n    print(f\"Units sold array: {np.array(units_data)}\")\n    print(f\"Revenue per product (vectorized): {revenue_data}\")\n    print(\"-\" * 20)\n\n    # --- Example 2: Sales Data Analysis ---\n    print(\"--- Analyzing Weekly Sales Data ---\")\n    weekly_sales_data = [250, 300, 280, 450, 500, 220, 180]\n    sales_stats = analyze_sales_data(weekly_sales_data)\n\n    print(f\"Sales for the week: {np.array(weekly_sales_data)}\")\n    print(f\"Total weekly sales: ${sales_stats['total']:.2f}\")\n    print(f\"Average daily sales: ${sales_stats['mean']:.2f}\")\n    print(f\"Best sales day: ${sales_stats['max']:.2f}\")\n    print(f\"Worst sales day: ${sales_stats['min']:.2f}\")\n    print(f\"Standard deviation of sales: ${sales_stats['std_dev']:.2f}\")\n    print(\"-\" * 20)\n\n    # --- Example 3: Conditional Filtering ---\n    print(\"--- Filtering for Above-Average Sales Days ---\")\n    sales_array = np.array(weekly_sales_data)\n    good_days = filter_above_average(sales_array)\n\n    print(f\"The sales figures for good days were: {good_days}\")\n    print(f\"Number of good sales days: {len(good_days)}\")\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 22: NumPy - Solutions\n\nThis file contains comprehensive solutions to all Day 22 exercises,\ndemonstrating NumPy fundamentals for business analytics and data science.\n\nAuthor: 50 Days of Python Course\nPurpose: Educational solutions for MBA students learning NumPy\n\"\"\"\n\nimport warnings\n\nimport numpy as np\n\n# Suppress numpy warnings for cleaner output\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n\ndef exercise_1_array_creation_and_vectorization():\n    \"\"\"\n    Exercise 1: Array Creation and Vectorization\n\n    Demonstrates the power of vectorized operations for business calculations\n    compared to traditional loop-based approaches.\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"\ud83d\udcca EXERCISE 1: Array Creation and Vectorization\")\n    print(\"=\" * 60)\n\n    # Create the data as requested\n    prices = [12.50, 15.00, 22.50, 18.00]\n    units = [100, 85, 120, 95]\n\n    print(f\"\ud83d\udcb0 Product Prices: {prices}\")\n    print(f\"\ud83d\udce6 Units Sold: {units}\")\n\n    # Convert to NumPy arrays\n    prices_array = np.array(prices)\n    units_array = np.array(units)\n\n    print(\"\\n\ud83d\udd27 Converting to NumPy arrays:\")\n    print(f\"   Prices array: {prices_array}\")\n    print(f\"   Units array: {units_array}\")\n    print(f\"   Prices array type: {type(prices_array)}\")\n    print(f\"   Data type: {prices_array.dtype}\")\n\n    # Vectorized multiplication (the magic of NumPy!)\n    revenue_array = prices_array * units_array\n\n    print(\"\\n\ud83d\udcc8 Revenue Calculation (Vectorized):\")\n    print(f\"   Revenue array: {revenue_array}\")\n    print(f\"   Total revenue: ${revenue_array.sum():,.2f}\")\n\n    # Demonstrate the difference with traditional loop approach\n    print(\"\\n\ud83d\udd0d Comparison with Traditional Loop Approach:\")\n    revenue_loop = []\n    for i in range(len(prices)):\n        revenue_loop.append(prices[i] * units[i])\n\n    print(f\"   Loop result: {revenue_loop}\")\n    print(f\"   Same result? {np.array_equal(revenue_array, np.array(revenue_loop))}\")\n\n    # Advanced: Show additional business insights\n    print(\"\\n\ud83d\udca1 Business Insights:\")\n    avg_price = prices_array.mean()\n    avg_units = units_array.mean()\n    avg_revenue = revenue_array.mean()\n\n    print(f\"   Average price per product: ${avg_price:.2f}\")\n    print(f\"   Average units sold: {avg_units:.1f}\")\n    print(f\"   Average revenue per product: ${avg_revenue:.2f}\")\n    print(\n        f\"   Best performing product (revenue): Product {np.argmax(revenue_array) + 1} (${revenue_array.max():.2f})\"\n    )\n    print(\n        f\"   Lowest performing product (revenue): Product {np.argmin(revenue_array) + 1} (${revenue_array.min():.2f})\"\n    )\n\n\ndef exercise_2_sales_data_analysis():\n    \"\"\"\n    Exercise 2: Sales Data Analysis\n\n    Demonstrates comprehensive statistical analysis of business data\n    using NumPy array methods.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83d\udcca EXERCISE 2: Sales Data Analysis\")\n    print(\"=\" * 60)\n\n    # Weekly sales data as requested\n    sales_data = np.array([250, 300, 280, 450, 500, 220, 180])\n    days_of_week = [\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\",\n    ]\n\n    print(\"\ud83d\udcc5 Weekly Sales Data:\")\n    for day, sales in zip(days_of_week, sales_data):\n        print(f\"   {day:&gt;9}: ${sales:&gt;6.2f}\")\n\n    # Calculate required metrics\n    total_weekly_sales = sales_data.sum()\n    average_daily_sales = sales_data.mean()\n    best_sales_day = sales_data.max()\n    worst_sales_day = sales_data.min()\n\n    print(\"\\n\ud83d\udcc8 Sales Metrics:\")\n    print(f\"   Total weekly sales: ${total_weekly_sales:,.2f}\")\n    print(f\"   Average daily sales: ${average_daily_sales:.2f}\")\n    print(f\"   Best sales day: ${best_sales_day:.2f}\")\n    print(f\"   Worst sales day: ${worst_sales_day:.2f}\")\n\n    # Additional business insights\n    print(\"\\n\ud83d\udca1 Advanced Business Insights:\")\n\n    # Standard deviation and variance\n    sales_std = sales_data.std()\n    sales_var = sales_data.var()\n\n    print(f\"   Sales volatility (std dev): ${sales_std:.2f}\")\n    print(f\"   Sales variance: {sales_var:.2f}\")\n\n    # Identify specific days\n    best_day_index = np.argmax(sales_data)\n    worst_day_index = np.argmin(sales_data)\n\n    print(f\"   Best performing day: {days_of_week[best_day_index]} (${best_sales_day})\")\n    print(\n        f\"   Worst performing day: {days_of_week[worst_day_index]} (${worst_sales_day})\"\n    )\n\n    # Performance relative to average\n    above_average_days = np.sum(sales_data &gt; average_daily_sales)\n    below_average_days = np.sum(sales_data &lt; average_daily_sales)\n\n    print(f\"   Days above average: {above_average_days}\")\n    print(f\"   Days below average: {below_average_days}\")\n\n    # Business recommendations\n    print(\"\\n\ud83d\udccb Business Recommendations:\")\n    if sales_std &gt; average_daily_sales * 0.3:\n        print(\n            \"   \u26a0\ufe0f  High sales volatility detected. Consider investigating factors causing variation.\"\n        )\n\n    weekend_sales = sales_data[5:7]  # Saturday and Sunday\n    weekday_sales = sales_data[0:5]  # Monday to Friday\n\n    avg_weekend = weekend_sales.mean()\n    avg_weekday = weekday_sales.mean()\n\n    print(f\"   Weekend average: ${avg_weekend:.2f}\")\n    print(f\"   Weekday average: ${avg_weekday:.2f}\")\n\n    if avg_weekend &lt; avg_weekday:\n        print(\n            \"   \ud83d\udca1 Weekend sales are lower. Consider weekend promotions or different staffing.\"\n        )\n    else:\n        print(\"   \u2705 Weekend sales are strong. Current strategy is working well.\")\n\n\ndef exercise_3_conditional_filtering():\n    \"\"\"\n    Exercise 3: Conditional Filtering with Arrays\n\n    Demonstrates boolean indexing and conditional filtering,\n    essential for data analysis and business intelligence.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83d\udd0d EXERCISE 3: Conditional Filtering with Arrays\")\n    print(\"=\" * 60)\n\n    # Using the same sales data from exercise 2\n    sales_data = np.array([250, 300, 280, 450, 500, 220, 180])\n    days_of_week = [\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\",\n    ]\n\n    print(f\"\ud83d\udcc5 Original Sales Data: {sales_data}\")\n\n    # Calculate average for filtering\n    average_daily_sales = sales_data.mean()\n    print(f\"\ud83d\udcca Average daily sales: ${average_daily_sales:.2f}\")\n\n    # Create boolean mask for days above average\n    above_average_boolean = sales_data &gt; average_daily_sales\n    print(f\"\\n\ud83d\udd0d Boolean mask (sales &gt; average): {above_average_boolean}\")\n\n    # Filter to get good sales days as requested\n    good_sales_days = sales_data[above_average_boolean]\n    print(f\"\ud83d\udcc8 Good sales days (above average): {good_sales_days}\")\n\n    # Additional filtering examples for business intelligence\n    print(\"\\n\ud83d\udcbc Advanced Filtering Examples:\")\n\n    # High performance days (&gt; 400)\n    high_performance = sales_data &gt; 400\n    high_sales_values = sales_data[high_performance]\n    print(f\"   High performance days (&gt;$400): {high_sales_values}\")\n\n    # Low performance days (&lt; 250)\n    low_performance = sales_data &lt; 250\n    low_sales_values = sales_data[low_performance]\n    print(f\"   Low performance days (&lt;$250): {low_sales_values}\")\n\n    # Days within one standard deviation of mean\n    std_dev = sales_data.std()\n    within_one_std = np.abs(sales_data - average_daily_sales) &lt;= std_dev\n    normal_days = sales_data[within_one_std]\n    print(f\"   Normal performance days (within 1 std dev): {normal_days}\")\n\n    # Get the actual day names for good sales days\n    good_day_names = np.array(days_of_week)[above_average_boolean]\n    print(f\"\\n\ud83d\udcc5 Names of good sales days: {list(good_day_names)}\")\n\n    # Multiple condition filtering\n    print(\"\\n\ud83c\udfaf Multiple Condition Filtering:\")\n\n    # Days with sales between 250 and 450\n    moderate_sales = (sales_data &gt;= 250) &amp; (sales_data &lt;= 450)\n    moderate_values = sales_data[moderate_sales]\n    print(f\"   Sales between $250-$450: {moderate_values}\")\n\n    # Weekend OR high sales (&gt; 400)\n    weekend_or_high = (sales_data &gt; 400) | np.isin(\n        np.arange(len(sales_data)), [5, 6]\n    )  # Sat, Sun indices\n    special_days = sales_data[weekend_or_high]\n    print(f\"   Weekend or high sales days: {special_days}\")\n\n    # Business intelligence summary\n    print(\"\\n\ud83d\udcca Business Intelligence Summary:\")\n    print(\n        f\"   \u2022 {np.sum(above_average_boolean)} out of {len(sales_data)} days exceeded average sales\"\n    )\n    print(f\"   \u2022 {np.sum(high_performance)} days had exceptional sales (&gt;$400)\")\n    print(f\"   \u2022 {np.sum(low_performance)} days underperformed (&lt;$250)\")\n    print(\n        f\"   \u2022 Performance consistency: {np.sum(within_one_std)} days within normal range\"\n    )\n\n\ndef bonus_exercise_advanced_numpy():\n    \"\"\"\n    Bonus Exercise: Advanced NumPy for Business Analytics\n\n    Demonstrates more sophisticated NumPy operations for real-world\n    business scenarios including multi-dimensional arrays and advanced operations.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83d\ude80 BONUS: Advanced NumPy for Business Analytics\")\n    print(\"=\" * 60)\n\n    # Multi-dimensional array: Sales by product by day\n    print(\"\ud83d\udcca Multi-Dimensional Sales Analysis\")\n\n    # Sales data for 3 products over 7 days\n    sales_matrix = np.array(\n        [\n            [120, 150, 130, 180, 200, 90, 80],  # Product A\n            [80, 90, 85, 120, 140, 70, 60],  # Product B\n            [200, 220, 210, 250, 280, 180, 160],  # Product C\n        ]\n    )\n\n    products = [\"Product A\", \"Product B\", \"Product C\"]\n    days = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\n    print(f\"Sales Matrix Shape: {sales_matrix.shape} (3 products \u00d7 7 days)\")\n    print(f\"Sales Matrix:\\n{sales_matrix}\")\n\n    # Analysis along different axes\n    print(\"\\n\ud83d\udcc8 Axis-based Analysis:\")\n\n    # Total sales per product (sum across days - axis=1)\n    total_by_product = sales_matrix.sum(axis=1)\n    print(f\"Total sales per product: {total_by_product}\")\n\n    for product, total in zip(products, total_by_product):\n        print(f\"   {product}: ${total:,}\")\n\n    # Total sales per day (sum across products - axis=0)\n    total_by_day = sales_matrix.sum(axis=0)\n    print(f\"\\nTotal sales per day: {total_by_day}\")\n\n    for day, total in zip(days, total_by_day):\n        print(f\"   {day}: ${total:,}\")\n\n    # Advanced operations\n    print(\"\\n\ud83d\udd0d Advanced Operations:\")\n\n    # Average sales per product\n    avg_by_product = sales_matrix.mean(axis=1)\n    print(\"Average daily sales per product:\")\n    for product, avg in zip(products, avg_by_product):\n        print(f\"   {product}: ${avg:.2f}\")\n\n    # Best and worst performing days for each product\n    print(\"\\nBest performing day for each product:\")\n    best_days_idx = sales_matrix.argmax(axis=1)\n    for i, (product, day_idx) in enumerate(zip(products, best_days_idx)):\n        best_sales = sales_matrix[i, day_idx]\n        print(f\"   {product}: {days[day_idx]} (${best_sales})\")\n\n    # Correlation analysis (bonus advanced topic)\n    print(\"\\n\ud83d\udcca Product Performance Correlation:\")\n    correlation_matrix = np.corrcoef(sales_matrix)\n    print(f\"Correlation matrix shape: {correlation_matrix.shape}\")\n\n    # Find most correlated products\n    for i in range(len(products)):\n        for j in range(i + 1, len(products)):\n            correlation = correlation_matrix[i, j]\n            print(f\"   {products[i]} vs {products[j]}: {correlation:.3f}\")\n\n    # Performance ranking\n    print(\"\\n\ud83c\udfc6 Performance Ranking:\")\n    product_ranking = np.argsort(total_by_product)[::-1]  # Descending order\n    for rank, product_idx in enumerate(product_ranking, 1):\n        product_name = products[product_idx]\n        total_sales = total_by_product[product_idx]\n        print(f\"   #{rank}: {product_name} - ${total_sales:,}\")\n\n    # Statistical insights\n    print(\"\\n\ud83d\udcc8 Statistical Insights:\")\n    overall_mean = sales_matrix.mean()\n    overall_std = sales_matrix.std()\n    print(f\"   Overall average daily sales: ${overall_mean:.2f}\")\n    print(f\"   Overall sales volatility (std dev): ${overall_std:.2f}\")\n\n    # Identify outliers (sales &gt; mean + 2*std)\n    outlier_threshold = overall_mean + 2 * overall_std\n    outliers = sales_matrix &gt; outlier_threshold\n    print(f\"   Outlier threshold (mean + 2*std): ${outlier_threshold:.2f}\")\n    print(f\"   Number of outlier days: {np.sum(outliers)}\")\n\n\ndef performance_comparison_demo():\n    \"\"\"\n    Demonstration of NumPy performance advantages over pure Python\n    for business data processing.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\u26a1 PERFORMANCE COMPARISON: NumPy vs Pure Python\")\n    print(\"=\" * 60)\n\n    import time\n\n    # Create large dataset for performance testing\n    size = 100000  # 100k data points\n    print(f\"\ud83d\udcca Processing {size:,} sales transactions\")\n\n    # Generate sample data\n    np.random.seed(42)  # For reproducible results\n    sales_data_large = np.random.uniform(\n        100, 1000, size\n    )  # Random sales between $100-$1000\n    sales_list = sales_data_large.tolist()  # Convert to Python list\n\n    print(\"\\n\ud83d\udd27 Test: Calculating 15% discount on all sales\")\n\n    # NumPy vectorized approach\n    start_time = time.time()\n    discounted_numpy = sales_data_large * 0.85\n    numpy_time = time.time() - start_time\n\n    # Pure Python approach\n    start_time = time.time()\n    discounted_python = [price * 0.85 for price in sales_list]\n    python_time = time.time() - start_time\n\n    print(\"\\n\u23f1\ufe0f  Performance Results:\")\n    print(f\"   NumPy vectorized: {numpy_time:.6f} seconds\")\n    print(f\"   Python list comp: {python_time:.6f} seconds\")\n    print(f\"   Speed improvement: {python_time / numpy_time:.1f}x faster with NumPy\")\n\n    # Verify results are the same\n    print(f\"   Results identical: {np.allclose(discounted_numpy, discounted_python)}\")\n\n    print(\"\\n\ud83d\udca1 Business Impact:\")\n    if python_time &gt; 0.001:  # If measurable difference\n        time_saved = python_time - numpy_time\n        print(f\"   Time saved per operation: {time_saved:.6f} seconds\")\n        print(f\"   For 1000 daily operations: {time_saved * 1000:.2f} seconds saved\")\n        print(f\"   Annual time savings: {time_saved * 1000 * 365 / 3600:.2f} hours\")\n\n    print(\"   \ud83d\udcbc For large-scale business analytics, NumPy is essential!\")\n\n\ndef practical_business_scenarios():\n    \"\"\"\n    Practical business scenarios demonstrating real-world NumPy applications.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83d\udcbc PRACTICAL BUSINESS SCENARIOS\")\n    print(\"=\" * 60)\n\n    # Scenario 1: Inventory Management\n    print(\"\ud83d\udce6 Scenario 1: Inventory Management\")\n\n    # Current inventory levels\n    inventory = np.array([150, 75, 200, 50, 300])\n    products = [\"Widget A\", \"Widget B\", \"Widget C\", \"Widget D\", \"Widget E\"]\n    reorder_point = np.array([100, 80, 150, 60, 250])\n\n    # Check which products need reordering\n    needs_reorder = inventory &lt; reorder_point\n    products_to_reorder = np.array(products)[needs_reorder]\n\n    print(f\"   Current inventory: {dict(zip(products, inventory))}\")\n    print(f\"   Products needing reorder: {list(products_to_reorder)}\")\n    print(f\"   Total products below reorder point: {np.sum(needs_reorder)}\")\n\n    # Scenario 2: Sales Forecasting\n    print(\"\\n\ud83d\udcc8 Scenario 2: Sales Trend Analysis\")\n\n    # Monthly sales data\n    months = np.array([\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"])\n    sales = np.array([10000, 12000, 15000, 14000, 18000, 22000])\n\n    # Calculate month-over-month growth\n    growth_rates = np.diff(sales) / sales[:-1] * 100\n\n    print(f\"   Monthly sales: {dict(zip(months, sales))}\")\n    print(\"   Month-over-month growth rates:\")\n    for i, rate in enumerate(growth_rates):\n        print(f\"      {months[i]} to {months[i + 1]}: {rate:.1f}%\")\n\n    avg_growth = growth_rates.mean()\n    print(f\"   Average monthly growth rate: {avg_growth:.1f}%\")\n\n    # Simple forecast for next month\n    next_month_forecast = sales[-1] * (1 + avg_growth / 100)\n    print(f\"   Forecast for Jul: ${next_month_forecast:,.0f}\")\n\n    # Scenario 3: Customer Segmentation\n    print(\"\\n\ud83d\udc65 Scenario 3: Customer Segmentation\")\n\n    # Customer data\n    customer_spending = np.array([500, 1500, 300, 2000, 800, 1200, 400, 3000, 600, 900])\n\n    # Define segments based on spending\n    high_value = customer_spending &gt; 1500\n    medium_value = (customer_spending &gt;= 800) &amp; (customer_spending &lt;= 1500)\n    low_value = customer_spending &lt; 800\n\n    print(f\"   Total customers: {len(customer_spending)}\")\n    print(\n        f\"   High-value customers (&gt;$1500): {np.sum(high_value)} ({np.sum(high_value) / len(customer_spending) * 100:.1f}%)\"\n    )\n    print(\n        f\"   Medium-value customers ($800-$1500): {np.sum(medium_value)} ({np.sum(medium_value) / len(customer_spending) * 100:.1f}%)\"\n    )\n    print(\n        f\"   Low-value customers (&lt;$800): {np.sum(low_value)} ({np.sum(low_value) / len(customer_spending) * 100:.1f}%)\"\n    )\n\n    # Average spending per segment\n    print(\"   Average spending per segment:\")\n    if np.sum(high_value) &gt; 0:\n        print(f\"      High-value: ${customer_spending[high_value].mean():.2f}\")\n    if np.sum(medium_value) &gt; 0:\n        print(f\"      Medium-value: ${customer_spending[medium_value].mean():.2f}\")\n    if np.sum(low_value) &gt; 0:\n        print(f\"      Low-value: ${customer_spending[low_value].mean():.2f}\")\n\n\ndef main():\n    \"\"\"\n    Main function to run all Day 22 solutions and demonstrations.\n    \"\"\"\n    print(\"\ud83d\udc0d Day 22: NumPy for Business Analytics - Solutions\")\n    print(\"\ud83c\udf93 50 Days of Python for MBA Program\")\n    print(\"\ud83d\udcda Comprehensive demonstrations of NumPy fundamentals\")\n\n    try:\n        # Core exercises\n        exercise_1_array_creation_and_vectorization()\n        exercise_2_sales_data_analysis()\n        exercise_3_conditional_filtering()\n\n        # Advanced content\n        bonus_exercise_advanced_numpy()\n        performance_comparison_demo()\n        practical_business_scenarios()\n\n        print(\"\\n\" + \"=\" * 60)\n        print(\"\ud83c\udf89 All NumPy Exercises Completed Successfully!\")\n        print(\"\ud83d\udca1 Key Skills Demonstrated:\")\n        print(\"   \ud83d\udcca Array creation and vectorized operations\")\n        print(\"   \ud83d\udcc8 Statistical analysis with NumPy methods\")\n        print(\"   \ud83d\udd0d Boolean indexing and conditional filtering\")\n        print(\"   \ud83c\udfe2 Multi-dimensional array operations\")\n        print(\"   \u26a1 Performance advantages over pure Python\")\n        print(\"   \ud83d\udcbc Real-world business applications\")\n        print(\"=\" * 60)\n\n        print(\"\\n\ud83d\ude80 Next Steps:\")\n        print(\"   \u2022 Master these NumPy fundamentals before moving to Pandas\")\n        print(\"   \u2022 Practice with your own business datasets\")\n        print(\"   \u2022 Explore NumPy's extensive documentation\")\n        print(\"   \u2022 Ready for Day 23: Pandas for advanced data manipulation!\")\n\n    except Exception as e:\n        print(f\"\u274c Error in main execution: {e}\")\n        import traceback\n\n        traceback.print_exc()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"lessons/day-23-pandas/","title":"\ud83d\udcd8 Day 23: Pandas - Your Data Analysis Superpower","text":"<p>If NumPy is the foundation, Pandas is the tool you will use every single day for practical data analysis. It is the most important library for data manipulation in Python, giving it the capabilities of a super-powered spreadsheet.</p>"},{"location":"lessons/day-23-pandas/#the-core-data-structures-series-and-dataframe","title":"The Core Data Structures: Series and DataFrame","text":"<ul> <li><code>Series</code>: A one-dimensional labeled array, like a single column in a spreadsheet.</li> <li><code>DataFrame</code>: A two-dimensional labeled table with columns of potentially different types. This is the primary object you will work with in Pandas.</li> </ul>"},{"location":"lessons/day-23-pandas/#key-dataframe-operations","title":"Key DataFrame Operations","text":"<ul> <li>Creation: Create a DataFrame from a dictionary (<code>pd.DataFrame(my_dict)</code>) or by reading a file (<code>pd.read_csv('my_file.csv')</code>).</li> <li>Inspection: Always inspect your data after loading.</li> <li><code>df.head()</code>: Shows the first 5 rows.</li> <li><code>df.info()</code>: Provides a summary of columns, data types, and non-null values.</li> <li><code>df.describe()</code>: Generates descriptive statistics for numerical columns.</li> <li>Selection: Select a single column (<code>df['ColumnName']</code>) or multiple columns (<code>df[['Col1', 'Col2']]</code>).</li> <li>Vectorized Operations: Create new columns by performing operations on existing ones (e.g., <code>df['Revenue'] = df['Price'] * df['Units Sold']</code>).</li> </ul>"},{"location":"lessons/day-23-pandas/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries (including <code>pandas</code>).</p>"},{"location":"lessons/day-23-pandas/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The content for this lesson is split into two main files:</p> <ol> <li><code>pandas_introduction.ipynb</code>: A new Jupyter Notebook that interactively walks through creating a DataFrame from scratch, inspecting it, and creating a visualization. This is the recommended starting point for this lesson.</li> <li><code>pandas_from_csv.py</code>: A refactored Python script that demonstrates the more common workflow of loading data from a CSV file and filtering it.</li> </ol>"},{"location":"lessons/day-23-pandas/#running-the-code","title":"Running the Code","text":"<ul> <li>To explore the notebook, you'll need to have Jupyter installed (<code>pip install jupyter</code>) and run <code>jupyter notebook</code> from your terminal.</li> <li>To run the script from the root directory of the project (<code>Coding-For-MBA</code>):   <pre><code>python Day_23_Pandas/pandas_from_csv.py\n</code></pre></li> <li>To run the tests for the script:   <pre><code>pytest tests/test_day_23.py\n</code></pre></li> </ul>"},{"location":"lessons/day-23-pandas/#exercises-day-23","title":"\ud83d\udcbb Exercises: Day 23","text":"<ol> <li> <p>Create an Employee DataFrame:</p> </li> <li> <p>In a new script (<code>my_solutions_23.py</code>), create a Python dictionary to store data for 3-4 employees (e.g., <code>Name</code>, <code>Department</code>, <code>Salary</code>).</p> </li> <li> <p>Convert this dictionary into a Pandas DataFrame and print it.</p> </li> <li> <p>Analyze Sales Data from a File:</p> </li> <li> <p>Import the <code>load_data_from_csv</code> and <code>filter_by_title</code> functions from the <code>pandas_from_csv</code> script.</p> </li> <li>The path to the data file is <code>data/hacker_news.csv</code>. Load it using the function.</li> <li> <p>Use the <code>filter_by_title</code> function to find all articles with \"Google\" in the title and print their titles.</p> </li> <li> <p>Calculate a New Column:</p> </li> <li> <p>Create a DataFrame with <code>'Price'</code> and <code>'Units Sold'</code> columns.</p> </li> <li>Create a new column called <code>'Revenue'</code> by multiplying the 'Price' and 'Units Sold' columns.</li> <li>Display the DataFrame with the new <code>'Revenue'</code> column.</li> </ol> <p>\ud83c\udf89 Welcome to Pandas! You've just learned how to create and inspect the most fundamental object in data analysis. In the next lesson, we'll dive deeper into selecting, filtering, and cleaning data.</p>"},{"location":"lessons/day-23-pandas/#additional-materials","title":"Additional Materials","text":"<ul> <li>pandas_from_csv.ipynb</li> <li>pandas_intro.ipynb</li> <li>solutions.ipynb</li> </ul> pandas_from_csv.py <p>View on GitHub</p> pandas_from_csv.py<pre><code>\"\"\"\nDay 23: Working with Real-World Data using Pandas from a CSV (Refactored)\n\nThis script demonstrates how to load data from a CSV file into a\nPandas DataFrame and perform basic filtering and inspection. This version\nis refactored into testable functions.\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Optional\n\nimport pandas as pd\n\n\ndef load_data_from_csv(file_path: str) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Loads data from a CSV file into a Pandas DataFrame.\n    Handles potential FileNotFoundError.\n    \"\"\"\n    try:\n        return pd.read_csv(file_path)\n    except FileNotFoundError:\n        print(f\"\u274c Error: The file was not found at {file_path}\")\n        return None\n\n\ndef filter_by_title(df: pd.DataFrame, keyword: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to find rows where the 'title' column contains a keyword.\n    This is case-insensitive.\n    \"\"\"\n    if df is None or \"title\" not in df.columns:\n        return pd.DataFrame()  # Return empty DataFrame if input is invalid\n\n    # Ensure title column is string type to use .str accessor\n    df[\"title\"] = df[\"title\"].astype(str)\n\n    return df.loc[df[\"title\"].str.contains(keyword, case=False, na=False)]\n\n\ndef main():\n    \"\"\"Main function to demonstrate loading and filtering a CSV.\"\"\"\n    print(\"--- Loading and Filtering Data from a CSV ---\")\n\n    # Construct the path to the data file relative to this script's location\n    # This makes the script more portable\n    resource_dir = Path(__file__).resolve().parent\n    data_path = resource_dir.parent / \"data\" / \"hacker_news.csv\"\n\n    # 1. Load the data\n    print(f\"Attempting to load data from: {data_path}\")\n    df = load_data_from_csv(data_path)\n\n    if df is not None:\n        print(f\"\u2705 Successfully loaded DataFrame with shape: {df.shape}\")\n        print(\"\\n--- First 5 rows ---\")\n        print(df.head())\n\n        # 2. Filter for titles containing 'Python'\n        print(\"\\n--- Filtering for titles containing 'Python' ---\")\n        python_titles_df = filter_by_title(df, \"Python\")\n        print(f\"Found {len(python_titles_df)} titles containing 'Python':\")\n        # Print only the title column for brevity\n        print(python_titles_df[\"title\"].to_string(index=False))\n\n        # 3. Filter for titles containing 'JavaScript'\n        print(\"\\n--- Filtering for titles containing 'JavaScript' ---\")\n        js_titles_df = filter_by_title(df, \"JavaScript\")\n        print(f\"Found {len(js_titles_df)} titles containing 'JavaScript':\")\n        print(js_titles_df[\"title\"].to_string(index=False))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> pandas_intro.py <p>View on GitHub</p> pandas_intro.py<pre><code>\"\"\"\nDay 23: Introduction to Pandas\n\nThis script demonstrates how to create a Pandas DataFrame,\nthe primary data structure for data analysis, and how to\nperform basic inspections.\n\"\"\"\n\n# The standard convention for importing pandas is to use the alias 'pd'\nimport pandas as pd\n\n# --- Creating a DataFrame from a Dictionary ---\n# This is a common way to create a DataFrame from scratch.\n# The keys of the dictionary become the column names.\n# The lists of values become the data in those columns.\ndata = {\n    \"Product Name\": [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\", \"Webcam\"],\n    \"Category\": [\n        \"Electronics\",\n        \"Electronics\",\n        \"Electronics\",\n        \"Electronics\",\n        \"Peripherals\",\n    ],\n    \"Price\": [1200, 25, 75, 300, 50],\n    \"Units Sold\": [150, 300, 220, 180, 250],\n}\n\n# Create the DataFrame\ndf = pd.DataFrame(data)\n\n\n# --- Inspecting the DataFrame ---\n# These are the first commands you should run after creating or loading a DataFrame.\nprint(\"--- Inspecting the DataFrame ---\")\n\n# .head() shows the first 5 rows\nprint(\"First 5 rows of the data (df.head()):\")\nprint(df.head())\nprint(\"-\" * 20)\n\n# .info() gives a summary of the DataFrame's structure\nprint(\"DataFrame summary (df.info()):\")\ndf.info()\nprint(\"-\" * 20)\n\n# .describe() provides descriptive statistics for numerical columns\nprint(\"Descriptive statistics (df.describe()):\")\nprint(df.describe())\nprint(\"-\" * 20)\n\n\n# --- Selecting Columns ---\nprint(\"--- Selecting Columns ---\")\n\n# To select a single column, use its name in square brackets.\n# This returns a Pandas Series.\nprice_column = df[\"Price\"]\nprint(\"The 'Price' column (a Pandas Series):\")\nprint(price_column)\nprint()\n\n# To select multiple columns, pass a list of column names.\n# This returns a new, smaller DataFrame.\nproduct_and_sales = df[[\"Product Name\", \"Units Sold\"]]\nprint(\"The 'Product Name' and 'Units Sold' columns (a new DataFrame):\")\nprint(product_and_sales)\nprint(\"-\" * 20)\n\n\n# --- Creating a New Column (Vectorized Operation) ---\nprint(\"--- Creating a New 'Revenue' Column ---\")\n# Like NumPy, Pandas allows for vectorized operations.\n# We can multiply two columns together without a loop.\ndf[\"Revenue\"] = df[\"Price\"] * df[\"Units Sold\"]\n\nprint(\"DataFrame after adding 'Revenue' column (df.head()):\")\nprint(df.head())\nprint(\"-\" * 20)\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 23: Solutions to Exercises\n\"\"\"\n\nimport pandas as pd\n\n# --- Exercise 1: Create an Employee DataFrame ---\nprint(\"--- Solution to Exercise 1 ---\")\nemployee_data = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"Department\": [\"Sales\", \"Engineering\", \"Marketing\", \"Sales\"],\n    \"Salary\": [80000, 120000, 95000, 85000],\n}\n\nemployee_df = pd.DataFrame(employee_data)\n\nprint(\"Employee DataFrame (employee_df.head()):\")\nprint(employee_df.head())\nprint()\nprint(\"Employee DataFrame Info (employee_df.info()):\")\nemployee_df.info()\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Analyze Sales Data ---\nprint(\"--- Solution to Exercise 2 ---\")\n# First, let's create the DataFrame from the lesson to work with\nsales_data = {\n    \"Product Name\": [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\"],\n    \"Category\": [\"Electronics\", \"Electronics\", \"Electronics\", \"Electronics\"],\n    \"Price\": [1200, 25, 75, 300],\n    \"Units Sold\": [150, 300, 220, 180],\n}\ndf = pd.DataFrame(sales_data)\n\n# Select and print the 'Product Name' column\nprint(\"Product Name column:\")\nprint(df[\"Product Name\"])\nprint()\n\n# Select and print 'Product Name' and 'Units Sold'\nprint(\"Product Name and Units Sold columns:\")\nprint(df[[\"Product Name\", \"Units Sold\"]])\nprint()\n\n# Use .describe() for a statistical summary\nprint(\"Statistical summary of numerical columns:\")\nprint(df.describe())\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Calculate a New Column ---\nprint(\"--- Solution to Exercise 3 ---\")\n# We use the same df from the previous exercise\n\n# Create the 'Revenue' column\ndf[\"Revenue\"] = df[\"Price\"] * df[\"Units Sold\"]\n\nprint(\"DataFrame with new 'Revenue' column (df.head()):\")\nprint(df.head())\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-24-pandas-advanced/","title":"\ud83d\udcd8 Day 24: Advanced Pandas - Working with Real Data","text":"<p>You'll rarely create data from scratch. The most common workflow is to load data from external sources like CSV files. Today, we'll focus on loading data and using powerful methods to select, filter, and clean it.</p>"},{"location":"lessons/day-24-pandas-advanced/#advanced-selection-loc-and-iloc","title":"Advanced Selection: <code>.loc</code> and <code>.iloc</code>","text":"<p>For complex selections, Pandas provides two powerful indexers:</p> <ul> <li><code>.loc</code> (Label-based): Selects data based on row and column labels.   <pre><code># Selects row with index label 3, and only the 'Product' and 'Revenue' columns\nsubset = df.loc[3, ['Product', 'Revenue']]\n</code></pre></li> <li><code>.iloc</code> (Integer-position based): Selects data based on its integer position.   <pre><code># Selects the first three rows (positions 0, 1, 2) and the first two columns (0, 1)\nsubset = df.iloc[0:3, 0:2]\n</code></pre></li> </ul>"},{"location":"lessons/day-24-pandas-advanced/#conditional-filtering-boolean-indexing","title":"Conditional Filtering (Boolean Indexing)","text":"<p>This is one of the most powerful features of Pandas. You can filter your DataFrame by providing a boolean (<code>True</code>/<code>False</code>) condition.</p> <pre><code># Find all high-revenue sales from the 'North' region\n# Note the parentheses around each condition\nhigh_rev_north = df[(df['Revenue'] &gt; 50000) &amp; (df['Region'] == 'North')]\n</code></pre>"},{"location":"lessons/day-24-pandas-advanced/#handling-missing-data","title":"Handling Missing Data","text":"<p>Real-world data is often messy and has missing values, represented as <code>NaN</code>.</p> <ul> <li><code>df.isnull().sum()</code>: A crucial command to count missing values in each column.</li> <li><code>df.dropna()</code>: Drops rows that contain any missing values.</li> <li><code>df.fillna(value)</code>: Fills missing values with a specified value (e.g., 0 or the column's mean).</li> </ul>"},{"location":"lessons/day-24-pandas-advanced/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-24-pandas-advanced/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>pandas_adv.py</code>, has been refactored to place each advanced operation into its own testable function.</p> <ol> <li>Review the Code: Open <code>Day_24_Pandas_Advanced/pandas_adv.py</code>. Examine functions like <code>filter_by_high_revenue()</code>, <code>filter_by_product_and_region()</code>, and <code>handle_missing_data()</code>.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script to see the functions in action:    <pre><code>python Day_24_Pandas_Advanced/pandas_adv.py\n</code></pre>    If the CSV file is missing, the refactored <code>handle_missing_data()</code> helper now raises    a clear <code>ValueError</code> explaining how to restore the dataset before continuing.</li> <li>Run the Tests: The tests use a sample DataFrame created in memory, so they don't depend on the external CSV file.    <pre><code>pytest tests/test_day_24.py\n</code></pre></li> </ol>"},{"location":"lessons/day-24-pandas-advanced/#interactive-plotly-visualisations","title":"\u2728 Interactive Plotly Visualisations","text":"<p>Plotly chart builders now sit alongside the existing data-wrangling helpers:</p> <ul> <li><code>build_revenue_by_region_bar_chart()</code> aggregates revenue totals for each region and renders an interactive bar chart.</li> <li><code>build_units_vs_price_scatter()</code> plots price sensitivity using <code>Units Sold</code> on the y-axis and encodes the point colour scale for quick outlier detection.</li> </ul> <p>To experiment locally:</p> <ol> <li>Install notebook dependencies if you have not done so already:    <pre><code>pip install notebook plotly\n</code></pre></li> <li>Launch Jupyter from the project root and open the companion notebook:    <pre><code>jupyter notebook Day_24_Pandas_Advanced/pandas_adv_interactive.ipynb\n</code></pre></li> <li>Run the cells to compare the quick Matplotlib baseline with the interactive Plotly versions. Hover, filter, and export the Plotly figures directly from the notebook toolbar.</li> </ol>"},{"location":"lessons/day-24-pandas-advanced/#profiling-the-workflow","title":"\ud83d\udd2c Profiling the Workflow","text":"<p>Curious about where Pandas spends its time? Launch the shared profiling helper to benchmark the lesson workflow:</p> <pre><code>python Day_24_Pandas_Advanced/profile_pandas_adv.py --mode cprofile\npython Day_24_Pandas_Advanced/profile_pandas_adv.py --mode timeit --repeat 5 --number 3\n</code></pre> <p>The first command prints a truncated <code>cProfile</code> report. In our baseline run the CSV load (<code>pandas.read_csv</code>) and the follow-up cleaning call (<code>handle_missing_data</code>) dominated the runtime, confirming that disk I/O and DataFrame materialisation are the hot spots.\u3010732170\u2020L1-L28\u3011 The <code>timeit</code> helper highlights how quickly the full workflow executes once the operating system cache is warm\u2014about 3 ms per iteration on average across five repeats.\u3010af7429\u2020L1-L7\u3011 If you plan to reuse the dataset across multiple analyses, load the CSV once and reuse the DataFrame rather than calling <code>read_csv</code> inside a tight loop.</p>"},{"location":"lessons/day-24-pandas-advanced/#exercises-day-24","title":"\ud83d\udcbb Exercises: Day 24","text":"<ol> <li> <p>Load and Inspect:</p> </li> <li> <p>In a new script (<code>my_solutions_24.py</code>), import <code>pandas as pd</code> and <code>pathlib</code>.</p> </li> <li>Load the <code>sales_data.csv</code> file (located in the <code>Day_24_Pandas_Advanced</code> directory) into a DataFrame.</li> <li> <p>Use <code>.head()</code> and <code>.info()</code> to inspect the loaded data.</p> </li> <li> <p>Select and Filter:</p> </li> <li> <p>Using the DataFrame from the previous exercise, import and use the <code>filter_by_product_and_region</code> function to find all sales of <code>\"Mouse\"</code> in the <code>\"South\"</code> region. Print the result.</p> </li> <li> <p>Import and use the <code>filter_by_high_revenue</code> function to find all sales with revenue over $70,000.</p> </li> <li> <p>Basic Data Cleaning:</p> </li> <li> <p>Import the <code>handle_missing_data</code> function.</p> </li> <li>Call the function twice on your DataFrame:<ul> <li>Once with <code>strategy='drop'</code> to remove rows with missing data.</li> <li>Once with <code>strategy='fill'</code> to fill missing revenue with the column average.</li> </ul> </li> <li>Print the <code>.shape</code> of both resulting DataFrames to see how they differ.</li> </ol> <p>\ud83c\udf89 Excellent work! You're now working with data like a real analyst\u2014loading it from files, inspecting it, and using powerful tools to filter and clean it. These are foundational skills for every data analysis project.</p>"},{"location":"lessons/day-24-pandas-advanced/#additional-materials","title":"Additional Materials","text":"<ul> <li>pandas_adv.ipynb</li> <li>profile_pandas_adv.ipynb</li> <li>solutions.ipynb</li> </ul> pandas_adv.py <p>View on GitHub</p> pandas_adv.py<pre><code>\"\"\"\nDay 24: Advanced Pandas - Working with Real Data (Refactored)\n\nThis script demonstrates loading data from a CSV file and\nusing advanced selection and cleaning techniques with Pandas,\nrefactored into testable functions.\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Any, List, Optional\n\nimport pandas as pd\nimport plotly.graph_objects as go\n\n\ndef load_sales_data(file_path: str) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Loads sales data from a CSV file into a Pandas DataFrame.\"\"\"\n    try:\n        return pd.read_csv(file_path)\n    except FileNotFoundError:\n        print(f\"\u274c Error: The file was not found at {file_path}\")\n        return None\n\n\ndef select_by_label(\n    df: pd.DataFrame, index_label: Any, columns: List[str]\n) -&gt; Optional[pd.Series]:\n    \"\"\"Selects data by row label and column names using .loc.\"\"\"\n    if df is None or df.empty:\n        return None\n    try:\n        return df.loc[index_label, columns]\n    except KeyError:\n        return None\n\n\ndef select_by_position(\n    df: pd.DataFrame, row_pos: int, col_slice: slice\n) -&gt; Optional[pd.Series]:\n    \"\"\"Selects data by integer position using .iloc.\"\"\"\n    if df is None or df.empty:\n        return None\n    try:\n        return df.iloc[row_pos, col_slice]\n    except IndexError:\n        return None\n\n\ndef filter_by_high_revenue(df: pd.DataFrame, threshold: float) -&gt; pd.DataFrame:\n    \"\"\"Filters the DataFrame for rows where Revenue exceeds a threshold.\"\"\"\n    if df is None or \"Revenue\" not in df.columns:\n        return pd.DataFrame()\n    return df[df[\"Revenue\"] &gt; threshold]\n\n\ndef filter_by_product_and_region(\n    df: pd.DataFrame, product: str, region: str\n) -&gt; pd.DataFrame:\n    \"\"\"Filters the DataFrame for a specific product and region.\"\"\"\n    if df is None or \"Product\" not in df.columns or \"Region\" not in df.columns:\n        return pd.DataFrame()\n    return df[(df[\"Product\"] == product) &amp; (df[\"Region\"] == region)]\n\n\ndef handle_missing_data(\n    df: Optional[pd.DataFrame], strategy: str = \"drop\", fill_value=None\n) -&gt; pd.DataFrame:\n    \"\"\"Handles missing data by either dropping rows or filling with a value.\"\"\"\n    if df is None or df.empty:\n        raise ValueError(\n            \"No sales data is available. Ensure the CSV exists and contains rows before\"\n            \" calling handle_missing_data.\"\n        )\n\n    df_copy = df.copy()\n    if strategy == \"drop\":\n        return df_copy.dropna()\n    elif strategy == \"fill\":\n        if fill_value is None:\n            # Default to filling with the mean for numeric columns\n            for col in df_copy.columns:\n                if pd.api.types.is_numeric_dtype(df_copy[col]):\n                    df_copy[col] = df_copy[col].fillna(df_copy[col].mean())\n        else:\n            df_copy = df_copy.fillna(fill_value)\n    return df_copy\n\n\ndef build_revenue_by_region_bar_chart(df: pd.DataFrame) -&gt; go.Figure:\n    \"\"\"Build an interactive bar chart comparing revenue across regions.\"\"\"\n\n    if df is None or df.empty:\n        raise ValueError(\"DataFrame must not be empty\")\n    if not {\"Region\", \"Revenue\"}.issubset(df.columns):\n        raise KeyError(\"DataFrame must include 'Region' and 'Revenue' columns\")\n\n    regional_revenue = (\n        df.groupby(\"Region\", dropna=False)[\"Revenue\"]\n        .sum(min_count=1)\n        .sort_values(ascending=False)\n    )\n    figure = go.Figure(\n        data=[\n            go.Bar(\n                x=regional_revenue.index.astype(str),\n                y=regional_revenue.values,\n                marker_color=\"#00A1D6\",\n                hovertemplate=\"Region: %{x}&lt;br&gt;Revenue: %{y:$,.0f}&lt;extra&gt;&lt;/extra&gt;\",\n            )\n        ]\n    )\n    figure.update_layout(\n        title=\"Revenue by Region\",\n        xaxis_title=\"Region\",\n        yaxis_title=\"Total Revenue\",\n        template=\"plotly_white\",\n    )\n    return figure\n\n\ndef build_units_vs_price_scatter(df: pd.DataFrame) -&gt; go.Figure:\n    \"\"\"Return a scatter plot showing how pricing relates to units sold.\"\"\"\n\n    if df is None or df.empty:\n        raise ValueError(\"DataFrame must not be empty\")\n    required_columns = {\"Units Sold\", \"Price\", \"Product\"}\n    if not required_columns.issubset(df.columns):\n        missing = \", \".join(sorted(required_columns - set(df.columns)))\n        raise KeyError(f\"Missing required columns: {missing}\")\n\n    figure = go.Figure(\n        data=[\n            go.Scatter(\n                x=df[\"Price\"],\n                y=df[\"Units Sold\"],\n                mode=\"markers\",\n                marker=dict(\n                    size=10,\n                    color=df[\"Units Sold\"],\n                    colorscale=\"Viridis\",\n                    showscale=True,\n                ),\n                text=df[\"Product\"],\n                hovertemplate=(\n                    \"Product: %{text}&lt;br&gt;Price: %{x:$,.0f}&lt;br&gt;Units Sold: %{y}&lt;extra&gt;&lt;/extra&gt;\"\n                ),\n            )\n        ]\n    )\n    figure.update_layout(\n        title=\"Units Sold vs. Price\",\n        xaxis_title=\"Price\",\n        yaxis_title=\"Units Sold\",\n        template=\"plotly_white\",\n    )\n    return figure\n\n\ndef main():\n    \"\"\"Main function to demonstrate advanced Pandas operations.\"\"\"\n    print(\"--- Loading and Inspecting sales_data.csv ---\")\n    resource_dir = Path(__file__).resolve().parent\n    data_path = resource_dir / \"sales_data.csv\"\n    df = load_sales_data(str(data_path))\n\n    if df is not None:\n        print(df.head())\n        print(\"-\" * 20)\n\n        print(\"--- Advanced Data Selection ---\")\n        product_3 = select_by_label(df, 3, [\"Product\", \"Revenue\"])\n        print(f\"Product and Revenue for row index 3 (using .loc):\\n{product_3}\\n\")\n\n        row_0 = select_by_position(df, 0, slice(0, 3))\n        print(f\"First row, first 3 columns (using .iloc):\\n{row_0}\\n\")\n        print(\"-\" * 20)\n\n        print(\"--- Conditional Filtering ---\")\n        high_revenue_df = filter_by_high_revenue(df, 50000)\n        print(f\"Found {len(high_revenue_df)} sales with revenue &gt; $50,000.\")\n\n        laptop_north_df = filter_by_product_and_region(df, \"Laptop\", \"North\")\n        print(f\"Found {len(laptop_north_df)} 'Laptop' sales in the 'North' region.\")\n        print(\"-\" * 20)\n\n        print(\"--- Handling Missing Data ---\")\n        print(f\"Original shape: {df.shape}\")\n        print(f\"Missing values count:\\n{df.isnull().sum()}\\n\")\n\n        df_dropped = handle_missing_data(df, strategy=\"drop\")\n        print(f\"Shape after dropping missing rows: {df_dropped.shape}\")\n\n        df_filled = handle_missing_data(df, strategy=\"fill\")\n        print(\n            f\"Missing values after filling with mean:\\n{df_filled.isnull().sum().sum()}\"\n        )\n        print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> profile_pandas_adv.py <p>View on GitHub</p> profile_pandas_adv.py<pre><code>\"\"\"Command-line helpers for profiling the Pandas advanced lesson.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport sys\nfrom pathlib import Path\nfrom typing import Callable\n\ntry:\n    from mypackage.profiling import print_report, profile_callable\nexcept ImportError:\n    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n    if str(PROJECT_ROOT) not in sys.path:\n        sys.path.append(str(PROJECT_ROOT))\n    from mypackage.profiling import print_report, profile_callable\n\ntry:  # pragma: no cover - runtime guard for script execution\n    from .pandas_adv import (\n        filter_by_high_revenue,\n        filter_by_product_and_region,\n        handle_missing_data,\n        load_sales_data,\n    )\nexcept ImportError:  # pragma: no cover - allows ``python profile_pandas_adv.py``\n    CURRENT_DIR = Path(__file__).resolve().parent\n    if str(CURRENT_DIR) not in sys.path:\n        sys.path.append(str(CURRENT_DIR))\n    from pandas_adv import (  # type: ignore  # pylint: disable=import-error\n        filter_by_high_revenue,\n        filter_by_product_and_region,\n        handle_missing_data,\n        load_sales_data,\n    )\n\n\ndef build_pipeline(\n    data_path: Path, threshold: float, product: str, region: str, missing_strategy: str\n) -&gt; Callable[[], None]:\n    \"\"\"Return a callable that executes the common lesson workflow.\"\"\"\n\n    def pipeline() -&gt; None:\n        df = load_sales_data(str(data_path))\n        if df is None or df.empty:\n            raise ValueError(\n                f\"Sales data could not be loaded from {data_path}. Ensure the CSV exists\"\n                \" and contains data.\"\n            )\n\n        filter_by_high_revenue(df, threshold)\n        filter_by_product_and_region(df, product, region)\n        handle_missing_data(df, strategy=missing_strategy)\n\n    return pipeline\n\n\ndef main() -&gt; None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        \"--mode\",\n        choices=(\"cprofile\", \"timeit\"),\n        default=\"cprofile\",\n        help=\"Profiling backend to use (default: cprofile)\",\n    )\n    parser.add_argument(\n        \"--threshold\",\n        type=float,\n        default=50_000,\n        help=\"Revenue threshold used in filter_by_high_revenue\",\n    )\n    parser.add_argument(\n        \"--product\",\n        default=\"Laptop\",\n        help=\"Product name used for filter_by_product_and_region\",\n    )\n    parser.add_argument(\n        \"--region\",\n        default=\"North\",\n        help=\"Region used for filter_by_product_and_region\",\n    )\n    parser.add_argument(\n        \"--missing-strategy\",\n        choices=(\"drop\", \"fill\"),\n        default=\"fill\",\n        help=\"Strategy used when calling handle_missing_data\",\n    )\n    parser.add_argument(\n        \"--repeat\",\n        type=int,\n        default=5,\n        help=\"Number of timing repeats when --mode=timeit\",\n    )\n    parser.add_argument(\n        \"--number\",\n        type=int,\n        default=1,\n        help=\"Number of calls per repeat when --mode=timeit\",\n    )\n    args = parser.parse_args()\n\n    data_path = Path(__file__).resolve().parent / \"sales_data.csv\"\n    pipeline = build_pipeline(\n        data_path=data_path,\n        threshold=args.threshold,\n        product=args.product,\n        region=args.region,\n        missing_strategy=args.missing_strategy,\n    )\n\n    profile_report, timing_report = profile_callable(\n        pipeline,\n        mode=args.mode,\n        repeat=args.repeat,\n        number=args.number,\n    )\n    print_report(profile_report=profile_report, timing_report=timing_report)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 24: Solutions to Exercises\n\"\"\"\n\nfrom pathlib import Path\n\nimport pandas as pd\n\n# --- Exercise 1: Load and Inspect ---\nprint(\"--- Solution to Exercise 1 ---\")\n# Load the data\nresource_dir = Path(__file__).resolve().parent\ndata_path = resource_dir / \"sales_data.csv\"\n\ntry:\n    df = pd.read_csv(data_path)\n    print(\"Successfully loaded sales_data.csv\")\n\n    # View the first few rows\n    print(\"\\n.head():\")\n    print(df.head())\n\n    # Check data types and for missing values\n    print(\"\\n.info():\")\n    df.info()\n\n    # Get statistical overview\n    print(\"\\n.describe():\")\n    print(df.describe())\n\nexcept FileNotFoundError:\n    print(\n        \"Error: sales_data.csv not found in the Day_24_Pandas_Advanced folder.\"\n        \" Keep the CSV beside this script.\"\n    )\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Select and Filter ---\nprint(\"--- Solution to Exercise 2 ---\")\nif \"df\" in locals():  # Check if the DataFrame was loaded successfully\n    # Select 'Product' and 'Revenue' for the first 5 rows using .loc\n    # Index labels are 0-4 for the first 5 rows.\n    product_revenue_subset = df.loc[  # pyright: ignore[reportPossiblyUnboundVariable]\n        0:4, [\"Product\", \"Revenue\"]\n    ]  # pyright: ignore[reportPossiblyUnboundVariable]\n    print(\"Product and Revenue for first 5 rows:\")\n    print(product_revenue_subset)\n    print()\n\n    # Select all sales from the 'South' region\n    south_sales = df[  # pyright: ignore[reportPossiblyUnboundVariable]\n        df[\"Region\"] == \"South\"  # pyright: ignore[reportPossiblyUnboundVariable]\n    ]  # pyright: ignore[reportPossiblyUnboundVariable]\n    print(\"All sales from the 'South' region:\")\n    print(south_sales)\n    print()\n\n    # Select sales with Units Sold &gt; 100 AND Revenue &gt; $20,000\n    high_performers = df[  # pyright: ignore[reportPossiblyUnboundVariable]\n        (df[\"Units Sold\"] &gt; 100)  # pyright: ignore[reportPossiblyUnboundVariable]\n        &amp; (df[\"Revenue\"] &gt; 20000)  # pyright: ignore[reportPossiblyUnboundVariable]\n    ]  # pyright: ignore[reportPossiblyUnboundVariable]\n    print(\"Sales with &gt;100 Units Sold and &gt;$20,000 Revenue:\")\n    print(high_performers)\nelse:\n    print(\"DataFrame 'df' not available for this exercise.\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Basic Data Cleaning ---\nprint(\"--- Solution to Exercise 3 ---\")\nif \"df\" in locals():\n    # Count missing values in each column\n    print(\"Count of missing values per column:\")\n    print(df.isnull().sum())  # pyright: ignore[reportPossiblyUnboundVariable]\n    print()\n\n    # Create a new DataFrame by dropping rows with missing values\n    df_cleaned = df.dropna()  # pyright: ignore[reportPossiblyUnboundVariable]\n    print(\n        \"Shape of original df:\",\n        df.shape,  # pyright: ignore[reportPossiblyUnboundVariable]\n    )  # pyright: ignore[reportPossiblyUnboundVariable]\n    print(\"Shape of cleaned df (after dropna):\", df_cleaned.shape)\n    print(\"Missing values count in cleaned df:\")\n    print(df_cleaned.isnull().sum())\n    print()\n\n    # Create another DataFrame where missing Revenue is filled with the mean\n    mean_revenue = df[  # pyright: ignore[reportPossiblyUnboundVariable]\n        \"Revenue\"\n    ].mean()  # pyright: ignore[reportPossiblyUnboundVariable]\n    print(f\"Mean revenue to be used for filling: ${mean_revenue:,.2f}\")\n    df_filled = df.copy()  # pyright: ignore[reportPossiblyUnboundVariable]\n    df_filled[\"Revenue\"] = df_filled[\"Revenue\"].fillna(mean_revenue)\n    print(\"Missing values count in filled df:\")\n    print(df_filled.isnull().sum())\n    print(\"First 5 rows of the filled DataFrame:\")\n    print(df_filled.head())\nelse:\n    print(\"DataFrame 'df' not available for this exercise.\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-25-data-cleaning/","title":"\ud83d\udcd8 Day 25: Data Cleaning - The Most Important Skill in Analytics","text":"<p>It's often said that data analysts spend about 80% of their time cleaning and preparing data. Messy, inconsistent data leads to incorrect analysis and bad business decisions. Learning to clean data effectively is a true superpower.</p>"},{"location":"lessons/day-25-data-cleaning/#common-data-cleaning-tasks","title":"Common Data Cleaning Tasks","text":"<ul> <li>Correcting Data Types: Columns are often loaded with the wrong type (e.g., a 'Price' column with '$' symbols is read as a string). Use <code>.astype()</code> to convert columns to the correct type (e.g., <code>float</code>, <code>datetime64[ns]</code>).</li> <li>String Manipulation: Use the <code>.str</code> accessor on a Series to apply string methods to every element at once (e.g., <code>df['Category'].str.lower()</code>, <code>df['Region'].str.strip()</code>).</li> <li>Standardizing Categories: Use the <code>.replace()</code> method to consolidate inconsistent values (e.g., mapping \"USA\" and \"United States\" to a single category).</li> <li>Handling Duplicates: Use <code>df.drop_duplicates()</code> to remove duplicate rows. The <code>subset</code> parameter lets you define which columns to check for duplicates (e.g., <code>subset=['OrderID']</code>).</li> </ul>"},{"location":"lessons/day-25-data-cleaning/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to set up your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-25-data-cleaning/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The script for this lesson, <code>data_cleaning.py</code>, has been refactored to encapsulate the entire cleaning process into a single, reusable function.</p> <ol> <li>Review the Code: Open <code>Day_25_Data_Cleaning/data_cleaning.py</code>. Examine the <code>clean_sales_data()</code> function, which performs all the cleaning steps on a DataFrame.</li> <li>Run the Script: From the root directory of the project (<code>Coding-For-MBA</code>), run the script. It will load the messy CSV, pass it to the cleaning function, and print the results.    <pre><code>python Day_25_Data_Cleaning/data_cleaning.py\n</code></pre></li> <li>Run the Tests: The tests use a sample messy DataFrame created in memory to verify that the entire cleaning pipeline works as expected.    <pre><code>pytest tests/test_day_25.py\n</code></pre></li> </ol>"},{"location":"lessons/day-25-data-cleaning/#exercises-day-25","title":"\ud83d\udcbb Exercises: Day 25","text":"<p>For these exercises, you will use the provided <code>messy_sales_data.csv</code> file.</p> <ol> <li> <p>Load and Clean:</p> </li> <li> <p>In a new script (<code>my_solutions_25.py</code>), import <code>pandas</code> and the <code>clean_sales_data</code> function from the lesson script.</p> </li> <li>Load the <code>messy_sales_data.csv</code> file into a DataFrame.</li> <li> <p>Pass your DataFrame to the <code>clean_sales_data</code> function to get a cleaned version.</p> </li> <li> <p>Verify the Cleaning:</p> </li> <li> <p>On your new <code>cleaned_df</code>, perform the following checks and print the results:</p> <ul> <li>Use <code>.info()</code> to confirm that 'Order Date' is a datetime and 'Price' is a float.</li> <li>Print the unique values of the 'Product' column (<code>cleaned_df['Product'].unique()</code>) to confirm they are all lowercase.</li> <li>Check the shape of the original DataFrame versus the cleaned one to see how many rows were removed.</li> </ul> </li> </ol> <p>\ud83c\udf89 Incredible work! Being able to take a messy, real-world dataset and turn it into a clean, analysis-ready format is arguably the most valuable skill a data analyst can possess.</p>"},{"location":"lessons/day-25-data-cleaning/#additional-materials","title":"Additional Materials","text":"<ul> <li>data_cleaning.ipynb</li> <li>solutions.ipynb</li> </ul> data_cleaning.py <p>View on GitHub</p> data_cleaning.py<pre><code>\"\"\"\nDay 25: Data Cleaning in Practice (Optimized)\n\nThis script demonstrates common data cleaning techniques on a\nmessy, real-world-style dataset using Pandas. This version includes\nperformance optimizations.\n\"\"\"\n\nfrom pathlib import Path\n\nimport pandas as pd\n\n\ndef clean_sales_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Cleans the sales data by correcting data types, standardizing text,\n    and removing duplicates.\n    \"\"\"\n    # --- 1. Correcting Data Types ---\n    df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n\n    # Optimized price cleaning using a single regex\n    df[\"Price\"] = df[\"Price\"].str.replace(r\"[$,]\", \"\", regex=True).astype(float)\n\n    # --- 2. Cleaning and Standardizing Text Data ---\n    df[\"Region\"] = df[\"Region\"].str.strip().str.lower()\n    df[\"Product\"] = df[\"Product\"].str.lower()\n    df[\"Region\"] = df[\"Region\"].replace({\"usa\": \"united states\"})\n\n    # --- 3. Handling Duplicates ---\n    df.drop_duplicates(inplace=True)\n    df.drop_duplicates(subset=[\"Order ID\"], keep=\"first\", inplace=True)\n\n    return df\n\n\ndef main():\n    \"\"\"\n    Main function to load, clean, and inspect the data.\n    \"\"\"\n    # --- Load the Messy Data ---\n    resource_dir = Path(__file__).resolve().parent\n    data_path = resource_dir / \"messy_sales_data.csv\"\n\n    print(\"--- Loading and Inspecting Messy Data ---\")\n    try:\n        df = pd.read_csv(data_path)\n        print(\"Original data types (df.info()):\")\n        df.info()\n        print(\"\\nOriginal data head:\")\n        print(df.head())\n    except FileNotFoundError:\n        print(\n            \"Error: messy_sales_data.csv not found in the Day_25_Data_Cleaning folder.\"\n        )\n        return\n\n    # --- Clean the Data ---\n    df_cleaned = clean_sales_data(\n        df.copy()\n    )  # Use a copy to avoid SettingWithCopyWarning\n\n    # --- Inspect Cleaned Data ---\n    print(\"\\n--- Inspecting Cleaned Data ---\")\n    print(\"\\nCleaned data types (df.info()):\")\n    df_cleaned.info()\n    print(\"\\nCleaned data head:\")\n    print(df_cleaned.head())\n    print(\"\\nUnique values in 'Region' column:\", df_cleaned[\"Region\"].unique())\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 25: Solutions to Exercises\n\"\"\"\n\nfrom pathlib import Path\n\nimport pandas as pd\n\n# --- Exercise 1: Load and Initial Clean ---\nprint(\"--- Solution to Exercise 1 ---\")\nresource_dir = Path(__file__).resolve().parent\ndata_path = resource_dir / \"messy_sales_data.csv\"\n\ntry:\n    # Load the data\n    df = pd.read_csv(data_path)\n    print(\"Original DataFrame info:\")\n    df.info()\n\n    # Convert 'Order Date' to datetime\n    df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n    print(\"\\n'Order Date' column converted to datetime.\")\n\n    # Clean and convert 'Price' to float\n    df[\"Price\"] = df[\"Price\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n    print(\"'Price' column cleaned and converted to float.\")\n\n    # Clean 'Region' column whitespace\n    df[\"Region\"] = df[\"Region\"].str.strip()\n    print(\"'Region' column whitespace stripped.\")\n\n    print(\"\\nDataFrame info after initial cleaning:\")\n    df.info()\n\nexcept FileNotFoundError:\n    print(\n        \"Error: messy_sales_data.csv not found in the Day_25_Data_Cleaning folder.\"\n        \" Keep the CSV beside this script.\"\n    )\n    df = pd.DataFrame()\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Standardize Categories ---\nprint(\"--- Solution to Exercise 2 ---\")\nif not df.empty:\n    # Standardize 'Product' column to lowercase\n    df[\"Product\"] = df[\"Product\"].str.lower()\n    print(\"'Product' column standardized to lowercase.\")\n    print(f\"Unique product values: {df['Product'].unique()}\")\n\n    # Standardize 'Region' column to 'USA'\n    df[\"Region\"] = df[\"Region\"].replace({\"United States\": \"USA\"})\n    print(\"'Region' column standardized to 'USA'.\")\n    print(f\"Unique region values: {df['Region'].unique()}\")\nelse:\n    print(\"DataFrame not available for this exercise.\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Handle Duplicates ---\nprint(\"--- Solution to Exercise 3 ---\")\nif not df.empty:\n    # Check for and count fully duplicate rows\n    num_duplicates = df.duplicated().sum()\n    print(f\"Number of fully duplicate rows found: {num_duplicates}\")\n\n    # Create df_cleaned by removing full duplicates\n    df_cleaned = df.drop_duplicates()\n    print(f\"Shape of original df: {df.shape}\")\n    print(f\"Shape after dropping duplicates (df_cleaned): {df_cleaned.shape}\")\n\n    # Check for duplicate Order IDs\n    num_duplicate_ids = df_cleaned.duplicated(subset=[\"Order ID\"]).sum()\n    print(f\"\\nNumber of duplicate Order IDs found: {num_duplicate_ids}\")\n\n    # Create df_final by removing duplicate Order IDs\n    df_final = df_cleaned.drop_duplicates(subset=[\"Order ID\"], keep=\"first\")\n    print(f\"Shape after dropping duplicate Order IDs (df_final): {df_final.shape}\")\n\n    print(\"\\nFinal cleaned DataFrame head:\")\n    print(df_final.head())\nelse:\n    print(\"DataFrame not available for this exercise.\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-26-statistics/","title":"\ud83d\udcd8 Day 26: Practical Statistics for Business Analysis","text":"<p>On Day 26 you expand beyond data wrangling and apply core statistical tools to business datasets. The refactored lesson script now exposes reusable helper functions for descriptive statistics, correlation analysis, and hypothesis testing so you can integrate them directly into your own notebooks or projects.</p>"},{"location":"lessons/day-26-statistics/#environment-setup","title":"Environment Setup","text":"<p>Before you begin, ensure you have followed the setup instructions in the main README.md to create your virtual environment and install the required libraries.</p>"},{"location":"lessons/day-26-statistics/#exploring-the-refactored-code","title":"Exploring the Refactored Code","text":"<p>The <code>stats.py</code> module has been organized into testable functions that separate computation from presentation.</p> <ol> <li>Review the Code: Open <code>Day_26_Statistics/stats.py</code> and look at the new    helpers:</li> <li><code>load_sales_data()</code> reads <code>sales_data.csv</code> and removes missing rows.</li> <li><code>summarize_revenue()</code> returns key revenue metrics and the full      <code>DataFrame.describe()</code> output.</li> <li><code>compute_correlations()</code> produces a correlation matrix for the numeric      sales fields.</li> <li><code>run_ab_test()</code> wraps SciPy's independent t-test and reports whether the      difference is statistically significant.</li> <li><code>build_revenue_distribution_chart()</code> visualises the revenue histogram as an      interactive Plotly figure.</li> <li><code>build_correlation_heatmap()</code> turns the correlation matrix into an      interactive heatmap with hover labels and colourbar explanations.</li> <li>Run the Script: From the project root, execute the module to see the    printed analysis that the helpers power.    <pre><code>python Day_26_Statistics/stats.py\n</code></pre></li> <li>Run the Tests: The automated tests create in-memory DataFrames and    duration samples to validate each helper without touching disk.    <pre><code>pytest tests/test_day_26.py\n</code></pre></li> </ol>"},{"location":"lessons/day-26-statistics/#explore-the-interactive-notebooks","title":"\ud83e\uddea Explore the Interactive Notebooks","text":"<p>Static Matplotlib previews are great for reports, but sometimes you need to hover over exact values or export a filtered view. Open the companion notebook to try the Plotly charts side by side with their static counterparts:</p> <ol> <li>Install notebook dependencies if you skipped them earlier:    <pre><code>pip install notebook plotly\n</code></pre></li> <li>Launch Jupyter and open the walkthrough:    <pre><code>jupyter notebook Day_26_Statistics/statistics_interactive.ipynb\n</code></pre></li> <li>Execute the notebook cells to compare the summary statistics, static plots,    and the new interactive revenue distribution and correlation heatmap.</li> </ol> <p>\ud83c\udf89 Great job! With these reusable statistics utilities you can move from simple summaries to rigorous, testable insights in your analyses.</p>"},{"location":"lessons/day-26-statistics/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> <li>stats.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 26: Solutions to Exercises\n\"\"\"\n\nfrom pathlib import Path\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\n# --- Exercise 1: Descriptive Statistics of Sales ---\nprint(\"--- Solution to Exercise 1 ---\")\nresource_dir = Path(__file__).resolve().parent\ndata_path = resource_dir / \"sales_data.csv\"\n\ntry:\n    df = pd.read_csv(data_path)\n    df.dropna(inplace=True)  # Clean the data first\n\n    revenue = df[\"Revenue\"]\n\n    print(f\"Mean Revenue: ${revenue.mean():,.2f}\")\n    print(f\"Median Revenue: ${revenue.median():,.2f}\")\n    print(f\"Standard Deviation of Revenue: ${revenue.std():,.2f}\")\n    print(f\"Minimum Revenue: ${revenue.min():,.2f}\")\n    print(f\"Maximum Revenue: ${revenue.max():,.2f}\")\n\nexcept FileNotFoundError:\n    print(\"Error: sales_data.csv not found. Keep the CSV beside this script.\")\n    df = pd.DataFrame()\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Correlation Analysis ---\nprint(\"--- Solution to Exercise 2 ---\")\nif not df.empty:\n    # Select only the numerical columns\n    numerical_cols = df[[\"Units Sold\", \"Price\", \"Revenue\"]]\n\n    # Calculate the correlation matrix\n    correlation_matrix = numerical_cols.corr()\n\n    print(\"Correlation Matrix:\")\n    print(correlation_matrix)\n    print(\n        \"\\nAnswer: 'Units Sold' and 'Revenue' have the strongest positive correlation (0.93).\"\n    )\nelse:\n    print(\"DataFrame not available for this exercise.\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: A/B Test Analysis (T-Test) ---\nprint(\"--- Solution to Exercise 3 ---\")\ngroup_a_durations = [10.5, 12.1, 11.8, 13.0, 12.5]\ngroup_b_durations = [12.8, 13.5, 13.2, 14.0, 13.8]\n\nprint(f\"Group A Durations: {group_a_durations}\")\nprint(f\"Group B Durations: {group_b_durations}\")\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(group_a_durations, group_b_durations)\n\nprint(f\"\\nP-value: {p_value:.4f}\")\n\n# Conclusion based on the p-value\nif p_value &lt; 0.05:  # pyright: ignore[reportOperatorIssue]\n    print(\n        \"Conclusion: The result is statistically significant. The two headlines likely have different effects on session duration.\"\n    )\nelse:\n    print(\n        \"Conclusion: The result is not statistically significant. We cannot conclude there is a difference between the headlines.\"\n    )\nprint(\"-\" * 20)\n</code></pre> stats.py <p>View on GitHub</p> stats.py<pre><code>\"\"\"Day 26: Practical Statistics in Python.\n\nThis module provides helper functions for loading sales data, generating\nsummary statistics, computing correlations, and running a simple A/B test.\n\nAll side effects (such as printing to the console) are encapsulated in the\n``main`` function so that the individual helpers are easy to import and test.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Iterable, Mapping, MutableMapping\n\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom pandas import DataFrame, Series\nfrom scipy.stats import ttest_ind\n\n\ndef load_sales_data(csv_path: Path | str | None = None) -&gt; DataFrame:\n    \"\"\"Load and clean the sales CSV file.\"\"\"\n\n    resource_dir = Path(__file__).resolve().parent\n    path = Path(csv_path) if csv_path is not None else resource_dir / \"sales_data.csv\"\n\n    try:\n        df = pd.read_csv(path)\n    except FileNotFoundError:\n        return pd.DataFrame()\n\n    return df.dropna(axis=0, how=\"any\")\n\n\ndef summarize_revenue(df: DataFrame) -&gt; Mapping[str, float | Series | DataFrame]:\n    \"\"\"Return descriptive statistics for the ``Revenue`` column.\"\"\"\n\n    if \"Revenue\" not in df:\n        raise KeyError(\"DataFrame must contain a 'Revenue' column\")\n\n    revenue = df[\"Revenue\"]\n    summary: MutableMapping[str, float | Series | DataFrame] = {\n        \"mean\": float(revenue.mean()),\n        \"median\": float(revenue.median()),\n        \"std\": float(revenue.std()),\n        \"min\": float(revenue.min()),\n        \"max\": float(revenue.max()),\n        \"describe\": df.describe(),\n    }\n    return summary\n\n\ndef compute_correlations(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Return the correlation matrix for the key numeric columns.\"\"\"\n\n    columns = [col for col in (\"Units Sold\", \"Price\", \"Revenue\") if col in df.columns]\n    if len(columns) &lt; 2:\n        raise ValueError(\n            \"At least two of 'Units Sold', 'Price', or 'Revenue' must be present\"\n        )\n\n    return df[columns].corr()\n\n\ndef build_revenue_distribution_chart(df: DataFrame) -&gt; go.Figure:\n    \"\"\"Create a histogram visualising the distribution of the ``Revenue`` column.\"\"\"\n\n    if \"Revenue\" not in df:\n        raise KeyError(\"DataFrame must contain a 'Revenue' column\")\n\n    revenue = df[\"Revenue\"].dropna()\n    figure = go.Figure(\n        data=[\n            go.Histogram(\n                x=revenue,\n                nbinsx=min(30, max(5, revenue.nunique() // 2 or 5)),\n                marker_color=\"#636EFA\",\n                opacity=0.85,\n                hovertemplate=\"Revenue: %{x:$,.0f}&lt;extra&gt;&lt;/extra&gt;\",\n            )\n        ]\n    )\n    figure.update_layout(\n        title=\"Revenue Distribution\",\n        xaxis_title=\"Revenue\",\n        yaxis_title=\"Frequency\",\n        template=\"plotly_white\",\n        bargap=0.05,\n    )\n    return figure\n\n\ndef build_correlation_heatmap(df: DataFrame) -&gt; go.Figure:\n    \"\"\"Create a heatmap to visualise correlations between key numeric metrics.\"\"\"\n\n    correlations = compute_correlations(df)\n    heatmap = go.Heatmap(\n        z=correlations.values,\n        x=list(correlations.columns),\n        y=list(correlations.index),\n        colorscale=\"Blues\",\n        zmin=-1,\n        zmax=1,\n        hovertemplate=\"%{y} vs %{x}: %{z:.2f}&lt;extra&gt;&lt;/extra&gt;\",\n        colorbar=dict(title=\"Correlation\"),\n    )\n    figure = go.Figure(data=[heatmap])\n    figure.update_layout(\n        title=\"Correlation Heatmap\",\n        template=\"plotly_white\",\n    )\n    return figure\n\n\ndef run_ab_test(\n    group_a: Iterable[float], group_b: Iterable[float], alpha: float = 0.05\n) -&gt; Mapping[str, float | bool]:\n    \"\"\"Run an independent t-test on two groups of durations.\"\"\"\n\n    t_statistic, p_value = ttest_ind(list(group_a), list(group_b))\n    return {\n        \"t_statistic\": float(t_statistic),\n        \"p_value\": float(p_value),\n        \"alpha\": float(alpha),\n        \"is_significant\": bool(p_value &lt; alpha),\n    }\n\n\ndef main() -&gt; None:\n    \"\"\"Execute the lesson workflow with helpful console output.\"\"\"\n\n    print(\"--- 1. Descriptive Statistics of Sales Data ---\")\n    df = load_sales_data()\n    if df.empty:\n        print(\"Error: sales_data.csv not found. Keep the CSV beside this script.\")\n    else:\n        revenue_summary = summarize_revenue(df)\n        print(f\"Mean Revenue: ${revenue_summary['mean']:,.2f}\")\n        print(f\"Median Revenue: ${revenue_summary['median']:,.2f}\")\n        print(f\"Standard Deviation of Revenue: ${revenue_summary['std']:,.2f}\")\n        print(f\"Minimum Revenue: ${revenue_summary['min']:,.2f}\")\n        print(f\"Maximum Revenue: ${revenue_summary['max']:,.2f}\")\n        print(\"\\nFull descriptive statistics (df.describe()):\")\n        print(revenue_summary[\"describe\"])\n    print(\"-\" * 20)\n\n    print(\"--- 2. Correlation Analysis ---\")\n    if not df.empty:\n        correlation_matrix = compute_correlations(df)\n        print(\"Correlation Matrix:\")\n        print(correlation_matrix)\n        print(\n            \"\\nAnalysis: 'Units Sold' and 'Revenue' have a strong positive correlation (0.93).\"\n        )\n        print(\"'Price' and 'Revenue' also have a strong positive correlation (0.83).\")\n        print(\n            \"'Price' and 'Units Sold' have a weak negative correlation (-0.23), which might be expected (higher price can sometimes mean fewer units).\"\n        )\n    else:\n        print(\"DataFrame not available for this exercise.\")\n    print(\"-\" * 20)\n\n    print(\"--- 3. A/B Test Analysis (T-Test) ---\")\n    group_a_durations = [10.5, 12.1, 11.8, 13.0, 12.5, 11.9, 12.3]\n    group_b_durations = [12.8, 13.5, 13.2, 14.0, 13.8, 14.1, 13.6]\n    print(f\"Group A (Old Headline) Durations: {group_a_durations}\")\n    print(f\"Group B (New Headline) Durations: {group_b_durations}\")\n\n    test_results = run_ab_test(group_a_durations, group_b_durations)\n    print(f\"\\nT-statistic: {test_results['t_statistic']:.4f}\")\n    print(f\"P-value: {test_results['p_value']:.4f}\")\n\n    if test_results[\"is_significant\"]:\n        print(\"\\nConclusion: The difference is statistically significant.\")\n        print(\n            \"We can conclude that the new headline (Group B) likely leads to longer session durations.\"\n        )\n    else:\n        print(\"\\nConclusion: The difference is not statistically significant.\")\n        print(\n            \"We cannot conclude that the new headline had a real effect on session duration.\"\n        )\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"lessons/day-27-visualization/","title":"\ud83d\udcd8 Day 27: Data Visualization - Communicating Insights","text":"<p>Visualising key business metrics makes it easier to communicate findings and uncover patterns. Day 27 introduces reusable Matplotlib and Seaborn helpers that create core business charts for the sales dataset you prepared in Day 24.</p>"},{"location":"lessons/day-27-visualization/#environment-setup","title":"Environment Setup","text":"<ol> <li>(Recommended) Create a virtual environment and activate it.</li> <li>Install dependencies from the root of the repository:    <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Ensure <code>sales_data.csv</code> from Day 24 is available in this lesson folder (or update the helper to point to your copy).</li> </ol>"},{"location":"lessons/day-27-visualization/#run-the-script","title":"Run the Script","text":"<p>Generate the four lesson visuals from the command line:</p> <pre><code>python Day_27_Visualization/visualization.py\n</code></pre> <p>Each call loads the shared plotting helpers, displays a chart, and waits for you to close the window before moving on.</p>"},{"location":"lessons/day-27-visualization/#explore-the-notebook","title":"Explore the Notebook","text":"<p>Open the companion notebook to iterate on the visuals and review interpretation guidance:</p> <pre><code>jupyter notebook Day_27_Visualization/visualization.ipynb\n</code></pre> <p>The notebook reuses the same plotting functions so you can experiment without duplicating logic.</p>"},{"location":"lessons/day-27-visualization/#run-tests","title":"Run Tests","text":"<p>A pytest suite validates the chart configuration (titles, labels, legends) using a headless Matplotlib backend:</p> <pre><code>pytest tests/test_day_27.py\n</code></pre> <p>Running the full repository test suite is also supported:</p> <pre><code>pytest\n</code></pre>"},{"location":"lessons/day-27-visualization/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> <li>visualization.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 27: Solutions to Exercises\n\"\"\"\n\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# --- Load and Prepare Data ---\n# We use the cleaned data from Day 24 for reliable plotting.\nresource_dir = Path(__file__).resolve().parent\ndata_path = resource_dir / \"sales_data.csv\"\n\ntry:\n    # We use the data from Day 24, so we need to reference its path\n    # parse_dates=['Date'] tells pandas to automatically convert the 'Date' column\n    df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n    df.dropna(inplace=True)  # Drop rows with missing values for simplicity\n    print(\"Data loaded successfully for exercises.\")\nexcept FileNotFoundError:\n    print(\"Error: sales_data.csv not found. Keep the CSV beside this script.\")\n    df = pd.DataFrame()\n\n\nif not df.empty:\n    # --- Exercise 1: Sales by Product ---\n    print(\"\\n--- Solution to Exercise 1 ---\")\n    plt.figure(figsize=(10, 6))\n    # Group by Product and sum the Units Sold for each\n    product_sales = df.groupby(\"Product\")[\"Units Sold\"].sum().reset_index()\n    sns.barplot(x=\"Product\", y=\"Units Sold\", data=product_sales)\n    plt.title(\"Total Units Sold by Product\")\n    plt.xlabel(\"Product Category\")\n    plt.ylabel(\"Total Units Sold\")\n    print(\"Displaying plot for Exercise 1. Please close the plot window.\")\n    plt.show()\n\n    # --- Exercise 2: Revenue Over Time ---\n    print(\"\\n--- Solution to Exercise 2 ---\")\n    # Group the data by date and sum the revenue for each day\n    daily_revenue = df.groupby(\"Date\")[\"Revenue\"].sum().reset_index()\n\n    plt.figure(figsize=(12, 6))\n    sns.lineplot(x=\"Date\", y=\"Revenue\", data=daily_revenue)\n    plt.title(\"Daily Revenue Trend\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Total Revenue ($)\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    print(\"Displaying plot for Exercise 2. Please close the plot window.\")\n    plt.show()\n\n    # --- Exercise 3: Price Distribution ---\n    print(\"\\n--- Solution to Exercise 3 ---\")\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x=\"Price\", bins=8, kde=False)\n    plt.title(\"Distribution of Product Prices\")\n    plt.xlabel(\"Price Bins ($)\")\n    plt.ylabel(\"Number of Products\")\n    print(\"Displaying plot for Exercise 3. Please close the plot window.\")\n    plt.show()\nelse:\n    print(\"\\nSkipping exercises as DataFrame could not be loaded.\")\n</code></pre> visualization.py <p>View on GitHub</p> visualization.py<pre><code>\"\"\"Day 27: Creating Business Visualizations.\n\nThis module provides reusable plotting functions for the lesson so that the\ncharts can be tested and embedded in notebooks without relying on the GUI\nbackend.  Each function returns a :class:`matplotlib.figure.Figure` instance.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Iterable\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Matplotlib/Seaborn global configuration\nsns.set_theme(style=\"whitegrid\")\n\nRESOURCE_DIR = Path(__file__).resolve().parent\nDEFAULT_DATA_PATHS: Iterable[Path] = (\n    RESOURCE_DIR / \"sales_data.csv\",\n    RESOURCE_DIR.parent / \"Day_24_Pandas_Advanced\" / \"sales_data.csv\",\n)\n\n\ndef load_sales_data(paths: Iterable[Path] = DEFAULT_DATA_PATHS) -&gt; pd.DataFrame:\n    \"\"\"Load the sales dataset used throughout the visualisation lesson.\n\n    Parameters\n    ----------\n    paths:\n        Candidate file paths that will be checked in order.  The first existing\n        CSV file is read.\n\n    Returns\n    -------\n    pandas.DataFrame\n        The cleaned sales data with parsed dates.  An empty DataFrame is\n        returned if none of the paths exist.\n    \"\"\"\n\n    for path in paths:\n        if path.exists():\n            df = pd.read_csv(path, parse_dates=[\"Date\"])\n            df.dropna(inplace=True)\n            return df\n\n    print(\"Error: sales_data.csv not found. Keep the CSV beside this script.\")\n    return pd.DataFrame()\n\n\ndef _validate_dataframe(df: pd.DataFrame, required_columns: Iterable[str]) -&gt; None:\n    missing = [col for col in required_columns if col not in df.columns]\n    if missing:\n        raise ValueError(f\"DataFrame is missing required columns: {missing}\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty; cannot build visualization.\")\n\n\ndef build_revenue_by_region_plot(df: pd.DataFrame) -&gt; plt.Figure:\n    \"\"\"Create a bar chart showing total revenue by region.\"\"\"\n\n    _validate_dataframe(df, [\"Region\", \"Revenue\"])\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.barplot(x=\"Region\", y=\"Revenue\", data=df, estimator=sum, errorbar=None, ax=ax)\n    ax.set_title(\"Total Revenue by Region\", fontsize=16)\n    ax.set_ylabel(\"Total Revenue ($)\")\n    ax.set_xlabel(\"Region\")\n    fig.tight_layout()\n    return fig\n\n\ndef build_daily_revenue_plot(df: pd.DataFrame) -&gt; plt.Figure:\n    \"\"\"Create a line chart showing daily total revenue.\"\"\"\n\n    _validate_dataframe(df, [\"Date\", \"Revenue\"])\n\n    daily_revenue = df.groupby(\"Date\")[\"Revenue\"].sum().reset_index()\n    fig, ax = plt.subplots(figsize=(12, 6))\n    sns.lineplot(x=\"Date\", y=\"Revenue\", data=daily_revenue, ax=ax)\n    ax.set_title(\"Daily Revenue Trend\", fontsize=16)\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"Total Revenue ($)\")\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)\n    fig.tight_layout()\n    return fig\n\n\ndef build_units_sold_distribution_plot(df: pd.DataFrame) -&gt; plt.Figure:\n    \"\"\"Create a histogram showing the distribution of units sold.\"\"\"\n\n    _validate_dataframe(df, [\"Units Sold\"])\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.histplot(x=\"Units Sold\", data=df, bins=10, kde=True, ax=ax)\n    ax.set_title(\"Distribution of Units Sold per Transaction\", fontsize=16)\n    ax.set_xlabel(\"Units Sold\")\n    ax.set_ylabel(\"Frequency\")\n    fig.tight_layout()\n    return fig\n\n\ndef build_price_vs_units_sold_plot(df: pd.DataFrame) -&gt; plt.Figure:\n    \"\"\"Create a scatter plot comparing price to units sold.\"\"\"\n\n    _validate_dataframe(df, [\"Price\", \"Units Sold\", \"Product\"])\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.scatterplot(x=\"Price\", y=\"Units Sold\", data=df, hue=\"Product\", ax=ax)\n    ax.set_title(\"Price vs. Units Sold\", fontsize=16)\n    ax.set_xlabel(\"Price ($)\")\n    ax.set_ylabel(\"Units Sold\")\n    ax.legend(title=\"Product\")\n    fig.tight_layout()\n    return fig\n\n\ndef main() -&gt; None:\n    \"\"\"Load the data and display the standard lesson charts.\"\"\"\n\n    df = load_sales_data()\n    if df.empty:\n        return\n\n    print(\n        \"Displaying Bar Chart: Total Revenue by Region. Close the window to continue.\"\n    )\n    build_revenue_by_region_plot(df).show()\n\n    print(\"Displaying Line Chart: Daily Revenue Trend. Close the window to continue.\")\n    build_daily_revenue_plot(df).show()\n\n    print(\n        \"Displaying Histogram: Distribution of Units Sold. Close the window to continue.\"\n    )\n    build_units_sold_distribution_plot(df).show()\n\n    print(\n        \"Displaying Scatter Plot: Price vs. Units Sold. Close the window to continue.\"\n    )\n    build_price_vs_units_sold_plot(df).show()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"lessons/day-28-advanced-visualization/","title":"\ud83d\udcd8 Day 28: Advanced Visualization & Customization","text":"<p>Creating a basic chart is just the first step. To effectively communicate your story, you need to customize your visualizations to make them clear, compelling, and professional. Today, we'll learn how to customize our plots and how to combine multiple plots into a single figure, like a dashboard.</p> <p>We'll continue to use Seaborn for plotting and Matplotlib for customization.</p>"},{"location":"lessons/day-28-advanced-visualization/#customizing-your-plots","title":"Customizing Your Plots","text":"<p>Once you've created a plot with Seaborn, you can use Matplotlib functions to fine-tune it.</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Start with a basic plot\nsns.barplot(x='Region', y='Revenue', data=df)\n\n# --- Customizations ---\n# Add a title with a specific font size\nplt.title('Total Revenue by Region', fontsize=16)\n\n# Add more descriptive axis labels\nplt.xlabel('Sales Region', fontsize=12)\nplt.ylabel('Total Revenue (in USD)', fontsize=12)\n\n# Change the limits of the y-axis\nplt.ylim(0, 200000)\n\n# Add a horizontal line, for example, to show a target\nplt.axhline(y=150000, color='r', linestyle='--', label='Sales Target')\nplt.legend() # Display the label for the horizontal line\n\n# Ensure labels fit\nplt.tight_layout()\n\n# Display the customized plot\nplt.show()\n</code></pre>"},{"location":"lessons/day-28-advanced-visualization/#creating-multiple-plots-subplots","title":"Creating Multiple Plots (Subplots)","text":"<p>Often, you want to display multiple charts together to tell a more complete story. Matplotlib's <code>subplots()</code> function is perfect for this. It creates a figure and a grid of axes.</p> <p><code>fig, axes = plt.subplots(nrows=, ncols=, figsize=())</code></p> <ul> <li><code>nrows</code>, <code>ncols</code>: The number of rows and columns in your grid of plots.</li> <li><code>figsize</code>: A tuple specifying the width and height of the entire figure in inches.</li> </ul> <p>You can then tell each Seaborn plot which <code>ax</code> (axis) to draw on.</p> <pre><code># Create a 1x2 grid of plots\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n\n# --- First Plot (on the left axis) ---\nsns.barplot(x='Region', y='Revenue', data=df, ax=axes[0])\naxes[0].set_title('Revenue by Region')\n\n# --- Second Plot (on the right axis) ---\nsns.histplot(df['Revenue'], bins=10, ax=axes[1])\naxes[1].set_title('Distribution of Revenue')\n\n# Add a title for the entire figure\nfig.suptitle('Sales Performance Overview', fontsize=20)\n\n# Display the dashboard\nplt.show()\n</code></pre>"},{"location":"lessons/day-28-advanced-visualization/#exercises-day-28","title":"\ud83d\udcbb Exercises: Day 28","text":"<p>For these exercises, you will use the cleaned <code>sales_data.csv</code> from Day 24.</p> <ol> <li> <p>Create a Customized Sales Chart:</p> </li> <li> <p>Load the cleaned sales data.</p> </li> <li>Create a bar chart showing the total <code>Revenue</code> for each <code>Product</code>.</li> <li> <p>Customize it:</p> <ul> <li>Give it the title \"Total Revenue per Product\".</li> <li>Set the y-axis label to \"Total Revenue (USD)\".</li> <li>Add a horizontal red dashed line representing the average revenue across all products.</li> <li>Save the figure to a file named <code>product_revenue.png</code>.</li> </ul> </li> <li> <p>Build a 2x1 Dashboard:</p> </li> <li> <p>Create a figure with two rows and one column of subplots.</p> </li> <li>Top Plot: A line chart showing the trend of <code>Units Sold</code> over <code>Date</code>. Make sure the date is on the x-axis.</li> <li>Bottom Plot: A scatter plot showing the relationship between <code>Price</code> and <code>Units Sold</code>.</li> <li>Give each plot its own descriptive title.</li> <li>Add an overall title to the entire figure: \"Sales Analysis Dashboard\".</li> </ol> <p>\ud83c\udf89 Fantastic! You can now create presentation-ready charts and combine them into simple dashboards. This ability to not just analyze, but also to present data in a customized and professional format is a key skill that separates great analysts from good ones.</p>"},{"location":"lessons/day-28-advanced-visualization/#additional-materials","title":"Additional Materials","text":"<ul> <li>advanced_visualization.ipynb</li> <li>solutions.ipynb</li> </ul> advanced_visualization.py <p>View on GitHub</p> advanced_visualization.py<pre><code>\"\"\"Utility functions for Day 28 advanced visualization examples.\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Iterable\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n\ndef _require_columns(df: pd.DataFrame, required: Iterable[str]) -&gt; None:\n    \"\"\"Raise a ``ValueError`` if any of the ``required`` columns are missing.\"\"\"\n\n    missing = set(required) - set(df.columns)\n    if missing:\n        raise ValueError(f\"DataFrame is missing required columns: {sorted(missing)}\")\n\n\ndef build_product_revenue_bar(df: pd.DataFrame) -&gt; plt.Figure:\n    \"\"\"Return a bar chart showing total revenue per product.\"\"\"\n\n    _require_columns(df, [\"Product\", \"Revenue\"])\n    if df.empty:\n        raise ValueError(\"DataFrame must contain at least one row to build the chart.\")\n\n    product_revenue = (\n        df.groupby(\"Product\", as_index=False)[\"Revenue\"]\n        .sum()\n        .sort_values(\"Revenue\", ascending=False)\n    )\n    avg_revenue = product_revenue[\"Revenue\"].mean()\n\n    fig, ax = plt.subplots(figsize=(12, 7))\n    sns.barplot(\n        x=\"Product\",\n        y=\"Revenue\",\n        data=product_revenue,\n        hue=\"Product\",\n        palette=\"viridis\",\n        legend=False,\n        ax=ax,\n    )\n\n    ax.set_title(\"Total Revenue per Product\", fontsize=18, weight=\"bold\")\n    ax.set_xlabel(\"Product Category\", fontsize=12)\n    ax.set_ylabel(\"Total Revenue (in USD)\", fontsize=12)\n    ax.tick_params(axis=\"x\", rotation=45)\n\n    ax.axhline(\n        y=avg_revenue,\n        color=\"red\",\n        linestyle=\"--\",\n        label=f\"Avg Revenue (${avg_revenue:,.2f})\",\n    )\n    ax.legend()\n    fig.tight_layout()\n    return fig\n\n\ndef build_sales_dashboard(df: pd.DataFrame) -&gt; plt.Figure:\n    \"\"\"Return a dashboard with a daily revenue line chart and revenue distribution histogram.\"\"\"\n\n    _require_columns(df, [\"Date\", \"Revenue\"])\n    if df.empty:\n        raise ValueError(\n            \"DataFrame must contain at least one row to build the dashboard.\"\n        )\n\n    daily_revenue = df.groupby(\"Date\")[\"Revenue\"].sum().sort_index().reset_index()\n\n    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 10))\n    sns.lineplot(x=\"Date\", y=\"Revenue\", data=daily_revenue, ax=axes[0], color=\"blue\")\n    axes[0].set_title(\"Daily Revenue Trend\", fontsize=14)\n    axes[0].set_xlabel(\"Date\")\n    axes[0].set_ylabel(\"Revenue ($)\")\n\n    sns.histplot(data=df, x=\"Revenue\", bins=15, kde=True, ax=axes[1], color=\"green\")\n    axes[1].set_title(\"Distribution of Individual Sale Revenue\", fontsize=14)\n    axes[1].set_xlabel(\"Revenue per Sale ($)\")\n    axes[1].set_ylabel(\"Frequency\")\n\n    fig.suptitle(\"Company Sales Dashboard\", fontsize=20, weight=\"bold\")\n    fig.tight_layout(rect=(0, 0.03, 1, 0.95))\n    return fig\n\n\ndef load_sales_data(data_path: Path | None = None) -&gt; pd.DataFrame:\n    \"\"\"Load the ``sales_data.csv`` file bundled with the lesson.\"\"\"\n\n    if data_path is None:\n        resource_dir = Path(__file__).resolve().parent\n        data_path = resource_dir / \"sales_data.csv\"\n\n    df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n    df.dropna(inplace=True)\n    return df\n\n\ndef main() -&gt; None:\n    \"\"\"Run the example workflow and display the generated figures.\"\"\"\n\n    try:\n        df = load_sales_data()\n    except FileNotFoundError:\n        print(\"Error: sales_data.csv not found. Keep the CSV beside this script.\")\n        return\n\n    print(\"Data loaded successfully.\")\n\n    print(\"\\n--- 1. Creating a Customized Plot ---\")\n    fig_bar = build_product_revenue_bar(df)\n    fig_bar.show()\n\n    print(\"\\n--- 2. Creating a 2x1 Dashboard ---\")\n    fig_dashboard = build_sales_dashboard(df)\n    fig_dashboard.show()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 28: Solutions to Exercises\n\"\"\"\n\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# --- Load and Prepare Data ---\n# We use the cleaned data from Day 24 for reliable plotting.\nresource_dir = Path(__file__).resolve().parent\ndata_path = resource_dir / \"sales_data.csv\"\n\ntry:\n    df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n    df.dropna(inplace=True)  # Drop rows with missing values for simplicity\n    print(\"Data loaded successfully for exercises.\")\nexcept FileNotFoundError:\n    print(\"Error: sales_data.csv not found. Keep the CSV beside this script.\")\n    df = pd.DataFrame()\n\n\nif not df.empty:\n    # --- Exercise 1: Create a Customized Sales Chart ---\n    print(\"\\n--- Solution to Exercise 1 ---\")\n\n    # Calculate total and average revenue by product\n    product_revenue = df.groupby(\"Product\")[\"Revenue\"].sum()\n    average_revenue = product_revenue.mean()\n\n    plt.figure(figsize=(10, 7))\n    sns.barplot(x=product_revenue.index, y=product_revenue.values)\n\n    # Customizations\n    plt.title(\"Total Revenue per Product\", fontsize=16)\n    plt.ylabel(\"Total Revenue (USD)\", fontsize=12)\n    plt.xlabel(\"Product\", fontsize=12)\n    plt.axhline(\n        y=average_revenue,\n        color=\"red\",\n        linestyle=\"--\",\n        label=f\"Average Revenue (${average_revenue:,.2f})\",\n    )\n    plt.legend()\n\n    # Save the figure\n    plt.savefig(\"product_revenue.png\", dpi=300)\n    print(\"Plot for Exercise 1 saved to 'product_revenue.png'. Displaying plot now.\")\n    plt.show()\n\n    # --- Exercise 2: Build a 2x1 Dashboard ---\n    print(\"\\n--- Solution to Exercise 2 ---\")\n\n    # Create the figure and axes grid\n    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 10))\n\n    # --- Top Plot: Units Sold Trend ---\n    daily_units = df.groupby(\"Date\")[\"Units Sold\"].sum().reset_index()\n    sns.lineplot(x=\"Date\", y=\"Units Sold\", data=daily_units, ax=axes[0])\n    axes[0].set_title(\"Trend of Units Sold Over Time\", fontsize=14)\n    axes[0].set_xlabel(\"Date\")\n    axes[0].set_ylabel(\"Total Units Sold\")\n\n    # --- Bottom Plot: Price vs. Units Sold ---\n    sns.scatterplot(x=\"Price\", y=\"Units Sold\", data=df, ax=axes[1], hue=\"Region\")\n    axes[1].set_title(\"Price vs. Units Sold by Region\", fontsize=14)\n    axes[1].set_xlabel(\"Price per Unit ($)\")\n    axes[1].set_ylabel(\"Units Sold per Transaction\")\n\n    # --- Final Touches ---\n    fig.suptitle(\"Sales Analysis Dashboard\", fontsize=20, weight=\"bold\")\n    plt.tight_layout(rect=(0, 0.03, 1, 0.95))\n\n    print(\"Displaying plot for Exercise 2. Please close the plot window.\")\n    plt.show()\n\nelse:\n    print(\"\\nSkipping exercises as DataFrame could not be loaded.\")\n</code></pre>"},{"location":"lessons/day-29-interactive-visualization/","title":"\ud83d\udcd8 Day 29: Interactive Visualization with Plotly","text":"<p>Static charts are good for reports, but in the modern era of business intelligence, users expect to be able to interact with their data. They want to hover over data points to get more details, zoom into specific time ranges, and filter data on the fly.</p> <p>For this, we use Plotly. Plotly is a powerful Python library for creating interactive, publication-quality graphs online. The charts can be displayed in Python notebooks, saved as HTML files, or embedded in web applications and dashboards.</p>"},{"location":"lessons/day-29-interactive-visualization/#plotly-express-the-easy-way-to-plot","title":"Plotly Express: The Easy Way to Plot","text":"<p>Plotly is a large library, but its <code>plotly.express</code> module (standardly imported as <code>px</code>) is a simple, high-level interface for creating entire figures at once. It's the recommended starting point for most use cases.</p> <p>The syntax for <code>plotly.express</code> is very similar to Seaborn's, which makes it easy to learn.</p>"},{"location":"lessons/day-29-interactive-visualization/#creating-interactive-charts","title":"Creating Interactive Charts","text":"<p>Let's see how to create interactive versions of the charts we made previously. When you run this code, Plotly will typically open the chart in your web browser. You can then interact with it.</p>"},{"location":"lessons/day-29-interactive-visualization/#1-interactive-bar-chart","title":"1. Interactive Bar Chart","text":"<pre><code>import plotly.express as px\n\n# Assuming 'df' is a DataFrame with 'Region' and 'Revenue'\n# We first group the data to get total revenue per region\nregion_revenue = df.groupby('Region')['Revenue'].sum().reset_index()\n\nfig = px.bar(region_revenue, x='Region', y='Revenue', title='Total Revenue by Region')\nfig.show()\n</code></pre> <p>Interaction: Hover over the bars to see the exact revenue for each region.</p>"},{"location":"lessons/day-29-interactive-visualization/#2-interactive-line-chart","title":"2. Interactive Line Chart","text":"<pre><code># Assuming 'daily_revenue' is a DataFrame with 'Date' and 'Revenue'\nfig = px.line(daily_revenue, x='Date', y='Revenue', title='Daily Revenue Trend')\nfig.show()\n</code></pre> <p>Interaction: Hover along the line to see the date and revenue for any point. Click and drag to zoom into a specific time period. Double-click to zoom back out.</p>"},{"location":"lessons/day-29-interactive-visualization/#3-interactive-scatter-plot","title":"3. Interactive Scatter Plot","text":"<pre><code># Assuming 'df' has 'Price' and 'Units Sold'\nfig = px.scatter(df, x='Price', y='Units Sold', color='Product', hover_data=['Region'], title='Price vs. Units Sold')\nfig.show()\n</code></pre> <p>Interaction: Hover over any point to see the Price, Units Sold, Product, and Region for that specific transaction. Click on items in the legend to toggle product categories on and off.</p>"},{"location":"lessons/day-29-interactive-visualization/#saving-your-plot","title":"Saving Your Plot","text":"<p>You can save your interactive chart as a standalone HTML file that anyone can open in their web browser.</p> <pre><code># After creating your figure with px...\nfig.write_html(\"interactive_revenue_chart.html\")\n</code></pre> <p>\u2139\ufe0f Version control note: Any HTML (and PNG) files generated by these exercises are ignored by git. Feel free to regenerate them locally whenever you rerun the lesson.</p>"},{"location":"lessons/day-29-interactive-visualization/#exercises-day-29","title":"\ud83d\udcbb Exercises: Day 29","text":"<p>For these exercises, you will use the cleaned <code>sales_data.csv</code> from Day 24.</p> <ol> <li> <p>Interactive Sales by Product:</p> </li> <li> <p>Load the cleaned sales data.</p> </li> <li>Group the data by <code>Product</code> to get the sum of <code>Revenue</code> for each product.</li> <li>Create an interactive bar chart using <code>plotly.express</code> that shows the total <code>Revenue</code> for each <code>Product</code>.</li> <li>Give your chart a descriptive title.</li> <li> <p>When you hover over a bar, it should show the Product and its total Revenue.</p> </li> <li> <p>Interactive Revenue vs. Units Sold:</p> </li> <li> <p>Load the cleaned sales data.</p> </li> <li>Create an interactive scatter plot showing <code>Revenue</code> on the y-axis and <code>Units Sold</code> on the x-axis.</li> <li>Color the points on the scatter plot by <code>Region</code>.</li> <li>Give the plot a title.</li> <li>Save the resulting chart to an HTML file named <code>revenue_scatterplot.html</code>.</li> </ol> <p>\ud83c\udf89 Incredible! You've now stepped into the world of interactive data visualization. Being able to create and share plots that allow stakeholders to explore the data for themselves is a highly valuable skill for any modern analyst.</p>"},{"location":"lessons/day-29-interactive-visualization/#additional-materials","title":"Additional Materials","text":"<ul> <li>interactive_visualization.ipynb</li> <li>solutions.ipynb</li> </ul> interactive_visualization.py <p>View on GitHub</p> interactive_visualization.py<pre><code>\"\"\"Reusable helpers for Day 29 interactive Plotly visualisations.\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Iterable\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n__all__ = [\n    \"load_sales_data\",\n    \"build_region_revenue_bar\",\n    \"build_daily_revenue_line\",\n    \"build_price_units_scatter\",\n    \"main\",\n]\n\n\ndef _require_columns(df: pd.DataFrame, required: Iterable[str]) -&gt; None:\n    \"\"\"Raise ``ValueError`` if ``df`` is missing any of ``required`` columns.\"\"\"\n\n    missing = set(required) - set(df.columns)\n    if missing:\n        columns = \", \".join(sorted(missing))\n        raise ValueError(f\"DataFrame is missing required columns: {columns}\")\n    if df.empty:\n        raise ValueError(\"DataFrame must contain at least one row to build the figure.\")\n\n\ndef load_sales_data(data_path: Path | str | None = None) -&gt; pd.DataFrame:\n    \"\"\"Return the ``sales_data.csv`` dataset bundled with the lesson.\"\"\"\n\n    if data_path is None:\n        resource_dir = Path(__file__).resolve().parent\n        data_path = resource_dir / \"sales_data.csv\"\n\n    df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n    return df.dropna().reset_index(drop=True)\n\n\ndef build_region_revenue_bar(df: pd.DataFrame) -&gt; go.Figure:\n    \"\"\"Return a bar chart showing total revenue by region.\"\"\"\n\n    _require_columns(df, [\"Region\", \"Revenue\"])\n\n    region_revenue = (\n        df.groupby(\"Region\", as_index=False)[\"Revenue\"].sum().sort_values(\"Region\")\n    )\n\n    fig = px.bar(\n        region_revenue,\n        x=\"Region\",\n        y=\"Revenue\",\n        color=\"Region\",\n        title=\"Total Revenue by Region\",\n        labels={\"Revenue\": \"Total Revenue (USD)\"},\n    )\n    fig.update_layout(showlegend=False)\n    return fig\n\n\ndef build_daily_revenue_line(df: pd.DataFrame) -&gt; go.Figure:\n    \"\"\"Return a daily revenue line chart with markers.\"\"\"\n\n    _require_columns(df, [\"Date\", \"Revenue\"])\n\n    daily_revenue = (\n        df.groupby(\"Date\", as_index=False)[\"Revenue\"].sum().sort_values(\"Date\")\n    )\n    # ``plotly`` preserves ``datetime64`` values when rendering, which pandas now\n    # returns from ``groupby`` aggregations.  Converting to plain ``datetime``\n    # objects keeps backwards compatibility with the existing visualisation and\n    # tests that expect Python ``datetime`` instances.\n    daily_revenue[\"Date\"] = [\n        pd.Timestamp(ts).to_pydatetime() for ts in daily_revenue[\"Date\"]\n    ]\n\n    fig = px.line(\n        daily_revenue,\n        x=\"Date\",\n        y=\"Revenue\",\n        title=\"Daily Revenue Trend\",\n        markers=True,\n    )\n    fig.update_traces(mode=\"lines+markers\")\n    fig.update_layout(yaxis_title=\"Revenue (USD)\")\n    for trace in fig.data:\n        python_datetimes = tuple(pd.Timestamp(x).to_pydatetime() for x in trace.x)\n        trace.update(x=python_datetimes)\n    return fig\n\n\ndef build_price_units_scatter(df: pd.DataFrame) -&gt; go.Figure:\n    \"\"\"Return a scatter plot comparing price and units sold with revenue sizing.\"\"\"\n\n    _require_columns(df, [\"Price\", \"Units Sold\", \"Revenue\", \"Product\", \"Region\"])\n\n    fig = px.scatter(\n        df,\n        x=\"Price\",\n        y=\"Units Sold\",\n        color=\"Product\",\n        size=\"Revenue\",\n        hover_data=[\"Region\", \"Revenue\"],\n        title=\"Price vs. Units Sold Analysis\",\n    )\n    fig.update_layout(\n        legend_title_text=\"Product\",\n        xaxis_title=\"Price (USD)\",\n        yaxis_title=\"Units Sold\",\n    )\n    return fig\n\n\ndef main() -&gt; None:\n    \"\"\"Load the lesson dataset and display the interactive figures.\"\"\"\n\n    try:\n        df = load_sales_data()\n    except FileNotFoundError:\n        print(\"Error: sales_data.csv not found. Keep the CSV beside this script.\")\n        return\n\n    print(\"Data loaded successfully.\")\n\n    print(\"\\n--- 1. Interactive Bar Chart: Revenue by Region ---\")\n    build_region_revenue_bar(df).show()\n\n    print(\"\\n--- 2. Interactive Line Chart: Revenue Over Time ---\")\n    build_daily_revenue_line(df).show()\n\n    print(\"\\n--- 3. Interactive Scatter Plot: Price vs. Units Sold ---\")\n    scatter = build_price_units_scatter(df)\n    scatter.show()\n    output_filename = \"interactive_scatter_plot.html\"\n    scatter.write_html(output_filename)\n    print(\n        f\"\\nScatter plot saved to '{output_filename}'. You can open this file in a web browser.\"\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 29: Solutions to Exercises\n\"\"\"\n\nfrom pathlib import Path\n\nimport pandas as pd\nimport plotly.express as px\n\n# --- Load and Prepare Data ---\nresource_dir = Path(__file__).resolve().parent\ndata_path = resource_dir / \"sales_data.csv\"\n\ntry:\n    df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n    df.dropna(inplace=True)  # Drop rows with missing values for simplicity\n    print(\"Data loaded successfully for exercises.\")\nexcept FileNotFoundError:\n    print(\"Error: sales_data.csv not found. Keep the CSV beside this script.\")\n    df = pd.DataFrame()\n\n\nif not df.empty:\n    # --- Exercise 1: Interactive Sales by Product ---\n    print(\"\\n--- Solution to Exercise 1 ---\")\n\n    # Group the data by Product to get the sum of Revenue\n    product_revenue = df.groupby(\"Product\")[\"Revenue\"].sum().reset_index()\n\n    # Create the interactive bar chart\n    fig1 = px.bar(\n        product_revenue,\n        x=\"Product\",\n        y=\"Revenue\",\n        title=\"Total Revenue by Product\",\n        labels={\"Revenue\": \"Total Revenue (USD)\"},\n        color=\"Product\",\n    )\n\n    # In a real environment, you would use fig1.show()\n    # For this exercise, we'll save it to an HTML file.\n    fig1.write_html(\"product_revenue_bar_chart.html\")\n    print(\"Plot for Exercise 1 saved to 'product_revenue_bar_chart.html'\")\n\n    # --- Exercise 2: Interactive Revenue vs. Units Sold ---\n    print(\"\\n--- Solution to Exercise 2 ---\")\n\n    # Create the interactive scatter plot\n    fig2 = px.scatter(\n        df,\n        x=\"Units Sold\",\n        y=\"Revenue\",\n        color=\"Region\",\n        title=\"Revenue vs. Units Sold by Region\",\n        labels={\"Units Sold\": \"Number of Units Sold\", \"Revenue\": \"Total Revenue (USD)\"},\n        hover_data=[\"Product\"],  # Add Product to the hover tooltip\n    )\n\n    # Save the chart to an HTML file\n    fig2.write_html(\"revenue_scatterplot.html\")\n    print(\"Plot for Exercise 2 saved to 'revenue_scatterplot.html'\")\n\nelse:\n    print(\"\\nSkipping exercises as DataFrame could not be loaded.\")\n</code></pre>"},{"location":"lessons/day-30-web-scraping/","title":"\ud83d\udcd8 Day 30: Web Scraping - Extracting Data from the Web","text":"<p>Sometimes, the data you need isn't available in a clean CSV file or through an API. It's simply displayed on a website. Web scraping is the process of automatically downloading the HTML code of a web page and extracting useful information from it.</p> <p>This is an incredibly powerful tool for a business analyst, allowing you to gather competitive intelligence, track news sentiment, collect product prices, and much more.</p>"},{"location":"lessons/day-30-web-scraping/#working-offline","title":"\ud83d\udce6 Working Offline","text":"<p>If you do not have internet access, you can still explore the examples in this lesson. The folder includes a curated <code>presidents.csv</code> containing a snapshot of key columns\u2014number, name, party, term dates, and vice presidents\u2014for every U.S. president through Joe Biden. The exercise scripts will look for this local file first, so you can experiment with parsing and analysis even when the Wikipedia page is unavailable. When a connection is available you can still re-run the scraper to refresh the dataset, which will regenerate <code>presidents.json</code>. Git ignores these generated JSON files so your repository stays clean.</p> <p>A VERY IMPORTANT NOTE ON ETHICS AND LEGALITY:</p> <ul> <li>Check <code>robots.txt</code>: Always check a website's <code>robots.txt</code> file (e.g., <code>https://example.com/robots.txt</code>) to see which parts of the site you are allowed to scrape. Respect the rules.</li> <li>Be Gentle: Don't send too many requests in a short period. You could overwhelm the website's server, which is inconsiderate and may get your IP address blocked. Introduce delays between your requests.</li> <li>Identify Yourself: Set a user-agent in your request headers that identifies your script or bot.</li> <li>Public Data Only: Only scrape data that is publicly visible. Do not attempt to scrape information that is behind a login or a paywall.</li> </ul>"},{"location":"lessons/day-30-web-scraping/#the-web-scraping-toolkit","title":"The Web Scraping Toolkit","text":"<p>We will use two main libraries for web scraping:</p> <ol> <li><code>requests</code>: A simple and elegant library for making HTTP requests to download web pages.</li> <li><code>BeautifulSoup</code>: A library for parsing HTML and XML documents. It creates a parse tree from the page's source code that you can use to extract data.</li> </ol>"},{"location":"lessons/day-30-web-scraping/#the-scraping-process","title":"The Scraping Process","text":"<ol> <li>Inspect the Page: Use your web browser's \"Inspect\" or \"View Source\" tool to understand the HTML structure of the page you want to scrape. Find the HTML tags (e.g., <code>&lt;h1&gt;</code>, <code>&lt;p&gt;</code>, <code>&lt;table&gt;</code>, <code>&lt;div&gt;</code>) that contain the data you need. Look for unique <code>id</code> or <code>class</code> attributes on those tags.</li> <li>Download the HTML: Use the <code>requests.get(url)</code> function to download the page's HTML content.</li> <li>Create a \"Soup\": Pass the downloaded HTML to the <code>BeautifulSoup</code> constructor to create a parsable object.</li> <li>Find Your Data: Use BeautifulSoup's methods, like <code>find()</code> and <code>find_all()</code>, to locate the specific HTML tags containing your data.</li> <li>Extract the Text: Once you have the tags, use the <code>.get_text()</code> method to extract the clean text from them.</li> <li>Structure the Data: Organize your extracted data into a list or, even better, a Pandas DataFrame.</li> </ol> <pre><code>import requests\nfrom bs4 import BeautifulSoup\n\nurl = 'http://example.com' # A simple example page\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Find the first &lt;h1&gt; tag\nheader = soup.find('h1').get_text()\n\n# Find all &lt;p&gt; (paragraph) tags\nparagraphs = soup.find_all('p')\nfirst_paragraph_text = paragraphs[0].get_text()\n</code></pre>"},{"location":"lessons/day-30-web-scraping/#profiling-the-scraper","title":"\ud83d\udd2c Profiling the Scraper","text":"<p>Profiling helps you spot whether networking or HTML parsing is the bottleneck. Two helper commands wire into the shared profiler:</p> <pre><code>python Day_30_Web_Scraping/profile_web_scraping.py --mode cprofile\npython Day_30_Web_Scraping/profile_web_scraping.py --mode timeit --local-html Day_30_Web_Scraping/books_sample.html --repeat 5 --number 3\n</code></pre> <p>The <code>cProfile</code> output shows that almost all time is spent inside <code>requests.Session.get</code>\u2014network I/O dominates the runtime, so batching requests or caching responses offers the biggest win.\u3010ad83b3\u2020L1-L29\u3011 For deterministic timing, use the saved <code>books_sample.html</code> page (refresh it with <code>curl http://books.toscrape.com/ -o Day_30_Web_Scraping/books_sample.html</code>). Parsing that local file takes ~0.03 seconds per iteration across five repeats, letting you focus on BeautifulSoup performance without hitting the network.\u3010de293a\u2020L1-L7\u3011 Reusing a single <code>requests.Session</code> and avoiding repeated downloads can dramatically cut the cost when scraping multiple pages.</p>"},{"location":"lessons/day-30-web-scraping/#exercises-day-30","title":"\ud83d\udcbb Exercises: Day 30","text":"<p>For these exercises, we will scrape the website <code>http://books.toscrape.com/</code>, a site specifically designed for scraping practice.</p> <ol> <li> <p>Scrape Book Titles:</p> </li> <li> <p>Visit <code>http://books.toscrape.com/</code>.</p> </li> <li>Write a script that downloads the page content.</li> <li>Create a BeautifulSoup object from the content.</li> <li>Find all the book titles on the first page. (Hint: Inspect the page to see what tag the titles are in. They are inside <code>&lt;h3&gt;</code> tags, within an <code>&lt;a&gt;</code> tag).</li> <li> <p>Create a list of all the book titles and print it.</p> </li> <li> <p>Scrape Book Prices:</p> </li> <li> <p>On the same page, find all the book prices. (Hint: They are in <code>p</code> tags with the class <code>price_color</code>).</p> </li> <li>Extract the text of the prices (e.g., \"\u00a351.77\").</li> <li> <p>Create a list of all the prices and print it.</p> </li> <li> <p>Create a DataFrame:</p> </li> <li> <p>Combine your work from the previous two exercises.</p> </li> <li>Create a script that scrapes both the titles and the prices.</li> <li>Store the results in a Pandas DataFrame with two columns: \"Title\" and \"Price\".</li> <li>Print the first 5 rows of your new DataFrame using <code>.head()</code>.</li> </ol> <p>\ud83c\udf89 Great job! Web scraping is a powerful skill that opens up a vast new source of data for your analyses. While it can be complex, mastering the basics of <code>requests</code> and <code>BeautifulSoup</code> is a huge step forward.</p>"},{"location":"lessons/day-30-web-scraping/#additional-materials","title":"Additional Materials","text":"<ul> <li>presidents.ipynb</li> <li>profile_web_scraping.ipynb</li> <li>solutions.ipynb</li> <li>web_scraping.ipynb</li> <li>web_scraping_bu.ipynb</li> </ul> presidents.py <p>View on GitHub</p> presidents.py<pre><code>\"\"\"Day 30: Web Scraping Presidents Data.\n\nThis script prefers locally curated data so learners can work offline, falls back\nto a lightweight mock dataset, and only reaches out to Wikipedia when it needs to\nrefresh the snapshot.\n\"\"\"\n\nimport csv\nimport json\nfrom pathlib import Path\nfrom typing import Optional\n\nimport pandas as pd\nimport requests\n\nMOCK_PRESIDENTS_DATA = [\n    {\n        \"number\": \"1\",\n        \"president\": \"George Washington\",\n        \"term_start\": \"1789-04-30\",\n        \"term_end\": \"1797-03-04\",\n        \"party\": \"Independent\",\n        \"vice_president\": \"John Adams\",\n    },\n    {\n        \"number\": \"2\",\n        \"president\": \"John Adams\",\n        \"term_start\": \"1797-03-04\",\n        \"term_end\": \"1801-03-04\",\n        \"party\": \"Federalist\",\n        \"vice_president\": \"Thomas Jefferson\",\n    },\n    {\n        \"number\": \"3\",\n        \"president\": \"Thomas Jefferson\",\n        \"term_start\": \"1801-03-04\",\n        \"term_end\": \"1809-03-04\",\n        \"party\": \"Democratic-Republican\",\n        \"vice_president\": \"Aaron Burr; George Clinton\",\n    },\n    {\n        \"number\": \"4\",\n        \"president\": \"James Madison\",\n        \"term_start\": \"1809-03-04\",\n        \"term_end\": \"1817-03-04\",\n        \"party\": \"Democratic-Republican\",\n        \"vice_president\": \"George Clinton; Elbridge Gerry\",\n    },\n    {\n        \"number\": \"5\",\n        \"president\": \"James Monroe\",\n        \"term_start\": \"1817-03-04\",\n        \"term_end\": \"1825-03-04\",\n        \"party\": \"Democratic-Republican\",\n        \"vice_president\": \"Daniel D. Tompkins\",\n    },\n    {\n        \"number\": \"6\",\n        \"president\": \"John Quincy Adams\",\n        \"term_start\": \"1825-03-04\",\n        \"term_end\": \"1829-03-04\",\n        \"party\": \"Democratic-Republican\",\n        \"vice_president\": \"John C. Calhoun\",\n    },\n    {\n        \"number\": \"7\",\n        \"president\": \"Andrew Jackson\",\n        \"term_start\": \"1829-03-04\",\n        \"term_end\": \"1837-03-04\",\n        \"party\": \"Democratic\",\n        \"vice_president\": \"John C. Calhoun; Martin Van Buren\",\n    },\n    {\n        \"number\": \"8\",\n        \"president\": \"Martin Van Buren\",\n        \"term_start\": \"1837-03-04\",\n        \"term_end\": \"1841-03-04\",\n        \"party\": \"Democratic\",\n        \"vice_president\": \"Richard Mentor Johnson\",\n    },\n    {\n        \"number\": \"9\",\n        \"president\": \"William Henry Harrison\",\n        \"term_start\": \"1841-03-04\",\n        \"term_end\": \"1841-04-04\",\n        \"party\": \"Whig\",\n        \"vice_president\": \"John Tyler\",\n    },\n    {\n        \"number\": \"10\",\n        \"president\": \"John Tyler\",\n        \"term_start\": \"1841-04-04\",\n        \"term_end\": \"1845-03-04\",\n        \"party\": \"Whig (expelled)\",\n        \"vice_president\": \"None\",\n    },\n]\n\n\ndef scrape_presidents_data(url: str) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Scrapes the list of US presidents from a Wikipedia page.\n\n    Args:\n        url (str): The URL of the Wikipedia page.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the presidents' data.\n    \"\"\"\n    try:\n        print(f\"\ud83c\udf10 Connecting to {url}...\")\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        }\n        response = requests.get(url, headers=headers, timeout=30)\n        response.raise_for_status()\n\n        print(\"\ud83d\udcca Parsing HTML tables...\")\n        # Use response.text instead of response.content for pd.read_html\n        tables = pd.read_html(response.text)\n\n        if not tables:\n            print(\"\u274c No tables found on the page\")\n            return None\n\n        print(f\"\u2705 Found {len(tables)} tables on the page\")\n\n        # The first table is typically the one we want\n        presidents_df = tables[0].copy()\n\n        print(f\"\ud83d\udccb Original data shape: {presidents_df.shape}\")\n\n        # Clean up the data\n        # Drop the last row if it appears to be a footnote (common in Wikipedia tables)\n        if len(presidents_df) &gt; 1:\n            presidents_df = presidents_df.iloc[:-1].copy()\n\n        # Replace empty strings with NaN\n        presidents_df.replace(\"\", float(\"NaN\"), inplace=True)\n        presidents_df.replace(\n            \"\u2014\", float(\"NaN\"), inplace=True\n        )  # Common dash used in Wikipedia\n\n        # Drop columns that are all NaN\n        presidents_df.dropna(how=\"all\", axis=1, inplace=True)\n\n        print(f\"\ud83d\udccb Cleaned data shape: {presidents_df.shape}\")\n        print(f\"\ud83d\udccb Columns: {list(presidents_df.columns)}\")\n\n        return presidents_df\n    except requests.exceptions.Timeout:\n        print(\"\u274c Request timed out. The server might be slow.\")\n        return None\n    except requests.exceptions.ConnectionError:\n        print(\"\u274c Connection error. Please check your internet connection.\")\n        return None\n    except requests.exceptions.HTTPError as e:\n        print(f\"\u274c HTTP error occurred: {e}\")\n        return None\n    except requests.exceptions.RequestException as e:\n        print(f\"\u274c Error downloading the page: {e}\")\n        return None\n    except (IndexError, KeyError) as e:\n        print(f\"\u274c Error parsing the table: {e}\")\n        print(\"\ud83d\udca1 The website structure may have changed.\")\n        return None\n    except Exception as e:\n        print(f\"\u274c Unexpected error occurred: {e}\")\n        return None\n\n\ndef convert_csv_to_json(csv_file_path: str, json_file_path: str) -&gt; None:\n    \"\"\"\n    Converts a CSV file to a JSON file.\n\n    The JSON file will have the first column of the CSV as keys.\n\n    Args:\n        csv_file_path (str): The path to the input CSV file.\n        json_file_path (str): The path to the output JSON file.\n    \"\"\"\n    data = {}\n    try:\n        print(f\"\ud83d\udcc4 Converting {csv_file_path} to {json_file_path}...\")\n        with open(csv_file_path, encoding=\"utf-8\") as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n\n            # Check if fieldnames exist and get the first column\n            if not csv_reader.fieldnames:\n                print(\"\u274c No column headers found in CSV file\")\n                return\n\n            key_column = csv_reader.fieldnames[0]\n            print(f\"\ud83d\udcca Using '{key_column}' as the key column\")\n\n            row_count = 0\n            for row in csv_reader:\n                key = row[key_column]\n                if key:  # Only add rows with non-empty keys\n                    data[key] = row\n                    row_count += 1\n\n            print(f\"\u2705 Processed {row_count} records\")\n\n        with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n            json.dump(data, json_file, indent=4, ensure_ascii=False)\n\n        print(f\"\u2705 Successfully converted to JSON: {json_file_path}\")\n    except FileNotFoundError:\n        print(f\"\u274c Error: The file {csv_file_path} was not found.\")\n    except PermissionError:\n        print(\"\u274c Error: Permission denied when accessing files.\")\n    except Exception as e:\n        print(f\"\u274c An error occurred during conversion: {e}\")\n\n\ndef save_mock_json(json_file_path: Path) -&gt; None:\n    \"\"\"Persist the lightweight mock dataset to a JSON file.\"\"\"\n\n    print(\"\ud83d\udcc4 Local CSV not found. Using built-in mock dataset.\")\n    data = {item[\"number\"]: item for item in MOCK_PRESIDENTS_DATA}\n    with json_file_path.open(\"w\", encoding=\"utf-8\") as json_file:\n        json.dump(data, json_file, indent=4, ensure_ascii=False)\n    print(f\"\u2705 Mock JSON file created: {json_file_path}\")\n\n\ndef main():\n    \"\"\"\n    Main function to scrape, process, and save the presidents' data.\n    \"\"\"\n    print(\"\ud83d\udd78\ufe0f  Day 30: Web Scraping Presidents Data\")\n    print(\"\ud83c\udfdb\ufe0f  Scraping US Presidents data from Wikipedia\")\n    print(\"=\" * 50)\n\n    # URL of the Wikipedia page with the list of US presidents\n    presidents_url = (\n        \"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\"\n    )\n\n    # Define file paths relative to the script's location\n    # This makes the script more portable\n    base_dir = Path(__file__).resolve().parent\n    csv_path = base_dir / \"presidents.csv\"\n    json_path = base_dir / \"presidents.json\"\n    # The JSON outputs are ignored by git so learners can regenerate them\n    # locally without creating untracked files.\n\n    print(f\"\ud83d\udcc1 Output files will be saved to: {base_dir}\")\n\n    # Scrape the data\n    if csv_path.exists():\n        print(\"\ud83d\udcc4 Found curated CSV. Converting to JSON without scraping.\")\n        convert_csv_to_json(str(csv_path), str(json_path))\n        return\n\n    # If the curated CSV is missing, provide learners with the mock dataset first.\n    save_mock_json(json_path)\n\n    presidents_df = scrape_presidents_data(presidents_url)\n\n    if presidents_df is not None:\n        try:\n            # Save the DataFrame to a temporary CSV file, without the index\n            temp_csv_path = base_dir / \"presidents_download.csv\"\n            print(\"\ud83d\udcbe Saving scraped data to a temporary CSV...\")\n            presidents_df.to_csv(temp_csv_path, index=False, encoding=\"utf-8\")\n            print(f\"\u2705 CSV file created: {temp_csv_path}\")\n\n            # Convert the CSV to JSON\n            convert_csv_to_json(str(temp_csv_path), str(json_path))\n\n            # Remove the temporary CSV file\n            try:\n                temp_csv_path.unlink()\n                print(\"\ud83e\uddf9 Removed temporary CSV file\")\n            except OSError as e:\n                print(\n                    f\"\u26a0\ufe0f  Warning: Could not remove temporary file {temp_csv_path}: {e}\"\n                )\n\n            # Verify the JSON file was created and show some info\n            if json_path.exists():\n                file_size = json_path.stat().st_size\n                print(f\"\\n\ud83c\udf89 Success! Created '{json_path}' ({file_size:,} bytes)\")\n\n                # Show a preview of the data\n                try:\n                    with json_path.open(\"r\", encoding=\"utf-8\") as f:\n                        data = json.load(f)\n                        print(f\"\ud83d\udcca Total presidents in dataset: {len(data)}\")\n                        if data:\n                            first_key = next(iter(data))\n                            print(\n                                f\"\ud83d\udccb Sample entry keys: {list(data[first_key].keys())[:5]}...\"\n                            )\n                except Exception as e:\n                    print(f\"\u26a0\ufe0f  Could not preview JSON data: {e}\")\n            else:\n                print(\"\u274c JSON file was not created successfully\")\n\n        except Exception as e:\n            print(f\"\u274c Error in main processing: {e}\")\n    else:\n        print(\"\u274c Failed to scrape presidents data.\")\n        print(\"\ud83d\udca1 This could be due to:\")\n        print(\"   \u2022 Network connectivity issues\")\n        print(\"   \u2022 Wikipedia page structure changes\")\n        print(\"   \u2022 Rate limiting on Wikipedia\")\n        print(\"   \u2022 Server blocking the request\")\n        print(\"   \u2022 Temporary website unavailability\")\n        print(\"\ud83d\udce6 Continuing with the mock dataset so you can keep practicing offline.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> profile_web_scraping.py <p>View on GitHub</p> profile_web_scraping.py<pre><code>\"\"\"Profile the book scraping workflow used in the web scraping lesson.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport sys\nfrom pathlib import Path\nfrom typing import Callable\n\ntry:\n    from mypackage.profiling import print_report, profile_callable\nexcept ImportError:\n    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n    if str(PROJECT_ROOT) not in sys.path:\n        sys.path.append(str(PROJECT_ROOT))\n    from mypackage.profiling import print_report, profile_callable\n\ntry:  # pragma: no cover - runtime guard for direct execution\n    from .web_scraping import URL, process_book_data, scrape_books\nexcept ImportError:  # pragma: no cover - allows ``python profile_web_scraping.py``\n    CURRENT_DIR = Path(__file__).resolve().parent\n    if str(CURRENT_DIR) not in sys.path:\n        sys.path.append(str(CURRENT_DIR))\n    from web_scraping import URL, process_book_data, scrape_books  # type: ignore\n\n\ndef build_pipeline(url: str, html_path: Path | None) -&gt; Callable[[], None]:\n    \"\"\"Return a callable that performs the scraping workflow.\"\"\"\n\n    if html_path is not None:\n        html_bytes = html_path.read_bytes()\n\n        def pipeline() -&gt; None:\n            process_book_data(html_bytes)\n\n    else:\n\n        def pipeline() -&gt; None:\n            scrape_books(url)\n\n    return pipeline\n\n\ndef main() -&gt; None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        \"--mode\",\n        choices=(\"cprofile\", \"timeit\"),\n        default=\"cprofile\",\n        help=\"Profiling backend to use (default: cprofile)\",\n    )\n    parser.add_argument(\n        \"--url\",\n        default=URL,\n        help=\"Target URL to scrape when not using --local-html\",\n    )\n    parser.add_argument(\n        \"--local-html\",\n        type=Path,\n        help=\"Optional path to a saved HTML page for offline profiling\",\n    )\n    parser.add_argument(\n        \"--repeat\",\n        type=int,\n        default=5,\n        help=\"Number of timing repeats when --mode=timeit\",\n    )\n    parser.add_argument(\n        \"--number\",\n        type=int,\n        default=1,\n        help=\"Number of calls per repeat when --mode=timeit\",\n    )\n    args = parser.parse_args()\n\n    if args.mode == \"timeit\" and args.local_html is None:\n        raise SystemExit(\n            \"--mode=timeit requires --local-html to avoid repeated network calls\"\n        )\n\n    pipeline = build_pipeline(url=args.url, html_path=args.local_html)\n    profile_report, timing_report = profile_callable(\n        pipeline,\n        mode=args.mode,\n        repeat=args.repeat,\n        number=args.number,\n    )\n    print_report(profile_report=profile_report, timing_report=timing_report)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 30: Solutions to Exercises\n\"\"\"\n\nimport bs4  # Add this import for type checking\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n\nURL = \"http://books.toscrape.com/\"\nheaders = {\"User-Agent\": \"Mozilla/5.0\"}\n\ntry:\n    response = requests.get(URL, headers=headers)\n    response.raise_for_status()\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    print(\"Successfully connected to books.toscrape.com\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"Could not connect to the website: {e}\")\n    soup = None\n\nif soup:\n    # --- Exercise 1: Scrape Book Titles ---\n    print(\"\\n--- Solution to Exercise 1 ---\")\n\n    # Find all &lt;h3&gt; tags, then find the &lt;a&gt; tag inside, and get its 'title' attribute\n    title_tags = soup.find_all(\"h3\")\n    titles = []\n    for tag in title_tags:\n        if isinstance(tag, bs4.element.Tag):\n            a_tag = tag.find(\"a\")\n            if isinstance(a_tag, bs4.element.Tag) and \"title\" in a_tag.attrs:\n                titles.append(a_tag.get(\"title\"))\n\n    print(\"Found the following titles:\")\n    # Print first 5 for brevity\n    for title in titles[:5]:\n        print(f\"- {title}\")\n    print(\"-\" * 20)\n\n    # --- Exercise 2: Scrape Book Prices ---\n    print(\"\\n--- Solution to Exercise 2 ---\")\n\n    # Find all &lt;p&gt; tags that have the class 'price_color'\n    price_tags = soup.find_all(\"p\", class_=\"price_color\")\n    prices = [tag.get_text() for tag in price_tags]\n\n    print(\"Found the following prices:\")\n    # Print first 5 for brevity\n    for price in prices[:5]:\n        print(f\"- {price}\")\n    print(\"-\" * 20)\n\n    # --- Exercise 3: Create a DataFrame ---\n    print(\"\\n--- Solution to Exercise 3 ---\")\n\n    # Check if we have the same number of titles and prices\n    if len(titles) == len(prices):\n        book_df = pd.DataFrame({\"Title\": titles, \"Price\": prices})\n        print(\"Created DataFrame from scraped data:\")\n        print(book_df.head())\n    else:\n        print(\n            \"Mismatch between number of titles and prices found. Cannot create DataFrame.\"\n        )\n    print(\"-\" * 20)\n\nelse:\n    print(\"\\nSkipping exercises as the website could not be scraped.\")\n</code></pre> web_scraping.py <p>View on GitHub</p> web_scraping.py<pre><code>\"\"\"\nDay 30: Web Scraping in Practice\n\nThis script demonstrates the fundamentals of web scraping by\nextracting book titles and prices from a practice website.\n\nThis educational example shows how to:\n- Make HTTP requests with proper headers\n- Parse HTML content with BeautifulSoup\n- Handle errors gracefully\n- Extract and clean data\n- Structure data in pandas DataFrame\n\nAuthor: 50 Days of Python Course\nPurpose: Educational example for MBA students\n\"\"\"\n\nimport time\nfrom typing import Any, Dict, Optional, Tuple\n\nimport bs4\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n\n# The URL of the website we want to scrape\n# This site is specifically designed for scraping practice.\nURL = \"http://books.toscrape.com/\"\n\n\nclass ScrapingError(Exception):\n    \"\"\"Custom exception for scraping errors.\"\"\"\n\n\ndef scrape_books(\n    url: str, session: Optional[requests.Session] = None\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n    \"\"\"\n    Scrape book data from the given URL.\n\n    Args:\n        url (str): The URL to scrape\n\n    Returns:\n        Tuple containing the raw scraped DataFrame, the cleaned DataFrame, and\n        a dictionary of summary statistics.\n    \"\"\"\n    # --- 1. Download the HTML Content ---\n    # Use requests.get() to download the page.\n    # It's good practice to include a 'User-Agent' header to identify your script.\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n    }\n\n    session = session or requests.Session()\n\n    try:\n        response = session.get(url, headers=headers, timeout=10)\n        response.raise_for_status()\n    except requests.exceptions.Timeout:\n        raise\n    except requests.exceptions.ConnectionError:\n        raise\n    except requests.exceptions.HTTPError:\n        raise\n    except requests.exceptions.RequestException as exc:\n        raise ScrapingError(\"Error downloading the page\") from exc\n\n    # If we get here, the request was successful\n    return process_book_data(response.content)\n\n\ndef process_book_data(\n    html_content: bytes,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n    \"\"\"\n    Process the HTML response and extract book data.\n\n    Args:\n        response: The HTTP response object\n\n    Returns:\n        Tuple containing the raw scraped DataFrame, the cleaned DataFrame, and\n        a dictionary of summary statistics.\n    \"\"\"\n    # --- 2. Create a BeautifulSoup Object ---\n    # This object parses the HTML content and makes it searchable.\n    soup = BeautifulSoup(html_content, \"html.parser\")\n\n    # --- 3. Find and Extract Data ---\n    # We inspected the page and found that book information is within &lt;article&gt; tags with the class 'product_pod'\n    books = soup.find_all(\"article\", class_=\"product_pod\")\n\n    if not books:\n        raise ValueError(\"No books found in the provided HTML content\")\n\n    titles = []\n    prices = []\n\n    # Loop through each book found on the page\n    for book in books:\n        # Type check to ensure book is a Tag\n        if not isinstance(book, bs4.element.Tag):\n            continue\n\n        # The title is in an 'a' tag within an 'h3' tag.\n        # We access the 'title' attribute of the 'a' tag.\n        h3_tag = book.find(\"h3\")\n        if isinstance(h3_tag, bs4.element.Tag):\n            a_tag = h3_tag.find(\"a\")\n            if isinstance(a_tag, bs4.element.Tag):\n                title = a_tag.get(\"title\")\n                titles.append(str(title) if title else \"N/A\")\n            else:\n                titles.append(\"N/A\")\n        else:\n            titles.append(\"N/A\")\n\n        # The price is in a 'p' tag with the class 'price_color'\n        price_tag = book.find(\"p\", attrs={\"class\": \"price_color\"})\n        if isinstance(price_tag, bs4.element.Tag):\n            price_text = price_tag.get_text(strip=True)\n            prices.append(price_text)\n        else:\n            prices.append(\"N/A\")\n\n    # --- 4. Structure the Data in a DataFrame ---\n    if not titles or not prices or len(titles) != len(prices):\n        raise ValueError(\"Mismatch between titles and prices in the HTML content\")\n\n    book_data = pd.DataFrame({\"Title\": titles, \"Price\": prices})\n\n    # --- 5. Data Cleaning (Bonus) ---\n    clean_data = book_data.copy()\n    clean_data[\"Price_Float\"] = pd.to_numeric(\n        clean_data[\"Price\"].str.replace(\"\u00a3\", \"\", regex=False), errors=\"coerce\"\n    )\n    clean_data = clean_data.dropna(subset=[\"Price_Float\"]).copy()\n\n    if clean_data.empty:\n        return book_data, clean_data, {}\n\n    # --- 6. Basic Analysis ---\n    price_series = clean_data[\"Price_Float\"]\n    analysis: Dict[str, Any] = {\n        \"average_price\": float(price_series.mean()),\n        \"min_price\": float(price_series.min()),\n        \"max_price\": float(price_series.max()),\n        \"count\": int(len(clean_data)),\n    }\n\n    most_expensive = clean_data.loc[price_series.idxmax()]\n    cheapest = clean_data.loc[price_series.idxmin()]\n\n    analysis[\"most_expensive_title\"] = most_expensive[\"Title\"]\n    analysis[\"most_expensive_price\"] = most_expensive[\"Price\"]\n    analysis[\"cheapest_title\"] = cheapest[\"Title\"]\n    analysis[\"cheapest_price\"] = cheapest[\"Price\"]\n\n    return book_data, clean_data, analysis\n\n\ndef main():\n    \"\"\"\n    Main function to demonstrate web scraping workflow.\n    \"\"\"\n    print(\"\ud83d\udd78\ufe0f  Day 30: Web Scraping Demonstration\")\n    print(\"\ud83d\udcda Scraping book data from books.toscrape.com\")\n    print(\"=\" * 50)\n\n    # Add a small delay to be respectful to the server\n    print(\"\u23f3 Starting scraping process...\")\n    time.sleep(1)\n\n    # Execute the scraping\n    try:\n        print(f\"\ud83c\udf10 Connecting to {URL}...\")\n        raw_df, clean_df, analysis = scrape_books(URL)\n    except requests.exceptions.Timeout:\n        print(\"\u274c Request timed out. The server might be slow or unresponsive.\")\n        return\n    except requests.exceptions.ConnectionError:\n        print(\"\u274c Connection error. Please check your internet connection.\")\n        return\n    except requests.exceptions.HTTPError as exc:\n        print(f\"\u274c HTTP error occurred: {exc}\")\n        return\n    except ScrapingError as exc:\n        print(f\"\u274c {exc}\")\n        print(\"\ud83d\udca1 This could be due to:\")\n        print(\"   \u2022 Network connectivity issues\")\n        print(\"   \u2022 Website being temporarily unavailable\")\n        print(\"   \u2022 Blocked by website's anti-bot protection\")\n        print(\"   \u2022 URL has changed or is incorrect\")\n        return\n    except ValueError as exc:\n        print(f\"\u274c {exc}\")\n        print(\"\ud83d\udca1 The website structure may have changed. Try updating the parser.\")\n        return\n\n    print(\"\u2705 Successfully downloaded the content!\")\n    print(f\"\ud83d\udcca Total books scraped: {len(raw_df)}\")\n\n    if clean_df.empty:\n        print(\"\u26a0\ufe0f  No valid price data found for analysis.\")\n        return\n\n    print(\"\\n--- Sample of Scraped Book Data ---\")\n    print(raw_df.head(10))\n\n    print(\"\\n--- Cleaned Price Data ---\")\n    print(clean_df.head(10))\n\n    print(\"\\n\ud83d\udcc8 Basic Price Analysis:\")\n    print(f\"   Average price: \u00a3{analysis['average_price']:.2f}\")\n    print(f\"   Minimum price: \u00a3{analysis['min_price']:.2f}\")\n    print(f\"   Maximum price: \u00a3{analysis['max_price']:.2f}\")\n    print(f\"   Number of books: {analysis['count']}\")\n    print(\n        f\"\ud83d\udcb0 Most expensive: '{analysis['most_expensive_title']}' - {analysis['most_expensive_price']}\"\n    )\n    print(f\"\ud83d\udcb8 Cheapest: '{analysis['cheapest_title']}' - {analysis['cheapest_price']}\")\n\n    print(\"\\n\ud83d\udca1 Next steps you could take:\")\n    print(\"   \u2022 Save data to CSV: clean_df.to_csv('books.csv', index=False)\")\n    print(\"   \u2022 Filter books by price range\")\n    print(\"   \u2022 Scrape additional pages for more data\")\n    print(\"   \u2022 Add more data fields (ratings, availability, etc.)\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> web_scraping_bu.py <p>View on GitHub</p> web_scraping_bu.py<pre><code>import json\nfrom pathlib import Path\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl1 = \"https://www.bu.edu/president/boston-university-facts-stats/\"\nresponse1 = requests.get(url1)\ncontent1 = response1.content\nsoup1 = BeautifulSoup(content1, \"html.parser\")\ntables = soup1.find_all(\"div\", class_=\"facts-wrapper\")\n\nlist_of_tables = []\n\nfor i in tables:\n    keys = []\n    values = []\n    temp_dict = {}\n    i = str(i)\n    category = i[i.find(\"&lt;h5&gt;\") + 4 : i.find(\"&lt;/h5&gt;\")]\n    temp_dict[\"Category\"] = category\n    all_key_start_indexes = [x + 7 for x in range(len(i)) if i.startswith('\"text\"&gt;', x)]\n    all_key_end_indexes = [x for x in range(len(i)) if i.startswith(\"&lt;/p&gt;\", x)]\n\n    for start_index, end_index in zip(all_key_start_indexes, all_key_end_indexes):\n        keys.append(i[start_index:end_index])\n\n    all_values_start_indexes = []\n    for v in range(len(i)):\n        if i.startswith('value\"&gt;', v):\n            all_values_start_indexes.append(v + 7)\n        if i.startswith('value-text\"&gt;', v):\n            all_values_start_indexes.append(v + 12)\n    all_values_end_indexes = [x for x in range(len(i)) if i.startswith(\"&lt;/span&gt;\", x)]\n\n    for m in range(len(all_values_end_indexes)):\n        values.append(i[all_values_start_indexes[m] : all_values_end_indexes[m]])\n\n    for r in range(len(keys)):\n        temp_dict[keys[r]] = values[r]\n    list_of_tables.append(temp_dict)\n\noutput_path = Path(__file__).resolve().parent / \"scraped_exercise_1.json\"\n\nwith output_path.open(\"w\", encoding=\"utf-8\") as fp:\n    json.dump(list_of_tables, fp)\n</code></pre>"},{"location":"lessons/day-31-databases/","title":"\ud83d\udcd8 Day 31: Working with Databases in Python","text":"<p>While CSV files are great for smaller datasets, most real-world business data is stored in databases. Databases are systems designed for storing, managing, and retrieving large amounts of structured data efficiently and safely.</p> <p>As a data analyst, you will almost certainly need to pull data from a database. Learning to do this with Python is a critical skill.</p>"},{"location":"lessons/day-31-databases/#why-databases","title":"Why Databases?","text":"<ul> <li>Persistence: The data is stored safely on disk and doesn't disappear when you turn off your computer.</li> <li>Scalability: Databases are designed to handle enormous amounts of data\u2014far more than can fit in a spreadsheet.</li> <li>Querying: Databases use a powerful language called SQL (Structured Query Language) to let you select, filter, and aggregate the exact data you need.</li> <li>Concurrency: They allow multiple users to access and modify the data at the same time without conflicts.</li> </ul>"},{"location":"lessons/day-31-databases/#introduction-to-sql","title":"Introduction to SQL","text":"<p>SQL is the standard language for relational databases. Here are a few of the most important commands:</p> <ul> <li><code>SELECT [column1], [column2]</code>: Specifies the columns you want.</li> <li><code>FROM [table]</code>: Specifies the table you're getting data from.</li> <li><code>WHERE [condition]</code>: Filters the rows based on a condition.</li> <li><code>GROUP BY [column]</code>: Groups rows that have the same values into summary rows.</li> <li><code>JOIN</code>: Combines rows from two or more tables based on a related column.</li> </ul>"},{"location":"lessons/day-31-databases/#pythons-sqlite3-module","title":"Python's <code>sqlite3</code> Module","text":"<p>Python has a built-in module called <code>sqlite3</code> that allows you to work with a lightweight, serverless database called SQLite. The entire database is just a single file on your computer, making it perfect for learning and for smaller applications.</p> <p>The standard workflow is:</p> <ol> <li>Connect: Create a connection object to the database file.</li> <li>Cursor: Create a cursor object, which allows you to execute SQL commands.</li> <li>Execute: Run your SQL query.</li> <li>Fetch: Retrieve the results from the cursor.</li> <li>Close: Close the connection.</li> </ol> <pre><code>import sqlite3\n\n# 1. Connect to the database (creates the file if it doesn't exist)\nconn = sqlite3.connect('my_database.db')\n# 2. Create a cursor\ncur = conn.cursor()\n# 3. Execute a query\ncur.execute(\"SELECT * FROM employees WHERE department = 'Sales';\")\n# 4. Fetch the results\nsales_employees = cur.fetchall()\n# 5. Close the connection\nconn.close()\n</code></pre>"},{"location":"lessons/day-31-databases/#the-pandas-way-read_sql_query","title":"The Pandas Way: <code>read_sql_query</code>","text":"<p>The <code>sqlite3</code> workflow is a bit verbose. For data analysis, there's a much easier way. Pandas can interact with databases directly and load the results of a query into a DataFrame.</p> <p>The <code>pd.read_sql_query()</code> function takes an SQL query and a connection object and returns a full DataFrame. This is the method you will use most of the time.</p> <pre><code>import pandas as pd\nimport sqlite3\n\nconn = sqlite3.connect('my_database.db')\nsql_query = \"SELECT * FROM employees WHERE department = 'Sales';\"\n\nsales_df = pd.read_sql_query(sql_query, conn)\n\nconn.close()\n# sales_df is now a Pandas DataFrame ready for analysis!\n</code></pre>"},{"location":"lessons/day-31-databases/#exercises-day-31","title":"\ud83d\udcbb Exercises: Day 31","text":"<ol> <li> <p>Create and Populate a Database:</p> </li> <li> <p>Write a script that creates a new SQLite database called <code>company.db</code>.</p> </li> <li>Create a table named <code>products</code> with three columns: <code>product_id</code> (INTEGER), <code>name</code> (TEXT), and <code>price</code> (REAL).</li> <li> <p>Insert 3-4 sample products into the table.</p> </li> <li> <p>Query with <code>sqlite3</code>:</p> </li> <li> <p>Write a script that connects to the <code>company.db</code> you created.</p> </li> <li>Execute a SQL query to select all products with a price greater than $100.</li> <li> <p>Fetch all the results and loop through them, printing each one.</p> </li> <li> <p>Query with Pandas:</p> </li> <li> <p>Write a script that connects to the <code>company.db</code>.</p> </li> <li>Use <code>pd.read_sql_query()</code> to select all records from the <code>products</code> table and load them into a DataFrame.</li> <li>Print the DataFrame.</li> </ol> <p>\ud83c\udf89 Excellent! You can now connect to and retrieve data from a database, the primary source of truth for most businesses. Knowing how to pull data directly into a Pandas DataFrame is a workflow you will use constantly as a data analyst.</p>"},{"location":"lessons/day-31-databases/#additional-materials","title":"Additional Materials","text":"<ul> <li>databases.ipynb</li> <li>databases_smoke_test.ipynb</li> <li>solutions.ipynb</li> </ul> databases.py <p>View on GitHub</p> databases.py<pre><code>\"\"\"Utilities for working with the Day 31 employee database.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport sqlite3\nfrom contextlib import contextmanager\nfrom typing import Iterator, List, Sequence, Tuple, Union\n\nimport pandas as pd\n\nDB_FILE = \"company_data.db\"\n\nEmployeeRecord = Tuple[int, str, str, float]\nSalaryResult = List[Tuple[str, float]]\nDatabaseLike = Union[str, os.PathLike[str], sqlite3.Connection]\n\nEMPLOYEE_ROWS: Sequence[EmployeeRecord] = (\n    (101, \"Alice\", \"Sales\", 80_000),\n    (102, \"Bob\", \"Engineering\", 120_000),\n    (103, \"Charlie\", \"Sales\", 85_000),\n    (104, \"Diana\", \"HR\", 70_000),\n    (105, \"Eve\", \"Engineering\", 130_000),\n)\n\n\n@contextmanager\ndef _get_connection(database: DatabaseLike) -&gt; Iterator[sqlite3.Connection]:\n    \"\"\"Yield a SQLite connection for a file path or existing connection.\"\"\"\n\n    if isinstance(database, sqlite3.Connection):\n        yield database\n        return\n\n    connection = sqlite3.connect(os.fspath(database))\n    try:\n        yield connection\n    finally:\n        connection.close()\n\n\ndef initialize_employee_db(database: DatabaseLike) -&gt; None:\n    \"\"\"Create the employees table and populate it with sample rows.\"\"\"\n\n    with _get_connection(database) as connection:\n        cursor = connection.cursor()\n        cursor.execute(\"DROP TABLE IF EXISTS employees\")\n        cursor.execute(\n            \"\"\"\n            CREATE TABLE employees (\n                employee_id INTEGER PRIMARY KEY,\n                name TEXT NOT NULL,\n                department TEXT NOT NULL,\n                salary REAL NOT NULL\n            )\n            \"\"\"\n        )\n        cursor.executemany(\n            \"INSERT INTO employees VALUES (?, ?, ?, ?)\",\n            EMPLOYEE_ROWS,\n        )\n        connection.commit()\n\n\ndef fetch_department_salaries(database: DatabaseLike, department: str) -&gt; SalaryResult:\n    \"\"\"Return ``(name, salary)`` pairs for the requested department.\"\"\"\n\n    query = \"\"\"\n        SELECT name, salary\n        FROM employees\n        WHERE department = ?\n        ORDER BY salary\n    \"\"\"\n    with _get_connection(database) as connection:\n        cursor = connection.execute(query, (department,))\n        return [(row[0], float(row[1])) for row in cursor.fetchall()]\n\n\ndef fetch_department_dataframe(database: DatabaseLike, department: str) -&gt; pd.DataFrame:\n    \"\"\"Return a ``pandas.DataFrame`` of the department's employees.\"\"\"\n\n    query = \"SELECT * FROM employees WHERE department = ? ORDER BY salary\"\n    with _get_connection(database) as connection:\n        return pd.read_sql_query(query, connection, params=(department,))\n\n\ndef cleanup_employee_db(database: DatabaseLike) -&gt; None:\n    \"\"\"Remove the SQLite file for the employee database if it exists.\"\"\"\n\n    if isinstance(database, sqlite3.Connection):\n        database.close()\n        return\n\n    db_path = os.fspath(database)\n    if os.path.exists(db_path):\n        os.remove(db_path)\n\n\ndef main(database: DatabaseLike = DB_FILE) -&gt; None:\n    \"\"\"Demonstrate basic interactions with the employee database.\"\"\"\n\n    print(f\"--- 1. Creating and populating the database: {database} ---\")\n    initialize_employee_db(database)\n    print(\"Database and table created successfully.\")\n    print(\"-\" * 20)\n\n    print(\"--- 2. Querying for 'Sales' employees with sqlite3 ---\")\n    sales_employees = fetch_department_salaries(database, \"Sales\")\n    print(\"Results from sqlite3:\")\n    for name, salary in sales_employees:\n        print(f\"  - Name: {name}, Salary: ${salary:,.2f}\")\n    print(\"-\" * 20)\n\n    print(\"--- 3. Querying all 'Engineering' employees with Pandas ---\")\n    engineering_df = fetch_department_dataframe(database, \"Engineering\")\n    print(\"DataFrame returned by pd.read_sql_query():\")\n    print(engineering_df)\n    avg_salary = engineering_df[\"salary\"].mean()\n    print(f\"\\nAverage Engineering Salary: ${avg_salary:,.2f}\")\n\n    cleanup_employee_db(database)\n    print(f\"\\nCleaned up and removed {database}.\")\n    print(\"-\" * 20)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 31: Solutions to Exercises\n\"\"\"\n\nimport os\nimport sqlite3\n\nimport pandas as pd\n\nDB_FILE = \"company.db\"\n\n# --- Exercise 1: Create and Populate a Database ---\nprint(\"--- Solution to Exercise 1 ---\")\ntry:\n    # Connect to the database (creates the file if it doesn't exist)\n    conn = sqlite3.connect(DB_FILE)\n    cursor = conn.cursor()\n\n    # Create the 'products' table\n    # \"IF NOT EXISTS\" prevents an error if the script is run multiple times\n    cursor.execute(\n        \"\"\"\n    CREATE TABLE IF NOT EXISTS products (\n        product_id INTEGER PRIMARY KEY,\n        name TEXT NOT NULL,\n        price REAL NOT NULL\n    )\"\"\"\n    )\n\n    # Insert sample data\n    # Using a list of tuples for the data\n    products_to_insert = [\n        (101, \"Laptop\", 1200.00),\n        (102, \"Mouse\", 25.50),\n        (103, \"Keyboard\", 75.00),\n        (104, \"Monitor\", 350.00),\n    ]\n    # executemany is efficient for inserting multiple rows\n    cursor.executemany(\n        \"INSERT INTO products (product_id, name, price) VALUES (?, ?, ?)\",\n        products_to_insert,\n    )\n\n    # Commit the changes to save them to the database file\n    conn.commit()\n    print(f\"Database '{DB_FILE}' created and populated successfully.\")\n\nexcept sqlite3.Error as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    # Always close the connection\n    if conn:  # pyright: ignore[reportPossiblyUnboundVariable]\n        conn.close()\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Query with `sqlite3` ---\nprint(\"--- Solution to Exercise 2 ---\")\ntry:\n    conn = sqlite3.connect(DB_FILE)\n    cursor = conn.cursor()\n\n    # SQL query to select products with price &gt; 100\n    sql_query = \"SELECT * FROM products WHERE price &gt; 100\"\n    cursor.execute(sql_query)\n\n    # Fetch all the results that match the query\n    expensive_products = cursor.fetchall()\n\n    print(\"Products with price &gt; $100:\")\n    for product in expensive_products:\n        print(f\"  - ID: {product[0]}, Name: {product[1]}, Price: ${product[2]:.2f}\")\n\nexcept sqlite3.Error as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    if conn:  # pyright: ignore[reportPossiblyUnboundVariable]\n        conn.close()\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Query with Pandas ---\nprint(\"--- Solution to Exercise 3 ---\")\ntry:\n    conn = sqlite3.connect(DB_FILE)\n\n    # The SQL query to select all data\n    sql_query_all = \"SELECT * FROM products\"\n\n    # Use pandas to execute the query and load data into a DataFrame\n    products_df = pd.read_sql_query(sql_query_all, conn)\n\n    print(\"All products loaded into a Pandas DataFrame:\")\n    print(products_df)\n\nexcept sqlite3.Error as e:\n    print(f\"An error occurred: {e}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    if conn:  # pyright: ignore[reportPossiblyUnboundVariable]\n        conn.close()\n    # Clean up the database file after the script is done\n    if os.path.exists(DB_FILE):\n        os.remove(DB_FILE)\n        print(f\"\\nCleaned up and removed '{DB_FILE}'.\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-32-other-databases/","title":"\ud83d\udcd8 Day 32: Connecting to Other Databases (MySQL & MongoDB)","text":"<p>In the previous lesson, we used <code>sqlite3</code>, which is fantastic for learning and small projects because it's built into Python and doesn't require a separate server. However, in a corporate environment, you will most likely be connecting to a more powerful, server-based database like MySQL, PostgreSQL, or a NoSQL database like MongoDB.</p> <p>The great news is that the patterns and skills you just learned are directly transferable.</p>"},{"location":"lessons/day-32-other-databases/#connecting-to-other-sql-databases-mysql-postgresql","title":"Connecting to Other SQL Databases (MySQL, PostgreSQL)","text":"<p>To connect to other SQL databases, the only thing that changes is the library you use to make the connection. The SQL queries you write and the way you use Pandas to interact with the database remain almost identical.</p> <p>You would first need to install the appropriate library:</p> <ul> <li>For MySQL: <code>pip install mysql-connector-python</code></li> <li>For PostgreSQL: <code>pip install psycopg2-binary</code></li> </ul> <p>Then, your connection code would look slightly different, requiring credentials like a username, password, and host address.</p> <p>Example with <code>psycopg2</code> for PostgreSQL:</p> <pre><code>import psycopg2\nimport pandas as pd\n\n# The connection details change, but the pattern is the same\nconn = psycopg2.connect(\n    host=\"your_database_host\",\n    database=\"your_database_name\",\n    user=\"your_username\",\n    password=\"your_password\"\n)\n\n# After this, using Pandas is EXACTLY the same!\nsql_query = \"SELECT * FROM sales WHERE region = 'North';\"\nnorth_sales_df = pd.read_sql_query(sql_query, conn)\n\nconn.close()\n\nprint(north_sales_df.head())\n</code></pre> <p>Key Takeaway: As an analyst, your main tool, <code>pandas.read_sql_query</code>, works the same way. You just need to get the correct connection details from your IT or data engineering team.</p>"},{"location":"lessons/day-32-other-databases/#a-brief-introduction-to-nosql-mongodb","title":"A Brief Introduction to NoSQL: MongoDB","text":"<p>Not all databases are \"relational\" (i.e., table-based) like SQL databases. Another popular category is NoSQL, and one of the most popular NoSQL databases is MongoDB.</p> <p>Instead of tables and rows, MongoDB stores data in collections of documents. A document is very similar to a Python dictionary or a JSON object, allowing for flexible and nested data structures.</p> <p>To work with MongoDB in Python, you use the <code>pymongo</code> library: <code>pip install pymongo</code></p> <p>Example with <code>pymongo</code> for MongoDB:</p> <pre><code>from pymongo import MongoClient\n\n# Connect to the MongoDB server\nclient = MongoClient('mongodb://your_username:your_password@your_host')\n\n# Select your database and collection (similar to a table)\ndb = client['company_db']\ncollection = db['employees']\n\n# Find a document (similar to a row)\n# This finds one employee in the 'Sales' department\nsales_employee = collection.find_one({'department': 'Sales'})\n\nprint(sales_employee)\n\n# Find multiple documents\nengineering_employees = collection.find({'department': 'Engineering'})\n\n# You can convert the results to a list of dictionaries...\nengineering_list = list(engineering_employees)\n# ...and then load that list directly into a Pandas DataFrame!\nengineering_df = pd.DataFrame(engineering_list)\n\nclient.close()\n</code></pre>"},{"location":"lessons/day-32-other-databases/#summary","title":"Summary","text":"<p>This was a conceptual lesson to show you that while the specific connection details may change, the skills you've learned are broadly applicable.</p> <ul> <li>For any SQL database, you will use a specific library to connect, but your interaction will primarily be writing SQL queries and using <code>pandas.read_sql_query</code>.</li> <li>For NoSQL databases like MongoDB, you'll use a different library (<code>pymongo</code>) and query language, but the goal is the same: get the data into a format that can be loaded into a Pandas DataFrame for analysis.</li> </ul> <p>There are no coding exercises for this day. The goal is to understand the concepts so you are prepared when you encounter these different database systems in the workplace.</p>"},{"location":"lessons/day-32-other-databases/#additional-materials","title":"Additional Materials","text":"<ul> <li>other_databases.ipynb</li> </ul> other_databases.py <p>View on GitHub</p> other_databases.py<pre><code>\"\"\"Reusable helpers that demonstrate SQL and MongoDB connection patterns.\n\nThe functions in this module deliberately receive their database clients as\narguments so they can be easily tested without establishing real network\nconnections.  The calling code is responsible for creating the client (or a\nstub implementation) and passing in the relevant credentials or connection\nconfiguration.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Callable, Iterable, Mapping, MutableMapping, Optional, Sequence\n\nCredentials = Mapping[str, Any]\nClientFactory = Callable[[Credentials], Any]\n\n\ndef execute_sql_query(\n    client_factory: ClientFactory,\n    query: str,\n    *,\n    credentials: Credentials,\n    parameters: Optional[Sequence[Any]] = None,\n    commit: bool = False,\n) -&gt; Sequence[Any]:\n    \"\"\"Execute a SQL query using a dependency-injected client factory.\n\n    Parameters\n    ----------\n    client_factory:\n        Callable that accepts a credentials mapping and returns a DB-API style\n        connection object.  In production this could be ``mysql.connector.connect``\n        or ``psycopg2.connect``; in tests this can be a stub or mock.\n    query:\n        SQL string to execute.\n    credentials:\n        Mapping that contains the connection details (host, database, user,\n        password, etc.).\n    parameters:\n        Optional sequence of parameters to pass to ``cursor.execute``.\n    commit:\n        Whether to call ``commit`` on the connection after execution.  This is\n        useful for ``INSERT``/``UPDATE`` statements.\n\n    Returns\n    -------\n    Sequence[Any]\n        The rows returned by ``cursor.fetchall()``.  For statements that do not\n        return rows this will typically be an empty sequence.\n    \"\"\"\n\n    connection = client_factory(credentials)\n    cursor = connection.cursor()\n    try:\n        if parameters is None:\n            cursor.execute(query)\n        else:\n            cursor.execute(query, parameters)\n\n        results: Sequence[Any] = []\n        description = getattr(cursor, \"description\", None)\n        if description:\n            results = cursor.fetchall()\n\n        if commit and hasattr(connection, \"commit\"):\n            connection.commit()\n\n        return results\n    finally:\n        # Ensure resources are released even if ``execute`` raises an error.\n        if hasattr(cursor, \"close\"):\n            cursor.close()\n        if hasattr(connection, \"close\"):\n            connection.close()\n\n\ndef upsert_sales_forecast(\n    client_factory: ClientFactory,\n    *,\n    credentials: Credentials,\n    forecast_rows: Iterable[Sequence[Any]],\n) -&gt; None:\n    \"\"\"Insert or update forecast data via an injected SQL client.\n\n    The function expects the ``forecast_rows`` iterable to contain tuples of the\n    form ``(region, forecast_month, revenue)``.  The specific SQL syntax is kept\n    intentionally generic so that the function remains database-agnostic.\n    \"\"\"\n\n    merge_query = \"\"\"\n        INSERT INTO sales_forecast (region, forecast_month, revenue)\n        VALUES (%s, %s, %s)\n        ON CONFLICT (region, forecast_month) DO UPDATE\n        SET revenue = EXCLUDED.revenue\n        \"\"\".strip()\n\n    connection = client_factory(credentials)\n    cursor = connection.cursor()\n    try:\n        for row in forecast_rows:\n            cursor.execute(merge_query, row)\n\n        if hasattr(connection, \"commit\"):\n            connection.commit()\n    finally:\n        if hasattr(cursor, \"close\"):\n            cursor.close()\n        if hasattr(connection, \"close\"):\n            connection.close()\n\n\ndef find_documents(\n    mongo_client: MutableMapping[str, Any],\n    *,\n    database: str,\n    collection: str,\n    filter_query: Optional[Mapping[str, Any]] = None,\n    projection: Optional[Mapping[str, Any]] = None,\n) -&gt; list[Any]:\n    \"\"\"Query a MongoDB collection using an injected client object.\"\"\"\n\n    filter_query = dict(filter_query or {})\n\n    db = mongo_client[database]\n    collection_handle = db[collection]\n\n    if projection is None:\n        cursor = collection_handle.find(filter_query)\n    else:\n        cursor = collection_handle.find(filter_query, projection)\n\n    return list(cursor)\n\n\ndef insert_documents(\n    mongo_client: MutableMapping[str, Any],\n    *,\n    database: str,\n    collection: str,\n    documents: Sequence[Mapping[str, Any]],\n) -&gt; Sequence[Any]:\n    \"\"\"Insert multiple documents into a MongoDB collection.\"\"\"\n\n    if not documents:\n        return []\n\n    db = mongo_client[database]\n    collection_handle = db[collection]\n    result = collection_handle.insert_many(list(documents))\n    return getattr(result, \"inserted_ids\", result)\n</code></pre>"},{"location":"lessons/day-33-api/","title":"\ud83d\udcd8 Day 33: Accessing Web APIs with `requests`","text":""},{"location":"lessons/day-33-api/#overview","title":"Overview","text":"<p>This lesson introduces a lightweight wrapper around the JSONPlaceholder demo API. The module exports three helpers\u2014<code>fetch_users</code>, <code>fetch_post</code>, and <code>fetch_posts_by_user</code>\u2014that return structured data for analytics workflows while keeping the HTTP layer easy to mock during tests.</p>"},{"location":"lessons/day-33-api/#learning-goals","title":"Learning goals","text":"<ul> <li>Understand how to issue HTTP GET requests with <code>requests</code>.</li> <li>Convert JSON payloads into <code>pandas.DataFrame</code> objects for analysis.</li> <li>Inject custom HTTP clients (for example, <code>requests.Session</code> objects or call   stubs) to make networked code testable.</li> </ul>"},{"location":"lessons/day-33-api/#requirements","title":"Requirements","text":"<p>Install the core and testing dependencies into your environment:</p> <pre><code>pip install -r requirements.txt\npip install pytest responses\n</code></pre> <p><code>responses</code> is only needed when you want to simulate the API locally or run the pytest suite.</p>"},{"location":"lessons/day-33-api/#running-the-lesson","title":"Running the lesson","text":""},{"location":"lessons/day-33-api/#live-api-demo","title":"Live API demo","text":"<p>Execute the script directly to fetch data from JSONPlaceholder:</p> <pre><code>python Day_33_API/api.py\n</code></pre> <p>The command prints a preview of the user list, details for post <code>1</code>, and a table of posts authored by user <code>2</code>.</p>"},{"location":"lessons/day-33-api/#mocked-endpoints","title":"Mocked endpoints","text":"<p>The helper functions accept a <code>requests.Session</code> or any callable with the same signature as <code>requests.get</code>. This allows you to supply canned responses when the internet is unavailable or you want deterministic examples:</p> <pre><code>from Day_33_API.api import fetch_users\n\nclass MockClient:\n    def get(self, url, **kwargs):\n        class _Response:\n            def __init__(self, payload):\n                self._payload = payload\n\n            def raise_for_status(self):\n                return None\n\n            def json(self):\n                return self._payload\n\n        return _Response([{\"id\": 1, \"name\": \"Ada\", \"username\": \"ada\"}])\n\nmocked_users = fetch_users(client=MockClient())\nprint(mocked_users)\n</code></pre> <p>The included pytest suite (see below) relies on the <code>responses</code> library to provide richer, request-aware mocks if you prefer a declarative API.</p>"},{"location":"lessons/day-33-api/#tests","title":"Tests","text":"<p>Run the Day 33 unit tests, which exercise both success and error paths using mocked HTTP responses:</p> <pre><code>pytest tests/test_day_33.py\n</code></pre> <p>To execute the entire collection of lesson tests, run <code>pytest</code> from the project root.</p>"},{"location":"lessons/day-33-api/#additional-materials","title":"Additional Materials","text":"<ul> <li>api.ipynb</li> <li>solutions.ipynb</li> </ul> api.py <p>View on GitHub</p> api.py<pre><code>\"\"\"Utility functions for interacting with the JSONPlaceholder API used in Day 33.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Optional\n\nimport pandas as pd\nimport requests\n\nJSONPLACEHOLDER_BASE_URL = \"https://jsonplaceholder.typicode.com\"\nUSERS_ENDPOINT = f\"{JSONPLACEHOLDER_BASE_URL}/users\"\nPOSTS_ENDPOINT = f\"{JSONPLACEHOLDER_BASE_URL}/posts\"\n\n\ndef _make_request(\n    url: str,\n    client: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; requests.Response:\n    \"\"\"Execute a GET request using the provided client.\n\n    The client can be a ``requests.Session`` (or any object that exposes a\n    ``get`` method) or a callable that mimics ``requests.get``.\n    \"\"\"\n\n    if client is None:\n        with requests.Session() as session:\n            response = session.get(url, **kwargs)\n    elif hasattr(client, \"get\"):\n        response = client.get(url, **kwargs)  # type: ignore[call-arg]\n    elif callable(client):\n        response = client(url, **kwargs)\n    else:\n        raise TypeError(\"client must be a requests.Session, callable, or None\")\n\n    response.raise_for_status()\n    return response\n\n\ndef fetch_users(\n    client: Optional[Any] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetch all users and return them as a :class:`pandas.DataFrame`.\"\"\"\n\n    response = _make_request(USERS_ENDPOINT, client=client)\n    return pd.DataFrame(response.json())\n\n\ndef fetch_post(\n    post_id: int,\n    client: Optional[Any] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Fetch a single post by ``post_id`` and return the JSON payload.\"\"\"\n\n    url = f\"{POSTS_ENDPOINT}/{post_id}\"\n    response = _make_request(url, client=client)\n    return response.json()\n\n\ndef fetch_posts_by_user(\n    user_id: int,\n    client: Optional[Any] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Fetch posts filtered by ``user_id`` and return them as a DataFrame.\"\"\"\n\n    response = _make_request(POSTS_ENDPOINT, client=client, params={\"userId\": user_id})\n    return pd.DataFrame(response.json())\n\n\ndef _print_preview(df: pd.DataFrame, label: str) -&gt; None:\n    \"\"\"Utility helper to display a DataFrame preview in the CLI demo.\"\"\"\n\n    print(f\"{label} (showing up to 5 rows):\")\n    if df.empty:\n        print(\"  No rows returned.\")\n    else:\n        print(df.head())\n    print(\"-\" * 20)\n\n\ndef main() -&gt; None:\n    \"\"\"Simple CLI demonstration that exercises the helper functions.\"\"\"\n\n    print(\"--- 1. Fetching a list of users ---\")\n    try:\n        users_df = fetch_users()\n        _print_preview(users_df, \"Users\")\n    except requests.RequestException as exc:\n        print(f\"Failed to fetch users: {exc}\")\n\n    print(\"--- 2. Fetching a single post (ID = 1) ---\")\n    try:\n        post = fetch_post(1)\n        print(f\"  User ID: {post['userId']}\")\n        print(f\"  Title: {post['title']}\")\n        print(\"-\" * 20)\n    except requests.RequestException as exc:\n        print(f\"Failed to fetch post: {exc}\")\n\n    print(\"--- 3. Fetching all posts by a specific user (userId = 2) ---\")\n    try:\n        posts_df = fetch_posts_by_user(2)\n        _print_preview(posts_df, \"Posts for userId=2\")\n    except requests.RequestException as exc:\n        print(f\"Failed to fetch posts: {exc}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 33: Solutions to Exercises\n\"\"\"\n\nimport pandas as pd\nimport requests\n\n# --- Exercise 1: Fetch All Posts ---\nprint(\"--- Solution to Exercise 1 ---\")\nposts_url = \"https://jsonplaceholder.typicode.com/posts\"\ntry:\n    response = requests.get(posts_url)\n    response.raise_for_status()\n    posts_data = response.json()\n    posts_df = pd.DataFrame(posts_data)\n    print(\"DataFrame of all posts (first 5 rows):\")\n    print(posts_df.head())\nexcept requests.exceptions.RequestException as e:\n    print(f\"Error fetching posts: {e}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 2: Fetch a Specific User's Data ---\nprint(\"--- Solution to Exercise 2 ---\")\nuser_id = 5\nuser_url = f\"https://jsonplaceholder.typicode.com/users/{user_id}\"\ntry:\n    response = requests.get(user_url)\n    response.raise_for_status()\n    user_data = response.json()\n    # The 'company' value is a nested dictionary\n    company_name = user_data.get(\"company\", {}).get(\"name\", \"N/A\")\n    print(f\"Data for User ID {user_id}:\")\n    print(f\"  Name: {user_data.get('name', 'N/A')}\")\n    print(f\"  Company: {company_name}\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"Error fetching user {user_id}: {e}\")\nprint(\"-\" * 20)\n\n\n# --- Exercise 3: Fetch Comments for a Specific Post ---\nprint(\"--- Solution to Exercise 3 ---\")\ncomments_url = \"https://jsonplaceholder.typicode.com/comments\"\nparams = {\"postId\": 3}\ntry:\n    response = requests.get(comments_url, params=params)\n    response.raise_for_status()\n    comments_data = response.json()\n    comments_df = pd.DataFrame(comments_data)\n    print(f\"DataFrame of comments for postId={params['postId']} (first 5 rows):\")\n    print(comments_df.head())\nexcept requests.exceptions.RequestException as e:\n    print(f\"Error fetching comments: {e}\")\nprint(\"-\" * 20)\n</code></pre>"},{"location":"lessons/day-34-building-an-api/","title":"\ud83d\udcd8 Day 34: Building a Simple API with Flask","text":"<p>Consuming data from APIs is a core skill. But what if you need to provide data from your analysis to another person or application? Instead of sending a CSV file, you can build your own API. This allows other services (like a web dashboard or another analyst's script) to access your data programmatically.</p> <p>Today, we'll learn how to build a simple web API using Flask, a popular \"micro-framework\" for Python. It's called a micro-framework because it's very lightweight and simple to start with, but can be extended to build complex applications.</p>"},{"location":"lessons/day-34-building-an-api/#what-is-a-web-framework","title":"What is a Web Framework?","text":"<p>A web framework like Flask handles all the low-level, complicated parts of web communication (like handling HTTP requests and responses), so you can focus on writing your application's logic.</p>"},{"location":"lessons/day-34-building-an-api/#your-first-flask-api","title":"Your First Flask API","text":"<p>A Flask application can be incredibly simple. Here is a basic \"Hello, World\" example:</p> <pre><code>from flask import Flask\n\n# 1. Create an instance of the Flask class\napp = Flask(__name__)\n\n# 2. Define a \"route\"\n# This tells Flask: \"When someone visits the main URL ('/'), run this function.\"\n@app.route('/')\ndef home():\n    return \"Welcome to our API!\"\n\n# 3. Run the app\n# The __name__ == '__main__' block ensures this code only runs when you execute the script directly.\nif __name__ == '__main__':\n    app.run(debug=True) # debug=True allows for auto-reloading on code changes\n</code></pre>"},{"location":"lessons/day-34-building-an-api/#returning-json-data","title":"Returning JSON Data","text":"<p>An API for data analysis isn't very useful if it just returns text. We need to return structured data, and the standard format for that is JSON. Flask provides a handy function called <code>jsonify</code> that converts a Python dictionary into a proper JSON response.</p> <pre><code>from flask import Flask, jsonify\n\napp = Flask(__name__)\n\n# Sample data (in a real app, this might come from a database or a Pandas DataFrame)\nproducts = [\n    {'id': 1, 'name': 'Laptop', 'price': 1200},\n    {'id': 2, 'name': 'Mouse', 'price': 25}\n]\n\n@app.route('/api/products', methods=['GET'])\ndef get_products():\n    return jsonify(products)\n</code></pre> <p>Now, if you run this app and navigate to <code>http://127.0.0.1:5000/api/products</code> in your browser or an API tool, you'll get the list of products in JSON format.</p>"},{"location":"lessons/day-34-building-an-api/#dynamic-routes","title":"Dynamic Routes","text":"<p>You can create routes that accept parameters. For example, to get a single product by its ID.</p> <pre><code>@app.route('/api/products/&lt;int:product_id&gt;', methods=['GET'])\ndef get_product(product_id):\n    # Find the product with the matching ID\n    product = next((p for p in products if p['id'] == product_id), None)\n    if product:\n        return jsonify(product)\n    else:\n        # Return a 404 Not Found error if the product doesn't exist\n        return jsonify({\"error\": \"Product not found\"}), 404\n</code></pre> <p>Now you can go to <code>/api/products/1</code> to get the laptop, or <code>/api/products/2</code> to get the mouse.</p>"},{"location":"lessons/day-34-building-an-api/#exercises-day-34","title":"\ud83d\udcbb Exercises: Day 34","text":"<ol> <li> <p>Create a Basic API Server:</p> </li> <li> <p>Create a file named <code>my_api.py</code>.</p> </li> <li>Set up a basic Flask application.</li> <li> <p>Create a root endpoint <code>/</code> that returns a simple welcome message like \"Welcome to the Company Data API\".</p> </li> <li> <p>Serve Employee Data:</p> </li> <li> <p>Inside your <code>my_api.py</code> script, create a list of employee dictionaries. Each employee should have an <code>id</code>, <code>name</code>, and <code>department</code>.</p> </li> <li>Create a new endpoint at <code>/api/employees</code>.</li> <li> <p>When a <code>GET</code> request is made to this endpoint, it should return the full list of employees as a JSON response.</p> </li> <li> <p>Serve a Single Employee's Data:</p> </li> <li> <p>Create a dynamic route <code>/api/employees/&lt;int:employee_id&gt;</code>.</p> </li> <li>This endpoint should find the employee with the matching ID from your list.</li> <li>If the employee is found, return their data as a JSON object.</li> <li>If no employee with that ID is found, return a JSON error message with a 404 status code.</li> </ol> <p>\ud83c\udf89 Incredible! You've just learned how to build a web API. This is a massive step. It bridges the gap between performing analysis for yourself and providing data and services to others, forming the backbone of modern data applications and microservices.</p>"},{"location":"lessons/day-34-building-an-api/#running-the-development-server","title":"Running the Development Server","text":"<p>The lesson's reference implementation exposes a <code>create_app()</code> factory in <code>api_server.py</code>. You can start the local development server with:</p> <pre><code>export FLASK_APP=Day_34_Building_an_API.api_server:create_app\nflask run --debug\n</code></pre> <p>The <code>--debug</code> flag enables auto-reload and better error messages while you iterate on your API. Flask will serve the application on <code>http://127.0.0.1:5000/</code> by default.</p>"},{"location":"lessons/day-34-building-an-api/#running-the-tests","title":"Running the Tests","text":"<p>Pytest is configured to exercise the API endpoints. From the repository root, run:</p> <pre><code>pytest tests/test_day_34.py\n</code></pre> <p>The tests use Flask's test client to verify that the root page and JSON endpoints respond with the expected payloads and HTTP status codes.</p>"},{"location":"lessons/day-34-building-an-api/#why-use-an-application-factory","title":"Why Use an Application Factory?","text":"<p>The application factory pattern encapsulates all app configuration inside a function. This makes it easy to:</p> <ul> <li>Create multiple instances of the app with different settings (e.g., testing versus production).</li> <li>Avoid running application code at import time, which keeps tools like the Flask CLI and pytest fast.</li> <li>Improve modularity by cleanly separating data configuration from route registration.</li> </ul> <p>Because <code>create_app()</code> returns a fully configured Flask instance, you can reuse it for development, testing, or even deployment without duplicating setup code.</p>"},{"location":"lessons/day-34-building-an-api/#additional-materials","title":"Additional Materials","text":"<ul> <li>api_server.ipynb</li> <li>data.ipynb</li> <li>solutions.ipynb</li> </ul> api_server.py <p>View on GitHub</p> api_server.py<pre><code>\"\"\"Day 34: Building a Simple API with Flask.\n\nThis module exposes a Flask application factory that serves sample\nbusiness data in JSON format.\n\"\"\"\n\nfrom flask import Flask, jsonify\n\nfrom Day_34_Building_an_API.data import EMPLOYEES, PRODUCTS\n\n\ndef create_app() -&gt; Flask:\n    \"\"\"Create and configure the Flask application for the sample API.\"\"\"\n    app = Flask(__name__)\n\n    app.config[\"PRODUCTS\"] = PRODUCTS\n    app.config[\"EMPLOYEES\"] = EMPLOYEES\n\n    @app.route(\"/\")\n    def home():\n        return (\n            \"&lt;h1&gt;Welcome to the Company Data API&lt;/h1&gt;\"\n            \"&lt;p&gt;Use endpoints like /api/v1/products to get data.&lt;/p&gt;\"\n        )\n\n    @app.route(\"/api/v1/products\", methods=[\"GET\"])\n    def get_all_products():\n        \"\"\"Return the full list of products as JSON.\"\"\"\n        return jsonify(app.config[\"PRODUCTS\"])\n\n    @app.route(\"/api/v1/products/&lt;int:product_id&gt;\", methods=[\"GET\"])\n    def get_single_product(product_id):\n        \"\"\"Return a single product matching the given ID.\"\"\"\n        products = app.config[\"PRODUCTS\"]\n        product = next((p for p in products if p[\"id\"] == product_id), None)\n\n        if product:\n            return jsonify(product)\n        return jsonify({\"error\": \"Product not found\"}), 404\n\n    @app.route(\"/api/v1/employees\", methods=[\"GET\"])\n    def get_all_employees():\n        \"\"\"Return the full list of employees as JSON.\"\"\"\n        return jsonify(app.config[\"EMPLOYEES\"])\n\n    return app\n\n\napp = create_app()\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n</code></pre> data.py <p>View on GitHub</p> data.py<pre><code>\"\"\"Sample data used by the Day 34 Flask API.\"\"\"\n\nPRODUCTS = [\n    {\"id\": 1, \"name\": \"Laptop\", \"price\": 1200, \"category\": \"Electronics\"},\n    {\"id\": 2, \"name\": \"Mouse\", \"price\": 25, \"category\": \"Peripherals\"},\n    {\"id\": 3, \"name\": \"Keyboard\", \"price\": 75, \"category\": \"Peripherals\"},\n]\n\nEMPLOYEES = [\n    {\"id\": 101, \"name\": \"Alice\", \"department\": \"Sales\"},\n    {\"id\": 102, \"name\": \"Bob\", \"department\": \"Engineering\"},\n]\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 34: Solutions to Exercises\n\nThis single file contains the complete solution for all three\nexercises, as they build upon each other.\n\"\"\"\n\nfrom flask import Flask, jsonify\n\n# --- Exercise 1: Create a Basic API Server ---\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef home():\n    \"\"\"Returns a simple welcome message for the root URL.\"\"\"\n    return \"Welcome to the Company Data API\"\n\n\n# --- Exercise 2: Serve Employee Data ---\n# Sample data for the API\nEMPLOYEES = [\n    {\"id\": 1, \"name\": \"John Doe\", \"department\": \"Sales\"},\n    {\"id\": 2, \"name\": \"Jane Smith\", \"department\": \"Engineering\"},\n    {\"id\": 3, \"name\": \"Peter Jones\", \"department\": \"Marketing\"},\n    {\"id\": 4, \"name\": \"Lisa Ray\", \"department\": \"Sales\"},\n]\n\n\n@app.route(\"/api/employees\", methods=[\"GET\"])\ndef get_employees():\n    \"\"\"Returns the full list of employees as JSON.\"\"\"\n    return jsonify(EMPLOYEES)\n\n\n# --- Exercise 3: Serve a Single Employee's Data ---\n@app.route(\"/api/employees/&lt;int:employee_id&gt;\", methods=[\"GET\"])\ndef get_employee(employee_id):\n    \"\"\"\n    Returns data for a single employee based on their ID.\n    Returns a 404 error if the employee is not found.\n    \"\"\"\n    # Find the employee in the list\n    employee = next((emp for emp in EMPLOYEES if emp[\"id\"] == employee_id), None)\n\n    if employee:\n        return jsonify(employee)\n    else:\n        # Return a JSON error message and a 404 status code\n        error_message = {\"error\": f\"Employee with ID {employee_id} not found.\"}\n        return jsonify(error_message), 404\n\n\nif __name__ == \"__main__\":\n    # To run this API server:\n    # 1. Save the code as a Python file (e.g., 'my_api.py').\n    # 2. Run from your terminal: python my_api.py\n    # 3. Open your web browser or an API tool like Postman.\n    # 4. Navigate to URLs like:\n    #    - http://127.0.0.1:5000/\n    #    - http://127.0.0.1:5000/api/employees\n    #    - http://127.0.0.1:5000/api/employees/2\n    #    - http://127.0.0.1:5000/api/employees/99 (to see the 404 error)\n    app.run(debug=True)\n</code></pre>"},{"location":"lessons/day-35-flask-web-framework/","title":"\ud83c\udf10 Day 35: Flask Web Framework","text":"<p>Welcome to Day 35! This lesson contains a small Flask project that analyses submitted text and reports simple statistics such as word counts and lexical diversity.</p>"},{"location":"lessons/day-35-flask-web-framework/#environment-setup","title":"Environment setup","text":"<ol> <li>Create and activate a virtual environment (recommended):    <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows use: .venv\\\\Scripts\\\\activate\n</code></pre></li> <li>Install the dependencies for this lesson:    <pre><code>pip install -r Day_35_Flask_Web_Framework/requirements.txt\n</code></pre></li> </ol>"},{"location":"lessons/day-35-flask-web-framework/#running-the-flask-application","title":"Running the Flask application","text":"<p>The application now exposes a <code>create_app()</code> factory in <code>Day_35_Flask_Web_Framework/app/__init__.py</code>, so it can be started with the Flask CLI:</p> <pre><code>FLASK_APP=Day_35_Flask_Web_Framework.app:create_app flask run\n</code></pre> <p>Alternatively, you can execute the module directly:</p> <pre><code>python -m Day_35_Flask_Web_Framework.app\n</code></pre> <p>Once running, visit http://127.0.0.1:5000/ to interact with the analyser. Use the Post page to submit text and review the calculated statistics on the Result page.</p>"},{"location":"lessons/day-35-flask-web-framework/#running-the-tests","title":"Running the tests","text":"<p>Automated tests validate both the Flask routes and the helper functions that power the analyser. Execute them from the repository root:</p> <pre><code>pytest -k day_35\n</code></pre>"},{"location":"lessons/day-35-flask-web-framework/#exercises-day-35","title":"\ud83d\udcbb Exercises: Day 35","text":"<ol> <li> <p>Create a new route:</p> </li> <li> <p>Add a new route to the <code>create_app</code> factory at the URL <code>/about</code>.</p> </li> <li> <p>Create a view function that returns a simple string, like \"This is the about page.\"</p> </li> <li> <p>Create a new template:</p> </li> <li> <p>Create a new HTML file named <code>contact.html</code> in the <code>templates</code> directory.</p> </li> <li> <p>Add a new route to the Flask app at the URL <code>/contact</code> that renders the <code>contact.html</code> template.</p> </li> <li> <p>Pass data to a template:</p> </li> <li> <p>Modify the <code>/about</code> route to pass your name to a new <code>about.html</code> template.</p> </li> <li>In the <code>about.html</code> template, display the name that was passed from the view function.</li> </ol>"},{"location":"lessons/day-35-flask-web-framework/#solutions","title":"Solutions","text":"<p>You can find the solutions to these exercises in the <code>solutions.py</code> file in this directory.</p> <p>\ud83c\udf89 Congratulations! You've learned the basics of Flask, a powerful tool for building web applications and dashboards with Python.</p>"},{"location":"lessons/day-35-flask-web-framework/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code># Day 35: Flask Web Framework - Solutions\n\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n## Exercise 1: Create a new route\n\n\n@app.route(\"/about-solution\")\ndef about_solution():\n    return \"This is the about page.\"\n\n\n## Exercise 2: Create a new template\n\n# 1. Create a new HTML file named 'contact.html' in the 'templates' directory.\n#    You can add some simple content to it, like:\n#\n#    &lt;!DOCTYPE html&gt;\n#    &lt;html&gt;\n#    &lt;head&gt;\n#        &lt;title&gt;Contact Us&lt;/title&gt;\n#    &lt;/head&gt;\n#    &lt;body&gt;\n#        &lt;h1&gt;Contact Us&lt;/h1&gt;\n#        &lt;p&gt;You can contact us at contact@example.com&lt;/p&gt;\n#    &lt;/body&gt;\n#    &lt;/html&gt;\n#\n# 2. Add a new route to app.py that renders this template.\n\n\n@app.route(\"/contact\")\ndef contact():\n    return render_template(\"contact.html\")\n\n\n## Exercise 3: Pass data to a template\n\n# 1. Create a new HTML file named 'about.html' in the 'templates' directory.\n#\n#    &lt;!DOCTYPE html&gt;\n#    &lt;html&gt;\n#    &lt;head&gt;\n#        &lt;title&gt;About Me&lt;/title&gt;\n#    &lt;/head&gt;\n#    &lt;body&gt;\n#        &lt;h1&gt;About {{ name }}&lt;/h1&gt;\n#    &lt;/body&gt;\n#    &lt;/html&gt;\n#\n# 2. Modify the '/about' route to pass your name to the template.\n\n\n@app.route(\"/about-me\")\ndef about_me():\n    my_name = \"Jules\"  # You can replace this with your name\n    return render_template(\"about.html\", name=my_name)\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre>"},{"location":"lessons/day-36-case-study/","title":"\ud83d\udcca Day 36 \u2013 Capstone Case Study","text":""},{"location":"lessons/day-36-case-study/#overview","title":"Overview","text":"<p>Day 36 ties together the full analytics workflow. You will load the <code>case_study_sales.csv</code> dataset, clean it, surface core revenue insights, and present a concise set of recommendations. The helpers in this folder mirror the solution structure so that you can focus on translating business questions into Python code.</p>"},{"location":"lessons/day-36-case-study/#files","title":"Files","text":"File Description <code>case_study.py</code> Student-facing template containing callable helpers and a minimal <code>main()</code> driver you can customize. <code>case_study_sales.csv</code> Sales dataset that powers the analysis. <code>solutions.py</code> Fully worked reference solution with detailed walkthrough code."},{"location":"lessons/day-36-case-study/#suggested-workflow","title":"Suggested Workflow","text":"<ol> <li>Use <code>load_case_study_data()</code> to import the CSV with parsed dates.</li> <li>Call <code>clean_case_study_data()</code> to enforce schema expectations and rebuild the    <code>Revenue</code> column when necessary.</li> <li>Explore metrics via <code>summarize_case_study()</code> or write your own aggregations    and visualizations.</li> <li>Capture narrative takeaways for the Head of Sales along with supporting    charts or tables.</li> </ol> <p>Running the script directly executes the helper pipeline and prints the top-line revenue summaries:</p> <pre><code>python Day_36_Case_Study/case_study.py\n</code></pre>"},{"location":"lessons/day-36-case-study/#tests","title":"Tests","text":"<p>Automated tests validate the helpers against the bundled dataset. From the repository root run:</p> <pre><code>pytest tests/test_day_36.py\n</code></pre>"},{"location":"lessons/day-36-case-study/#additional-materials","title":"Additional Materials","text":"<ul> <li>case_study.ipynb</li> <li>solutions.ipynb</li> </ul> case_study.py <p>View on GitHub</p> case_study.py<pre><code>\"\"\"Utility helpers for the Day 36 capstone case study.\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nimport pandas as pd\n\nDATA_PATH = Path(__file__).with_name(\"case_study_sales.csv\")\n\n\ndef load_case_study_data(path: str | Path = DATA_PATH) -&gt; pd.DataFrame:\n    \"\"\"Return the raw sales data as a :class:`~pandas.DataFrame`.\n\n    Parameters\n    ----------\n    path:\n        Location of the ``case_study_sales.csv`` file. Defaults to the copy that\n        ships with the repository.\n    \"\"\"\n\n    return pd.read_csv(path, parse_dates=[\"Date\"])\n\n\ndef clean_case_study_data(data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clean the raw case-study data for downstream analysis.\n\n    The helper performs a light-touch cleanup that mirrors the steps students\n    complete in the lesson notebook:\n\n    * ensure the ``Date`` column uses ``datetime64`` values,\n    * coerce numeric fields to numbers while dropping unparseable records,\n    * calculate ``Revenue`` from ``Price`` and ``Units Sold`` when it is\n      missing.\n    \"\"\"\n\n    cleaned = data.copy()\n\n    cleaned[\"Date\"] = pd.to_datetime(cleaned[\"Date\"], errors=\"coerce\")\n\n    numeric_columns = [\"Price\", \"Units Sold\"]\n    has_revenue = \"Revenue\" in cleaned.columns\n    if has_revenue:\n        numeric_columns.append(\"Revenue\")\n\n    for column in numeric_columns:\n        cleaned[column] = pd.to_numeric(cleaned[column], errors=\"coerce\")\n\n    cleaned = cleaned.dropna(subset=[\"Date\", \"Price\", \"Units Sold\"])\n\n    if not has_revenue:\n        cleaned[\"Revenue\"] = cleaned[\"Price\"] * cleaned[\"Units Sold\"]\n    else:\n        missing_revenue = cleaned[\"Revenue\"].isna()\n        if missing_revenue.any():\n            cleaned.loc[missing_revenue, \"Revenue\"] = (\n                cleaned.loc[missing_revenue, \"Price\"]\n                * cleaned.loc[missing_revenue, \"Units Sold\"]\n            )\n\n    cleaned[\"Units Sold\"] = cleaned[\"Units Sold\"].round().astype(\"Int64\")\n    cleaned[\"Revenue\"] = cleaned[\"Revenue\"].astype(float)\n\n    return cleaned.reset_index(drop=True)\n\n\ndef summarize_case_study(data: pd.DataFrame, *, top_n: int = 5) -&gt; Dict[str, Any]:\n    \"\"\"Generate headline metrics for the capstone analysis.\"\"\"\n\n    if data.empty:\n        raise ValueError(\"Cannot summarize an empty DataFrame.\")\n\n    summary: Dict[str, Any] = {\n        \"top_products\": data.groupby(\"Product\")[\"Revenue\"]\n        .sum()\n        .sort_values(ascending=False)\n        .head(top_n),\n        \"region_revenue\": data.groupby(\"Region\")[\"Revenue\"]\n        .sum()\n        .sort_values(ascending=False),\n        \"segment_revenue\": data.groupby(\"Customer Segment\")[\"Revenue\"]\n        .sum()\n        .sort_values(ascending=False),\n        \"channel_revenue\": data.groupby(\"Sales Channel\")[\"Revenue\"]\n        .sum()\n        .sort_values(ascending=False),\n        \"price_units_correlation\": data[[\"Price\", \"Units Sold\"]]\n        .corr()\n        .loc[\"Price\", \"Units Sold\"],\n        \"monthly_revenue\": data.set_index(\"Date\")[\"Revenue\"].resample(\"M\").sum(),\n    }\n\n    return summary\n\n\ndef main() -&gt; None:\n    \"\"\"Run a minimal command-line summary for the case study.\"\"\"\n\n    raw = load_case_study_data()\n    cleaned = clean_case_study_data(raw)\n    summary = summarize_case_study(cleaned)\n\n    print(\"Top products by revenue:\")\n    print(summary[\"top_products\"])\n    print(\"\\nRevenue by region:\")\n    print(summary[\"region_revenue\"])\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"\nDay 36: Solution to the Capstone Case Study\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# --- Step 1 &amp; 2: Load and Clean the Data ---\nprint(\"--- Step 1 &amp; 2: Loading and Cleaning Data ---\")\ntry:\n    df = pd.read_csv(\"case_study_sales.csv\", parse_dates=[\"Date\"])\n    print(\"Data loaded successfully.\")\n\n    # Check for missing values\n    if df.isnull().sum().sum() &gt; 0:\n        print(\"Missing values found. Dropping rows with missing data.\")\n        df.dropna(inplace=True)\n\n    # Ensure numeric columns are properly typed\n    numeric_columns = [\"Price\", \"Units Sold\"]\n    has_revenue_column = \"Revenue\" in df.columns\n    if has_revenue_column:\n        numeric_columns.append(\"Revenue\")\n    for col in numeric_columns:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n    if has_revenue_column:\n        if df[\"Revenue\"].isnull().any():\n            print(\n                \"Revenue column contained missing or non-numeric values. Recalculating from Price and Units Sold.\"\n            )\n            df[\"Revenue\"] = df[\"Price\"] * df[\"Units Sold\"]\n    else:\n        print(\"Revenue column not found. Creating it from Price and Units Sold.\")\n        df[\"Revenue\"] = df[\"Price\"] * df[\"Units Sold\"]\n\n    print(\"Data cleaned and 'Revenue' column created.\")\n\nexcept FileNotFoundError:\n    print(\"Error: case_study_sales.csv not found.\")\n    df = pd.DataFrame()\n\nif not df.empty:\n    # --- Step 3: Exploratory Data Analysis (EDA) ---\n    print(\"\\n--- Step 3: Answering Key Business Questions ---\")\n\n    # 1. Top 5 products by total revenue\n    top_products = (\n        df.groupby(\"Product\")[\"Revenue\"].sum().sort_values(ascending=False).head(5)\n    )\n    print(\"\\n1. Top 5 Products by Revenue:\")\n    print(top_products)\n\n    # 2. Top sales region by revenue\n    region_revenue = df.groupby(\"Region\")[\"Revenue\"].sum().sort_values(ascending=False)\n    print(\"\\n2. Revenue by Region:\")\n    print(region_revenue)\n\n    # 3. Customer segment and sales channel performance\n    segment_revenue = (\n        df.groupby(\"Customer Segment\")[\"Revenue\"].sum().sort_values(ascending=False)\n    )\n    channel_revenue = (\n        df.groupby(\"Sales Channel\")[\"Revenue\"].sum().sort_values(ascending=False)\n    )\n    print(\"\\n3. Revenue by Customer Segment:\")\n    print(segment_revenue)\n    print(\"\\n4. Revenue by Sales Channel:\")\n    print(channel_revenue)\n\n    # 4. Correlation between Price and Units Sold\n    correlation = df[[\"Price\", \"Units Sold\"]].corr()\n    print(\"\\n5. Correlation between Price and Units Sold:\")\n    print(correlation)\n\n    # 5. Monthly revenue trend\n    monthly_revenue = df.set_index(\"Date\")[\"Revenue\"].resample(\"M\").sum()\n    print(\"\\n6. Monthly Revenue Trend:\")\n    print(monthly_revenue)\n\n    # --- Step 4: Visualize Your Findings ---\n    print(\"\\n--- Step 4: Generating Visualizations ---\")\n\n    # Plot 1: Top 5 Products by Revenue\n    plt.figure(figsize=(10, 6))\n    top_products.plot(kind=\"bar\")\n    plt.title(\"Top 5 Products by Total Revenue\", fontsize=16)\n    plt.ylabel(\"Total Revenue ($)\")\n    plt.xlabel(\"Product\")\n    plt.xticks(rotation=45)\n    print(\"Displaying plot 1...\")\n    plt.show()\n\n    # Plot 2: Revenue by Region\n    plt.figure(figsize=(10, 6))\n    region_revenue.plot(kind=\"pie\", autopct=\"%1.1f%%\", startangle=90)\n    plt.title(\"Revenue Contribution by Region\", fontsize=16)\n    plt.ylabel(\"\")  # Hide the y-label for pie charts\n    print(\"Displaying plot 2...\")\n    plt.show()\n\n    # Plot 3: Revenue by Customer Segment\n    plt.figure(figsize=(10, 6))\n    segment_revenue.plot(kind=\"bar\")\n    plt.title(\"Revenue by Customer Segment\", fontsize=16)\n    plt.ylabel(\"Total Revenue ($)\")\n    plt.xlabel(\"Customer Segment\")\n    plt.xticks(rotation=45)\n    print(\"Displaying plot 3...\")\n    plt.show()\n\n    # Plot 4: Monthly Revenue Trend\n    plt.figure(figsize=(12, 6))\n    monthly_revenue.plot(kind=\"line\", marker=\"o\")\n    plt.title(\"Monthly Revenue Trend\", fontsize=16)\n    plt.ylabel(\"Total Revenue ($)\")\n    plt.xlabel(\"Date\")\n    print(\"Displaying plot 4...\")\n    plt.show()\n\n    # --- Step 5: Summary and Recommendations ---\n    print(\"\\n--- Step 5: Summary and Recommendations ---\")\n    summary = \"\"\"\n    Key Insights:\n    1. Laptops continue to dominate revenue, with smartphones and tablets forming a strong secondary tier.\n    2. The North and South regions lead performance, but international sales are a growing share thanks to strong marketplace channels.\n    3. Enterprise customers drive the bulk of revenue across both online and partner channels, while consumer sales excel through retail and marketplace partners.\n    4. There remains only a modest relationship between price and units sold, so pricing adjustments should be paired with targeted marketing.\n\n    Recommendations:\n    1. Double-down on laptop bundles and smartphone upsells for enterprise and consumer segments, respectively.\n    2. Invest in the marketplace channel internationally while reinforcing partner relationships in top-performing regions.\n    3. Monitor pricing experiments carefully, coupling them with targeted campaigns rather than broad discounts.\n    \"\"\"\n    print(summary)\n\nelse:\n    print(\"\\nSkipping analysis as DataFrame could not be loaded.\")\n</code></pre>"},{"location":"lessons/day-37-conclusion/","title":"\ud83c\udf89 Day 37: Conclusion & Your Journey Forward \ud83c\udf89","text":""},{"location":"lessons/day-37-conclusion/#congratulations","title":"Congratulations","text":"<p>You did it! You have successfully completed the core analytics track of the 50-Day Python for Business Analytics journey. Take a moment to appreciate how far you've come. You started with the absolute basics of Python and have progressed to loading, cleaning, analyzing, visualizing, and even serving data through an API. You now possess the foundational skill set of a modern data analyst.</p>"},{"location":"lessons/day-37-conclusion/#a-recap-of-your-journey","title":"A Recap of Your Journey","text":"<p>Let's look back at the critical skills you've acquired:</p> <ul> <li>Python Fundamentals (Days 1-14): You mastered the core building blocks of Python\u2014variables, data structures (lists, dictionaries), logic (conditionals), and automation (loops, functions)\u2014all through the lens of solving business problems.</li> <li>Business Analytics Toolkit (Days 15-34): You learned the essential tools of the trade. You can now use NumPy for high-performance calculations, wrangle and manipulate any dataset with Pandas, create beautiful static and interactive visualizations with Seaborn and Plotly, pull insights with foundational statistics, and work confidently with files, virtual environments, and databases.</li> <li>Application &amp; Sharing (Days 35-37): You explored the Flask web framework, built and consumed APIs, and delivered an end-to-end Capstone Case Study that mirrors the challenges analysts face in the real world.</li> </ul> <p>You have built a portfolio of projects throughout this course that demonstrates a practical, end-to-end understanding of the data analysis workflow.</p>"},{"location":"lessons/day-37-conclusion/#your-final-capstone-project-idea","title":"Your Final Capstone Project Idea","text":"<p>To truly solidify your skills, we recommend tackling one final, more comprehensive project. This project will combine everything you've learned.</p>"},{"location":"lessons/day-37-conclusion/#project-idea-the-interactive-sales-dashboard-api","title":"Project Idea: The Interactive Sales Dashboard API","text":"<ol> <li> <p>Data Backend:</p> </li> <li> <p>Use the <code>case_study_sales.csv</code> data from Day 36.</p> </li> <li> <p>Create a simple SQLite database and load the sales data into it.</p> </li> <li> <p>Flask API Server:</p> </li> <li> <p>Build a Flask API with the following endpoints:</p> <ul> <li><code>/api/summary</code>: Returns key metrics as JSON (e.g., total revenue, total units sold, number of transactions).</li> <li><code>/api/sales/by_product</code>: Returns the total revenue for each product as JSON.</li> <li><code>/api/sales/by_region</code>: Returns the total revenue for each region as JSON.</li> </ul> </li> <li> <p>Interactive Visualization Endpoint (Challenge):</p> </li> <li> <p>Create an endpoint like <code>/api/charts/revenue_by_region</code>.</p> </li> <li>This endpoint should generate an interactive bar chart with Plotly showing revenue by region.</li> <li>Instead of <code>fig.show()</code>, use <code>fig.to_html(full_html=False, include_plotlyjs='cdn')</code>. This returns the chart as an HTML string.</li> <li>Return this HTML string from your API endpoint. You can then view this by visiting the URL in your browser.</li> </ol>"},{"location":"lessons/day-37-conclusion/#whats-next","title":"What's Next?","text":"<p>Your journey doesn't end here. You now have the foundation to explore the most exciting fields in data\u2014and the remaining days of the 50-day program will take you even further. Here are some potential next steps:</p> <ul> <li>Machine Learning (Days 38-45): Use your Pandas skills as a launchpad to learn <code>scikit-learn</code>, the primary library for machine learning in Python. Start with concepts like Linear Regression and Classification.</li> <li>Deep Learning &amp; NLP (Days 46-50): Explore neural networks, computer vision, sequence models, and natural language processing to tackle cutting-edge analytics problems.</li> <li>Advanced Dashboarding: Learn dedicated dashboarding libraries like Dash (which is built on Flask and Plotly) or Streamlit to create powerful, interactive web applications for your analysis.</li> <li>Big Data Technologies: If your interest is in \"Big Data,\" you can now start to explore how tools like Apache Spark (with its Python API, PySpark) use similar concepts to analyze massive datasets that don't fit on a single computer.</li> <li>Cloud Computing: Learn how to run your Python scripts and deploy your APIs on cloud platforms like AWS, Google Cloud, or Azure.</li> </ul> <p>Thank you for your hard work and dedication throughout this course. You have invested in a skill that will provide immense value throughout your business career.</p> <p>Happy analyzing!</p>"},{"location":"lessons/day-37-conclusion/#additional-materials","title":"Additional Materials","text":"<ul> <li>conclusion.ipynb</li> </ul> conclusion.py <p>View on GitHub</p> conclusion.py<pre><code>\"\"\"Utility helpers for the Day 37 conclusion recap artifacts.\n\nThe functions in this module return plain Python data structures so they can\nbe easily unit tested or repurposed by downstream tooling. A small CLI is\nprovided to render the recap in the terminal for quick reference.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom argparse import ArgumentParser, Namespace\nfrom typing import Iterable, List, Sequence\n\n\ndef get_recap_checklist() -&gt; List[str]:\n    \"\"\"Return the high-level checklist summarising the program highlights.\"\"\"\n\n    return [\n        \"Review the Python foundations covered across the 50-day journey.\",\n        \"Revisit data analytics workflows, from cleaning through visualization.\",\n        \"Connect the dots between statistical thinking and business strategy.\",\n        \"Reflect on automation opportunities identified during the lessons.\",\n    ]\n\n\ndef get_next_steps() -&gt; List[dict]:\n    \"\"\"Return recommended actions after completing the curriculum.\"\"\"\n\n    return [\n        {\n            \"title\": \"Build a portfolio project\",\n            \"description\": (\n                \"Apply the analytics-to-insights pipeline on a business dataset \"\n                \"and document the impact for stakeholders.\"\n            ),\n        },\n        {\n            \"title\": \"Deepen machine learning skills\",\n            \"description\": (\n                \"Experiment with supervised and unsupervised models using the \"\n                \"frameworks introduced in later lessons.\"\n            ),\n        },\n        {\n            \"title\": \"Share knowledge with peers\",\n            \"description\": (\n                \"Host a lunch-and-learn or internal workshop to reinforce your \"\n                \"understanding and surface collaboration ideas.\"\n            ),\n        },\n    ]\n\n\ndef _format_checklist(checklist: Sequence[str]) -&gt; Iterable[str]:\n    for item in checklist:\n        yield f\" - {item}\"\n\n\ndef _format_next_steps(next_steps: Sequence[dict]) -&gt; Iterable[str]:\n    for index, step in enumerate(next_steps, start=1):\n        title = step.get(\"title\", f\"Step {index}\")\n        description = step.get(\"description\", \"\")\n        if description:\n            yield f\"{index}. {title}: {description}\"\n        else:\n            yield f\"{index}. {title}\"\n\n\ndef build_parser() -&gt; ArgumentParser:\n    parser = ArgumentParser(\n        description=(\n            \"Render the Coding for MBA Day 37 recap including the core \"\n            \"checklist and suggested next steps.\"\n        )\n    )\n    parser.add_argument(\n        \"--section\",\n        choices=(\"checklist\", \"next-steps\", \"all\"),\n        default=\"all\",\n        help=\"Choose which recap section to display.\",\n    )\n    return parser\n\n\ndef _render(section: str) -&gt; str:\n    if section == \"checklist\":\n        lines = [\"Day 37 Recap Checklist:\", *_format_checklist(get_recap_checklist())]\n    elif section == \"next-steps\":\n        lines = [\n            \"Recommended Next Steps:\",\n            *_format_next_steps(get_next_steps()),\n        ]\n    else:\n        lines = [\n            \"Day 37 Recap Checklist:\",\n            *_format_checklist(get_recap_checklist()),\n            \"\",\n            \"Recommended Next Steps:\",\n            *_format_next_steps(get_next_steps()),\n        ]\n    return \"\\n\".join(lines)\n\n\ndef main(argv: Sequence[str] | None = None) -&gt; None:\n    \"\"\"Entry point for the command line interface.\"\"\"\n\n    parser = build_parser()\n    args: Namespace = parser.parse_args(list(argv) if argv is not None else None)\n    print(_render(args.section))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"lessons/day-38-linear-algebra/","title":"Day 38: Math Foundations - Linear Algebra","text":""},{"location":"lessons/day-38-linear-algebra/#overview","title":"Overview","text":"<p>Linear algebra underpins much of machine learning. This lesson revisits the building blocks\u2014vectors, matrices, and eigendecomposition\u2014so you can reason about data transformations and model behaviour with confidence.</p>"},{"location":"lessons/day-38-linear-algebra/#key-concepts","title":"Key Concepts","text":"<ul> <li>Vectors: Mathematical objects with magnitude and direction that often represent individual data points or feature sets.</li> <li>Matrices: Rectangular arrays that hold datasets and enable transformations when combined with vectors or other matrices.</li> <li>Dot Product: A measure of similarity between two vectors computed by multiplying corresponding elements and summing the results.</li> <li>Eigenvectors and Eigenvalues: Special vector\u2013scalar pairs that describe how a matrix stretches space; essential for dimensionality reduction techniques such as PCA.</li> </ul>"},{"location":"lessons/day-38-linear-algebra/#practice-exercises","title":"Practice Exercises","text":"<ol> <li>Vector Operations: Use <code>vector_operations()</code> to compute the sum, difference, and dot product for <code>[1, 2, 3]</code> and <code>[4, 5, 6]</code>.</li> <li>Matrix Operations: Call <code>matrix_operations()</code> to produce the sum and product for the matrices <code>[[1, 2], [3, 4]]</code> and <code>[[5, 6], [7, 8]]</code>.</li> <li>Eigen-analysis: Explore the eigenvalues and eigenvectors of <code>[[4, 1], [2, 3]]</code> via <code>eigen_analysis()</code>.</li> </ol>"},{"location":"lessons/day-38-linear-algebra/#how-to-use-this-folder","title":"How to Use This Folder","text":"<ul> <li>Run the worked examples: <code>python Day_38_Linear_Algebra/solutions.py</code></li> <li>Execute the automated checks: <code>pytest tests/test_day_38.py</code></li> </ul>"},{"location":"lessons/day-38-linear-algebra/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>import numpy as np\n\n\ndef vector_operations(v1=None, v2=None):\n    \"\"\"Return the sum, difference, and dot product of two vectors.\"\"\"\n\n    v1_array = np.array([1, 2, 3]) if v1 is None else np.asarray(v1)\n    v2_array = np.array([4, 5, 6]) if v2 is None else np.asarray(v2)\n\n    vector_sum = v1_array + v2_array\n    vector_difference = v1_array - v2_array\n    dot_product = np.dot(v1_array, v2_array)\n\n    return vector_sum, vector_difference, dot_product\n\n\ndef matrix_operations(m1=None, m2=None):\n    \"\"\"Return the sum and product of two matrices.\"\"\"\n\n    m1_array = np.array([[1, 2], [3, 4]]) if m1 is None else np.asarray(m1)\n    m2_array = np.array([[5, 6], [7, 8]]) if m2 is None else np.asarray(m2)\n\n    matrix_sum = m1_array + m2_array\n    matrix_product = np.dot(m1_array, m2_array)\n\n    return matrix_sum, matrix_product\n\n\ndef eigen_analysis(matrix=None):\n    \"\"\"Return the eigenvalues and eigenvectors for the provided matrix.\"\"\"\n\n    matrix_array = np.array([[4, 1], [2, 3]]) if matrix is None else np.asarray(matrix)\n    eigenvalues, eigenvectors = np.linalg.eig(matrix_array)\n    return eigenvalues, eigenvectors\n\n\ndef main():\n    print(\"--- Vector Operations ---\")\n    vector_sum, vector_difference, dot_product = vector_operations()\n    print(f\"Sum of v1 and v2: {vector_sum}\")\n    print(f\"Difference of v1 and v2: {vector_difference}\")\n    print(f\"Dot product of v1 and v2: {dot_product}\")\n    print(\"-\" * 25)\n\n    print(\"\\n--- Matrix Operations ---\")\n    matrix_sum, matrix_product = matrix_operations()\n    print(f\"Sum of M1 and M2:\\n{matrix_sum}\")\n    print(f\"Product of M1 and M2:\\n{matrix_product}\")\n    print(\"-\" * 25)\n\n    print(\"\\n--- Eigen-analysis ---\")\n    eigenvalues, eigenvectors = eigen_analysis()\n    print(f\"Matrix A:\\n{np.array([[4, 1], [2, 3]])}\")\n    print(f\"Eigenvalues: {eigenvalues}\")\n    print(f\"Eigenvectors:\\n{eigenvectors}\")\n    print(\"-\" * 25)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"lessons/day-39-calculus/","title":"Day 39: Math Foundations - Calculus","text":""},{"location":"lessons/day-39-calculus/#overview","title":"Overview","text":"<p>Calculus powers the optimisation routines that train machine learning models. Derivatives, gradients, and the chain rule reveal how model parameters should change to reduce loss functions.</p>"},{"location":"lessons/day-39-calculus/#key-concepts","title":"Key Concepts","text":"<ul> <li>Derivatives: Measure how a function changes with respect to its input; the slope guides the direction of parameter updates.</li> <li>Gradients: Vectors of partial derivatives for multivariate functions that point toward steepest ascent. Optimisation algorithms step in the opposite direction.</li> <li>Chain Rule: Provides the derivative of nested functions and forms the backbone of backpropagation in neural networks.</li> </ul>"},{"location":"lessons/day-39-calculus/#practice-exercises","title":"Practice Exercises","text":"<ol> <li>Symbolic Differentiation: <code>symbolic_derivative()</code> returns a polynomial and its derivative using SymPy.</li> <li>Numerical Gradient: <code>numerical_gradient()</code> estimates the gradient of <code>x^2*y + y^3</code> at <code>(2, 3)</code> via finite differences.</li> <li>Chain Rule Application: <code>chain_rule_derivative()</code> expands <code>(2x + 1)^2</code> and differentiates it with respect to <code>x</code>.</li> </ol>"},{"location":"lessons/day-39-calculus/#how-to-use-this-folder","title":"How to Use This Folder","text":"<ul> <li>Run the worked examples: <code>python Day_39_Calculus/solutions.py</code></li> <li>Execute the automated checks: <code>pytest tests/test_day_39.py</code></li> </ul>"},{"location":"lessons/day-39-calculus/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>import numpy as np\nimport sympy as sp\n\n\ndef symbolic_derivative():\n    \"\"\"Return a polynomial and its derivative.\"\"\"\n\n    x_symbol = sp.Symbol(\"x\")\n    polynomial = x_symbol**3 - 2 * x_symbol**2 + 5\n    derivative = sp.diff(polynomial, x_symbol)\n    return polynomial, derivative\n\n\ndef numerical_gradient(x_val=2.0, y_val=3.0, step=1e-6):\n    \"\"\"Return the numerical gradient of f(x, y) = x^2 * y + y^3 at a point.\"\"\"\n\n    def f(x, y):\n        return x**2 * y + y**3\n\n    df_dx = (f(x_val + step, y_val) - f(x_val, y_val)) / step\n    df_dy = (f(x_val, y_val + step) - f(x_val, y_val)) / step\n    return np.array([df_dx, df_dy])\n\n\ndef chain_rule_derivative():\n    \"\"\"Return the composite function y(x) and its derivative using the chain rule.\"\"\"\n\n    x_symbol = sp.Symbol(\"x\")\n    u_expression = 2 * x_symbol + 1\n    y_expression = (u_expression) ** 2\n    derivative = sp.diff(y_expression, x_symbol)\n    return sp.expand(y_expression), sp.expand(derivative)\n\n\ndef main():\n    polynomial, derivative = symbolic_derivative()\n    print(\"--- Symbolic Differentiation ---\")\n    print(f\"Original function f(x): {polynomial}\")\n    print(f\"Derivative f'(x): {derivative}\")\n    print(\"-\" * 30)\n\n    gradient = numerical_gradient()\n    print(\"\\n--- Numerical Gradient ---\")\n    print(\"Function f(x, y) = x^2 * y + y^3\")\n    print(f\"Numerical gradient at (2, 3): {gradient.tolist()}\")\n    print(\"Analytical gradient: [12, 31]\")\n    print(\"-\" * 30)\n\n    composite, composite_derivative = chain_rule_derivative()\n    print(\"\\n--- Chain Rule Application ---\")\n    print(f\"Composite function y(x): {composite}\")\n    print(f\"Derivative dy/dx: {composite_derivative}\")\n    print(\"-\" * 30)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"lessons/day-40-intro-to-ml/","title":"Day 40: Introduction to Machine Learning & Core Concepts","text":""},{"location":"lessons/day-40-intro-to-ml/#overview","title":"Overview","text":"<p>This lesson introduces the machine learning workflow and highlights how evaluation techniques ensure reliable models. You'll generate data, configure cross-validation, train a linear model, and compute mean squared error metrics.</p> <p>Prerequisites: Install scikit-learn with <code>pip install scikit-learn</code> before running the exercises.</p>"},{"location":"lessons/day-40-intro-to-ml/#key-concepts","title":"Key Concepts","text":"<ul> <li>Learning Paradigms: Supervised, unsupervised, and reinforcement learning cover most ML problems and inform how we collect data and labels.</li> <li>Bias\u2013Variance Trade-off: Balances model simplicity and flexibility; too much bias underfits, too much variance overfits.</li> <li>Cross-Validation: Splits data into multiple folds so every observation is used for both training and validation, yielding a robust performance estimate.</li> </ul>"},{"location":"lessons/day-40-intro-to-ml/#practice-exercises","title":"Practice Exercises","text":"<ol> <li>Dataset Generation: <code>generate_dataset()</code> creates a noisy linear regression dataset for experimentation.</li> <li>Cross-Validation Setup: <code>setup_kfold()</code> configures the resampling strategy, while <code>train_linear_regression()</code> and <code>evaluate_model()</code> encapsulate model training and scoring.</li> <li>Model Assessment: <code>cross_validate_model()</code> runs the full loop and reports per-fold and average mean squared error.</li> </ol>"},{"location":"lessons/day-40-intro-to-ml/#how-to-use-this-folder","title":"How to Use This Folder","text":"<ul> <li>Run the worked examples: <code>python Day_40_Intro_to_ML/solutions.py</code></li> <li>Execute the automated checks: <code>pytest tests/test_day_40.py</code></li> </ul>"},{"location":"lessons/day-40-intro-to-ml/#whats-next","title":"What's next?","text":"<p>Explore the full Machine Learning Curriculum Roadmap to see how the Day 40 lesson fits into a multi-phase path covering deep learning, responsible AI, and MLOps.</p>"},{"location":"lessons/day-40-intro-to-ml/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n\ndef generate_dataset(\n    num_samples=100, start=0.0, stop=10.0, noise_scale=3.0, random_state=None\n):\n    \"\"\"Generate a simple linear dataset with Gaussian noise.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    X = np.linspace(start, stop, num_samples, dtype=float).reshape(-1, 1)\n    noise = rng.normal(0.0, noise_scale, num_samples)\n    y = 3 * X.flatten() + 5 + noise\n    return X, y\n\n\ndef setup_kfold(n_splits=5, shuffle=True, random_state=42):\n    \"\"\"Return a configured KFold cross-validator.\"\"\"\n\n    return KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n\n\ndef train_linear_regression(X_train, y_train):\n    \"\"\"Train and return a LinearRegression model.\"\"\"\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    return model\n\n\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"Return the mean squared error of predictions.\"\"\"\n\n    predictions = model.predict(X_test)\n    return mean_squared_error(y_test, predictions)\n\n\ndef cross_validate_model(X, y, kfold=None):\n    \"\"\"Perform k-fold cross-validation and return per-fold and average MSEs.\"\"\"\n\n    if kfold is None:\n        kfold = setup_kfold()\n\n    mse_scores = []\n    for train_index, test_index in kfold.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        model = train_linear_regression(X_train, y_train)\n        mse_scores.append(evaluate_model(model, X_test, y_test))\n\n    return mse_scores, float(np.mean(mse_scores))\n\n\ndef main():\n    X, y = generate_dataset()\n    print(f\"Generated a dataset with {X.shape[0]} samples.\")\n    print(\"-\" * 30)\n\n    print(\"Performing 5-fold cross-validation...\")\n    mse_scores, average_mse = cross_validate_model(X, y)\n    for fold, mse in enumerate(mse_scores, start=1):\n        print(f\"Fold {fold}: MSE = {mse:.4f}\")\n\n    print(\"-\" * 30)\n    print(f\"Average MSE across 5 folds: {average_mse:.4f}\")\n    print(\n        \"This average score is a more robust estimate of how the model will perform on unseen data.\"\n    )\n    print(\"-\" * 30)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"lessons/day-41-supervised-learning-regression/","title":"Day 41 \u00b7 Supervised Learning \u2013 Regression","text":""},{"location":"lessons/day-41-supervised-learning-regression/#whats-in-this-folder","title":"What's in this folder?","text":"<ul> <li><code>solutions.py</code> \u2013 modular helpers for generating synthetic regression data, training a linear regression model, evaluating it, and saving visualisations.</li> <li><code>tests/test_day_41.py</code> \u2013 pytest coverage for the helper functions and the end-to-end demo workflow.</li> </ul>"},{"location":"lessons/day-41-supervised-learning-regression/#how-to-run-the-demo","title":"How to run the demo","text":"<pre><code>python Day_41_Supervised_Learning_Regression/solutions.py\n</code></pre> <p>This command prints the evaluation metrics and saves <code>regression_fit.png</code> in the working directory.</p>"},{"location":"lessons/day-41-supervised-learning-regression/#key-functions","title":"Key functions","text":"Function Description <code>generate_regression_data</code> Creates a deterministic dataset using NumPy for reproducible experimentation. <code>train_regression_model</code> Fits a <code>LinearRegression</code> model on the provided training split. <code>make_regression_predictions</code> Returns predictions for a trained model. <code>evaluate_regression_model</code> Computes Mean Squared Error (MSE) and the coefficient of determination (R\u00b2). <code>plot_regression_results</code> Saves a scatter plot with the fitted regression line to disk. <code>run_linear_regression_demo</code> Orchestrates the full pipeline and returns the evaluation metrics."},{"location":"lessons/day-41-supervised-learning-regression/#tests","title":"Tests","text":"<p>Run the regression unit tests with:</p> <pre><code>pytest tests/test_day_41.py\n</code></pre>"},{"location":"lessons/day-41-supervised-learning-regression/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Reusable helpers for the Day 41 regression lesson.\n\nThe module exposes composable utilities to generate synthetic data,\ntrain a linear regression model, evaluate it, and optionally persist\nvisualisations.  When executed as a script it will run the full demo and\nsave a regression plot to ``regression_fit.png``.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef generate_regression_data(\n    n_samples: int = 100,\n    slope: float = 2.5,\n    intercept: float = 10.0,\n    noise_std: float = 1.0,\n    random_state: int = 42,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Create a deterministic synthetic regression dataset.\"\"\"\n    rng = np.random.default_rng(random_state)\n    X = 2 * rng.random((n_samples, 1))\n    noise = rng.normal(0.0, noise_std, n_samples)\n    y = intercept + slope * X.flatten() + noise\n    return X, y\n\n\ndef split_regression_data(\n    X: np.ndarray,\n    y: np.ndarray,\n    test_size: float = 0.2,\n    random_state: int = 42,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Split features and labels into train/test sets.\"\"\"\n    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n\ndef train_regression_model(\n    X_train: np.ndarray, y_train: np.ndarray\n) -&gt; LinearRegression:\n    \"\"\"Fit a :class:`~sklearn.linear_model.LinearRegression` model.\"\"\"\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    return model\n\n\ndef make_regression_predictions(model: LinearRegression, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return predictions for the provided features.\"\"\"\n    return model.predict(X)\n\n\ndef evaluate_regression_model(\n    y_true: np.ndarray, y_pred: np.ndarray\n) -&gt; Dict[str, float]:\n    \"\"\"Calculate common regression metrics for the provided predictions.\"\"\"\n    return {\n        \"mse\": mean_squared_error(y_true, y_pred),\n        \"r2\": r2_score(y_true, y_pred),\n    }\n\n\ndef plot_regression_results(\n    X: np.ndarray,\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    filepath: str | Path = \"regression_fit.png\",\n) -&gt; Tuple[plt.Figure, Path]:\n    \"\"\"Create and save a scatter/line plot of the regression fit.\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.scatter(X, y_true, color=\"blue\", label=\"Actual values\")\n    # Sorting ensures the regression line is displayed correctly.\n    sorted_indices = np.argsort(X.flatten())\n    ax.plot(\n        X.flatten()[sorted_indices],\n        y_pred[sorted_indices],\n        color=\"red\",\n        linewidth=2,\n        label=\"Regression line\",\n    )\n    ax.set_title(\"Linear Regression Fit\")\n    ax.set_xlabel(\"Independent Variable (X)\")\n    ax.set_ylabel(\"Dependent Variable (y)\")\n    ax.grid(True)\n    ax.legend()\n\n    output_path = Path(filepath)\n    fig.savefig(output_path)\n    return fig, output_path\n\n\ndef run_linear_regression_demo(\n    save_path: str | Path = \"regression_fit.png\",\n) -&gt; Dict[str, float]:\n    \"\"\"Execute the full demo workflow and return evaluation metrics.\"\"\"\n    X, y = generate_regression_data()\n    X_train, X_test, y_train, y_test = split_regression_data(X, y)\n    model = train_regression_model(X_train, y_train)\n    y_pred = make_regression_predictions(model, X_test)\n    metrics = evaluate_regression_model(y_test, y_pred)\n    plot_regression_results(X_test, y_test, y_pred, filepath=save_path)\n    return metrics\n\n\nif __name__ == \"__main__\":\n    metrics = run_linear_regression_demo()\n    print(\"--- Linear Regression Example ---\")\n    print(\"Generated a dataset with 100 samples.\")\n    print(\"Training set size: 80 samples\")\n    print(\"Testing set size: 20 samples\")\n    print(\"-\" * 30)\n    print(f\"Learned metrics -&gt; MSE: {metrics['mse']:.4f}, R^2: {metrics['r2']:.4f}\")\n    print(\"Saved a plot of the regression fit to 'regression_fit.png'\")\n</code></pre>"},{"location":"lessons/day-42-supervised-learning-classification-part-1/","title":"Day 42 \u00b7 Supervised Learning \u2013 Classification (Part 1)","text":""},{"location":"lessons/day-42-supervised-learning-classification-part-1/#whats-in-this-folder","title":"What's in this folder?","text":"<ul> <li><code>solutions.py</code> \u2013 reusable helpers for loading the Iris dataset, scaling features, and training logistic regression and KNN classifiers with deterministic settings.</li> <li><code>tests/test_day_42.py</code> \u2013 pytest coverage for the dataset preparation utilities and classification metrics.</li> </ul>"},{"location":"lessons/day-42-supervised-learning-classification-part-1/#how-to-run-the-demo","title":"How to run the demo","text":"<pre><code>python Day_42_Supervised_Learning_Classification_Part_1/solutions.py\n</code></pre> <p>This command trains both classifiers and prints their accuracy scores.</p>"},{"location":"lessons/day-42-supervised-learning-classification-part-1/#key-functions","title":"Key functions","text":"Function Description <code>load_and_prepare_iris</code> Splits the Iris dataset and returns both raw and standardised feature matrices. <code>train_logistic_regression</code> Fits a multi-class logistic regression model with a deterministic random seed. <code>train_knn_classifier</code> Trains a K-Nearest Neighbours classifier on the scaled features. <code>evaluate_classifier</code> Computes the accuracy score for a trained classifier. <code>run_classification_demo</code> Executes the full workflow and returns a metrics dictionary for both models."},{"location":"lessons/day-42-supervised-learning-classification-part-1/#tests","title":"Tests","text":"<p>Run the classification unit tests with:</p> <pre><code>pytest tests/test_day_42.py\n</code></pre>"},{"location":"lessons/day-42-supervised-learning-classification-part-1/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Reusable helpers for logistic regression and KNN on the Iris dataset.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n\n@dataclass\nclass IrisData:\n    X_train: np.ndarray\n    X_test: np.ndarray\n    y_train: np.ndarray\n    y_test: np.ndarray\n    X_train_scaled: np.ndarray\n    X_test_scaled: np.ndarray\n    scaler: StandardScaler\n\n\ndef load_and_prepare_iris(\n    test_size: float = 0.3,\n    random_state: int = 42,\n) -&gt; IrisData:\n    \"\"\"Load the Iris dataset and return scaled/unscaled splits.\"\"\"\n    iris = load_iris()\n    X_train, X_test, y_train, y_test = train_test_split(\n        iris.data,\n        iris.target,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=iris.target,\n    )\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    return IrisData(\n        X_train=X_train,\n        X_test=X_test,\n        y_train=y_train,\n        y_test=y_test,\n        X_train_scaled=X_train_scaled,\n        X_test_scaled=X_test_scaled,\n        scaler=scaler,\n    )\n\n\ndef train_logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    *,\n    random_state: int = 42,\n    max_iter: int = 200,\n) -&gt; LogisticRegression:\n    \"\"\"Train a logistic regression classifier with deterministic settings.\"\"\"\n    model = LogisticRegression(random_state=random_state, max_iter=max_iter)\n    model.fit(X_train, y_train)\n    return model\n\n\ndef train_knn_classifier(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    *,\n    n_neighbors: int = 5,\n) -&gt; KNeighborsClassifier:\n    \"\"\"Train a K-Nearest Neighbours classifier.\"\"\"\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    model.fit(X_train, y_train)\n    return model\n\n\ndef evaluate_classifier(\n    model,\n    X_test: np.ndarray,\n    y_test: np.ndarray,\n) -&gt; Dict[str, float]:\n    \"\"\"Return a dictionary of evaluation metrics for the classifier.\"\"\"\n    accuracy = accuracy_score(y_test, model.predict(X_test))\n    return {\"accuracy\": accuracy}\n\n\ndef run_classification_demo() -&gt; Dict[str, Dict[str, float]]:\n    \"\"\"Run the Day 42 classification demo and return the metrics.\"\"\"\n    data = load_and_prepare_iris()\n    log_reg = train_logistic_regression(data.X_train_scaled, data.y_train)\n    knn = train_knn_classifier(data.X_train_scaled, data.y_train)\n    return {\n        \"logistic_regression\": evaluate_classifier(\n            log_reg, data.X_test_scaled, data.y_test\n        ),\n        \"knn\": evaluate_classifier(knn, data.X_test_scaled, data.y_test),\n    }\n\n\nif __name__ == \"__main__\":\n    metrics = run_classification_demo()\n    print(\"--- Classification Example on Iris Dataset ---\")\n    print(\"Training Logistic Regression and KNN models...\")\n    for model_name, model_metrics in metrics.items():\n        print(\n            f\"{model_name.replace('_', ' ').title()} accuracy: {model_metrics['accuracy']:.4f}\"\n        )\n</code></pre>"},{"location":"lessons/day-43-supervised-learning-classification-part-2/","title":"Day 43 \u00b7 Supervised Learning \u2013 Classification (Part 2)","text":""},{"location":"lessons/day-43-supervised-learning-classification-part-2/#whats-in-this-folder","title":"What's in this folder?","text":"<ul> <li><code>solutions.py</code> \u2013 modular helpers for preparing the Iris dataset, fitting SVM and decision tree classifiers, and evaluating them.</li> <li><code>tests/test_day_43.py</code> \u2013 pytest checks covering dataset preparation and accuracy scoring for both models.</li> </ul>"},{"location":"lessons/day-43-supervised-learning-classification-part-2/#how-to-run-the-demo","title":"How to run the demo","text":"<pre><code>python Day_43_Supervised_Learning_Classification_Part_2/solutions.py\n</code></pre> <p>This command trains the SVM and decision tree models, printing their accuracy scores.</p>"},{"location":"lessons/day-43-supervised-learning-classification-part-2/#key-functions","title":"Key functions","text":"Function Description <code>load_and_prepare_iris</code> Splits and scales the Iris dataset for use with multiple classifiers. <code>train_svm_classifier</code> Trains a deterministic SVM classifier using the RBF kernel. <code>train_decision_tree_classifier</code> Fits a decision tree classifier with a fixed random seed. <code>evaluate_classifier</code> Computes accuracy for a fitted classifier. <code>run_classification_demo</code> Executes the full workflow and returns accuracy metrics for both models."},{"location":"lessons/day-43-supervised-learning-classification-part-2/#tests","title":"Tests","text":"<p>Run the advanced classification unit tests with:</p> <pre><code>pytest tests/test_day_43.py\n</code></pre>"},{"location":"lessons/day-43-supervised-learning-classification-part-2/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Reusable helpers for SVM and Decision Tree classifiers on the Iris dataset.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n@dataclass\nclass IrisData:\n    X_train: np.ndarray\n    X_test: np.ndarray\n    y_train: np.ndarray\n    y_test: np.ndarray\n    X_train_scaled: np.ndarray\n    X_test_scaled: np.ndarray\n    scaler: StandardScaler\n\n\ndef load_and_prepare_iris(\n    test_size: float = 0.3,\n    random_state: int = 42,\n) -&gt; IrisData:\n    \"\"\"Load the Iris dataset and return scaled/unscaled splits.\"\"\"\n    iris = load_iris()\n    X_train, X_test, y_train, y_test = train_test_split(\n        iris.data,\n        iris.target,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=iris.target,\n    )\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    return IrisData(\n        X_train=X_train,\n        X_test=X_test,\n        y_train=y_train,\n        y_test=y_test,\n        X_train_scaled=X_train_scaled,\n        X_test_scaled=X_test_scaled,\n        scaler=scaler,\n    )\n\n\ndef train_svm_classifier(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    *,\n    kernel: str = \"rbf\",\n    random_state: int = 42,\n) -&gt; SVC:\n    \"\"\"Train an SVM classifier with deterministic hyperparameters.\"\"\"\n    model = SVC(kernel=kernel, random_state=random_state)\n    model.fit(X_train, y_train)\n    return model\n\n\ndef train_decision_tree_classifier(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    *,\n    random_state: int = 42,\n) -&gt; DecisionTreeClassifier:\n    \"\"\"Train a decision tree classifier.\"\"\"\n    model = DecisionTreeClassifier(random_state=random_state)\n    model.fit(X_train, y_train)\n    return model\n\n\ndef evaluate_classifier(\n    model,\n    X_test: np.ndarray,\n    y_test: np.ndarray,\n) -&gt; Dict[str, float]:\n    \"\"\"Return a dictionary of evaluation metrics for the classifier.\"\"\"\n    accuracy = accuracy_score(y_test, model.predict(X_test))\n    return {\"accuracy\": accuracy}\n\n\ndef run_classification_demo() -&gt; Dict[str, Dict[str, float]]:\n    \"\"\"Run the Day 43 classification demo and return the metrics.\"\"\"\n    data = load_and_prepare_iris()\n    svm_model = train_svm_classifier(data.X_train_scaled, data.y_train)\n    tree_model = train_decision_tree_classifier(data.X_train, data.y_train)\n    return {\n        \"svm\": evaluate_classifier(svm_model, data.X_test_scaled, data.y_test),\n        \"decision_tree\": evaluate_classifier(tree_model, data.X_test, data.y_test),\n    }\n\n\nif __name__ == \"__main__\":\n    metrics = run_classification_demo()\n    print(\"--- Advanced Classification on Iris Dataset ---\")\n    for model_name, model_metrics in metrics.items():\n        print(\n            f\"{model_name.replace('_', ' ').title()} accuracy: {model_metrics['accuracy']:.4f}\"\n        )\n</code></pre>"},{"location":"lessons/day-44-unsupervised-learning/","title":"Day 44: Unsupervised Learning","text":""},{"location":"lessons/day-44-unsupervised-learning/#overview","title":"Overview","text":"<p>Day 44 introduces two foundational unsupervised learning workflows:</p> <ul> <li>K-Means clustering groups unlabeled observations into clusters using   distance to learned centroids.</li> <li>Principal Component Analysis (PCA) compresses high-dimensional data   into a smaller number of orthogonal components for easier visualization   and downstream modeling.</li> </ul> <p>Install scikit-learn before running the examples:</p> <pre><code>pip install scikit-learn\n</code></pre>"},{"location":"lessons/day-44-unsupervised-learning/#whats-inside","title":"What's inside","text":"<ul> <li><code>solutions.py</code> \u2013 reusable functions for generating blob data, fitting   K-Means, and projecting datasets with PCA. Executing the file still   creates the original visualisations.</li> <li><code>README.md</code> \u2013 lesson summary (this document).</li> </ul>"},{"location":"lessons/day-44-unsupervised-learning/#running-the-lesson-script","title":"Running the lesson script","text":"<p>Execute the scripted walkthrough, which will save clustering and PCA plots to the project directory:</p> <pre><code>python Day_44_Unsupervised_Learning/solutions.py\n</code></pre>"},{"location":"lessons/day-44-unsupervised-learning/#running-the-tests","title":"Running the tests","text":"<p>Automated tests validate the reusable helpers. Run just the Day 44 checks with:</p> <pre><code>pytest tests/test_day_44.py\n</code></pre> <p>To execute the entire suite, simply call <code>pytest</code> from the repository root.</p>"},{"location":"lessons/day-44-unsupervised-learning/#further-exploration","title":"Further exploration","text":"<ul> <li>Experiment with different numbers of clusters in <code>fit_kmeans</code> to observe   how centroids move.</li> <li>Try increasing the number of PCA components and inspect the cumulative   explained variance to decide how many dimensions to keep.</li> </ul>"},{"location":"lessons/day-44-unsupervised-learning/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Reusable utilities for Day 44 unsupervised learning demos.\"\"\"\n\nfrom typing import Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris, make_blobs\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef generate_blobs(\n    n_samples: int = 300,\n    centers: int = 4,\n    cluster_std: float = 0.7,\n    random_state: int | None = 42,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Create a synthetic blob dataset for clustering demonstrations.\"\"\"\n\n    return make_blobs(\n        n_samples=n_samples,\n        centers=centers,\n        cluster_std=cluster_std,\n        random_state=random_state,\n    )\n\n\ndef fit_kmeans(\n    X: np.ndarray,\n    n_clusters: int = 4,\n    random_state: int | None = 42,\n    n_init: int = 10,\n) -&gt; Tuple[KMeans, np.ndarray, np.ndarray]:\n    \"\"\"Fit a K-Means model and return the estimator, labels, and cluster centers.\"\"\"\n\n    model = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    labels = model.fit_predict(X)\n    return model, labels, model.cluster_centers_\n\n\ndef load_iris_data() -&gt; Tuple[np.ndarray, np.ndarray, list[str]]:\n    \"\"\"Load the Iris dataset along with target values and class labels.\"\"\"\n\n    iris = load_iris()\n    return iris.data, iris.target, iris.target_names.tolist()\n\n\ndef run_pca(\n    X: np.ndarray,\n    n_components: int = 2,\n) -&gt; Tuple[np.ndarray, PCA, StandardScaler]:\n    \"\"\"Scale the features, run PCA, and return the transformed data with models.\"\"\"\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    pca = PCA(n_components=n_components)\n    transformed = pca.fit_transform(X_scaled)\n    return transformed, pca, scaler\n\n\ndef _plot_kmeans_results(\n    X: np.ndarray, labels: np.ndarray, centers: np.ndarray\n) -&gt; None:\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap=\"viridis\", label=\"Data Points\")\n    plt.scatter(\n        centers[:, 0],\n        centers[:, 1],\n        c=\"red\",\n        s=200,\n        alpha=0.75,\n        marker=\"X\",\n        label=\"Centroids\",\n    )\n    plt.title(\"K-Means Clustering\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(\"kmeans_clusters.png\")\n\n\ndef _plot_pca_results(\n    transformed: np.ndarray, targets: np.ndarray, target_names: list[str]\n) -&gt; None:\n    plt.figure(figsize=(10, 6))\n    scatter = plt.scatter(\n        transformed[:, 0],\n        transformed[:, 1],\n        c=targets,\n        cmap=\"viridis\",\n        edgecolor=\"k\",\n        s=60,\n    )\n    plt.title(\"PCA of Iris Dataset\")\n    plt.xlabel(\"First Principal Component\")\n    plt.ylabel(\"Second Principal Component\")\n    plt.legend(handles=scatter.legend_elements()[0], labels=target_names)\n    plt.grid(True)\n    plt.savefig(\"pca_iris.png\")\n\n\nif __name__ == \"__main__\":\n    print(\"--- K-Means Clustering Example ---\")\n    blobs, _ = generate_blobs()\n    model, blob_labels, centers = fit_kmeans(blobs)\n    print(\"K-Means applied to synthetic data with 4 clusters.\")\n    print(f\"Cluster centers:\\n{centers}\")\n    _plot_kmeans_results(blobs, blob_labels, centers)\n    print(\"Saved K-Means visualization to 'kmeans_clusters.png'\")\n    print(\"-\" * 30)\n\n    print(\"\\n--- PCA Example ---\")\n    iris_features, iris_target, iris_names = load_iris_data()\n    transformed, pca_model, _ = run_pca(iris_features)\n    print(f\"Original shape: {iris_features.shape}\")\n    print(f\"Shape after PCA: {transformed.shape}\")\n    explained_variance = pca_model.explained_variance_ratio_\n    print(f\"Explained variance by component: {explained_variance}\")\n    total_variance = explained_variance.sum() * 100\n    print(f\"Total variance explained by 2 components: {total_variance:.2f}%\")\n    _plot_pca_results(transformed, iris_target, iris_names)\n    print(\"Saved PCA visualization to 'pca_iris.png'\")\n    print(\"-\" * 30)\n</code></pre>"},{"location":"lessons/day-45-feature-engineering-and-evaluation/","title":"Day 45: Feature Engineering & Model Evaluation","text":""},{"location":"lessons/day-45-feature-engineering-and-evaluation/#overview","title":"Overview","text":"<p>Day 45 demonstrates how thoughtful preprocessing and rigorous evaluation combine to build trustworthy models:</p> <ul> <li>Feature engineering pipelines clean, scale, and encode raw columns so   downstream estimators receive consistent numeric inputs.</li> <li>Evaluation workflows compare predictions against held-out data using   confusion matrices and rich classification reports.</li> </ul> <p>Install scikit-learn before exploring the examples:</p> <pre><code>pip install scikit-learn\n</code></pre>"},{"location":"lessons/day-45-feature-engineering-and-evaluation/#whats-inside","title":"What's inside","text":"<ul> <li><code>solutions.py</code> \u2013 helper functions for assembling preprocessing pipelines,   transforming the toy dataset, training a logistic regression model, and   returning evaluation metrics.</li> <li><code>README.md</code> \u2013 lesson overview (this document).</li> </ul>"},{"location":"lessons/day-45-feature-engineering-and-evaluation/#running-the-lesson-script","title":"Running the lesson script","text":"<p>Execute the end-to-end walkthrough, which prints processed feature arrays, confusion matrix details, and a classification report:</p> <pre><code>python Day_45_Feature_Engineering_and_Evaluation/solutions.py\n</code></pre>"},{"location":"lessons/day-45-feature-engineering-and-evaluation/#running-the-tests","title":"Running the tests","text":"<p>Run the dedicated Day 45 tests to validate the preprocessing and evaluation utilities:</p> <pre><code>pytest tests/test_day_45.py\n</code></pre> <p>To execute the entire project test suite, run <code>pytest</code> from the repository root.</p>"},{"location":"lessons/day-45-feature-engineering-and-evaluation/#further-exploration","title":"Further exploration","text":"<ul> <li>Swap in additional categorical columns and confirm that the preprocessing   pipeline scales automatically.</li> <li>Replace the logistic regression classifier in <code>build_model_pipeline</code> with   another estimator (e.g., RandomForestClassifier) and compare the resulting   confusion matrix.</li> <li>Continue into the responsible AI deep dive in   <code>Day_62_Model_Interpretability_and_Fairness</code>   to study post-hoc explanations and mitigation strategies built atop the   evaluation workflows from this lesson.</li> </ul>"},{"location":"lessons/day-45-feature-engineering-and-evaluation/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Composable feature engineering and evaluation utilities for Day 45.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Dict, Iterable, Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n\ndef create_sample_dataframe() -&gt; pd.DataFrame:\n    \"\"\"Return the toy purchase dataset used in the lesson.\"\"\"\n\n    data = {\n        \"age\": [25, 30, 35, 40, np.nan, 45, 50],\n        \"salary\": [50000, 60000, np.nan, 80000, 90000, 100000, 110000],\n        \"city\": [\n            \"New York\",\n            \"London\",\n            \"Paris\",\n            \"New York\",\n            \"London\",\n            \"Tokyo\",\n            \"Paris\",\n        ],\n        \"purchased\": [0, 1, 0, 1, 1, 0, 1],\n    }\n    return pd.DataFrame(data)\n\n\ndef build_preprocessing_pipeline(\n    numeric_features: Iterable[str] = (\"age\", \"salary\"),\n    categorical_features: Iterable[str] = (\"city\",),\n) -&gt; ColumnTransformer:\n    \"\"\"Create a ColumnTransformer that handles numeric and categorical columns.\"\"\"\n\n    numeric_transformer = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n    categorical_transformer = Pipeline(\n        steps=[\n            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n        ]\n    )\n\n    return ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_transformer, list(numeric_features)),\n            (\"cat\", categorical_transformer, list(categorical_features)),\n        ]\n    )\n\n\ndef preprocess_dataframe(\n    df: pd.DataFrame,\n    preprocessor: ColumnTransformer | None = None,\n) -&gt; Tuple[np.ndarray, pd.Series, ColumnTransformer]:\n    \"\"\"Fit the preprocessing pipeline and return the transformed feature matrix.\"\"\"\n\n    if \"purchased\" not in df.columns:\n        raise ValueError(\"Expected 'purchased' target column in the dataframe.\")\n\n    X = df.drop(\"purchased\", axis=1)\n    y = df[\"purchased\"]\n    preprocessor = preprocessor or build_preprocessing_pipeline()\n    X_processed = preprocessor.fit_transform(X)\n    return X_processed, y, preprocessor\n\n\ndef build_model_pipeline(preprocessor: ColumnTransformer) -&gt; Pipeline:\n    \"\"\"Combine preprocessing with a logistic regression classifier.\"\"\"\n\n    return Pipeline(\n        steps=[\n            (\"preprocess\", preprocessor),\n            (\"classifier\", LogisticRegression()),\n        ]\n    )\n\n\ndef evaluate_model(\n    df: pd.DataFrame,\n    test_size: float = 0.3,\n    random_state: int | None = 42,\n) -&gt; Tuple[Pipeline, Dict[str, object]]:\n    \"\"\"Train the pipeline and compute evaluation metrics on a test split.\"\"\"\n\n    if \"purchased\" not in df.columns:\n        raise ValueError(\"Expected 'purchased' target column in the dataframe.\")\n\n    X = df.drop(\"purchased\", axis=1)\n    y = df[\"purchased\"]\n    preprocessor = build_preprocessing_pipeline()\n    pipeline = build_model_pipeline(preprocessor)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n\n    metrics: Dict[str, object] = {\n        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n        \"classification_report\": classification_report(y_test, y_pred, zero_division=0),\n    }\n    return pipeline, metrics\n\n\nif __name__ == \"__main__\":\n    print(\"--- Feature Engineering Example ---\")\n    dataframe = create_sample_dataframe()\n    print(\"Original DataFrame:\")\n    print(dataframe)\n    print(\"-\" * 30)\n\n    processed, target, fitted_preprocessor = preprocess_dataframe(dataframe)\n    print(\"Shape of data after preprocessing:\", processed.shape)\n    print(\"Note: 'city' was expanded into multiple columns by OneHotEncoder.\")\n    print(\"Transformed data (first 3 rows):\")\n    print(processed[:3])\n    print(\"-\" * 30)\n\n    print(\"\\n--- Model Evaluation Example ---\")\n    model, metrics = evaluate_model(dataframe)\n    confusion = metrics[\"confusion_matrix\"]\n    print(\"Confusion Matrix:\")\n    print(confusion)\n    print(\"TN | FP\")\n    print(\"FN | TP\")\n    print(\n        f\"True Negatives (TN): {confusion[0, 0]} | False Positives (FP): {confusion[0, 1]}\"\n    )\n    print(\n        f\"False Negatives (FN): {confusion[1, 0]} | True Positives (TP): {confusion[1, 1]}\"\n    )\n    print(\"-\" * 30)\n    print(\"Classification Report:\")\n    print(metrics[\"classification_report\"])\n    print(\n        \"This report provides a breakdown of precision, recall, and f1-score for each class.\"\n    )\n    print(\"-\" * 30)\n</code></pre>"},{"location":"lessons/day-46-intro-to-neural-networks/","title":"Day 46: Introduction to Neural Networks & Frameworks","text":"<p>Welcome to Day 46! Today, we begin our exploration of Deep Learning by introducing the fundamental building block: the Artificial Neural Network (ANN). We'll also discuss the major frameworks used to build these powerful models.</p> <p>Prerequisites: Install TensorFlow with <code>pip install tensorflow</code> to run today's neural-network examples (this installs the CPU build; GPU acceleration requires following TensorFlow's CUDA setup). You'll also need scikit-learn for data prep with <code>pip install scikit-learn</code>. Need a refresher on using <code>pip</code>? Revisit the Day 20 Python Package Manager lesson.</p>"},{"location":"lessons/day-46-intro-to-neural-networks/#key-concepts","title":"Key Concepts","text":""},{"location":"lessons/day-46-intro-to-neural-networks/#what-is-a-neural-network","title":"What is a Neural Network?","text":"<p>An Artificial Neural Network is a computational model inspired by the structure and function of the human brain. It consists of interconnected nodes, called neurons, organized in layers.</p>"},{"location":"lessons/day-46-intro-to-neural-networks/#structure-of-a-neural-network","title":"Structure of a Neural Network","text":"<ol> <li> <p>Input Layer: Receives the initial data or features. The number of neurons in this layer corresponds to the number of features in the dataset.</p> </li> <li> <p>Hidden Layers: These are the intermediate layers between the input and output layers. A neural network can have zero or more hidden layers. A \"deep\" neural network has multiple hidden layers.</p> </li> <li> <p>Each neuron in a hidden layer receives inputs from the previous layer, applies a mathematical operation (a weighted sum followed by an activation function), and passes the result to the next layer.</p> </li> <li> <p>Output Layer: Produces the final result.</p> </li> <li> <p>For a regression task, it might have a single neuron with a linear activation.</p> </li> <li>For a classification task, it might have multiple neurons (one for each class) with a softmax activation function to output probabilities.</li> </ol>"},{"location":"lessons/day-46-intro-to-neural-networks/#activation-functions","title":"Activation Functions","text":"<p>Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. Without them, a neural network would just be a linear regression model.</p> <ul> <li>Common Examples:</li> <li>ReLU (Rectified Linear Unit): <code>f(x) = max(0, x)</code>. The most widely used activation function.</li> <li>Sigmoid: <code>f(x) = 1 / (1 + e^(-x))</code>. Used in the output layer for binary classification.</li> <li>Softmax: Generalizes the sigmoid function for multi-class classification.</li> </ul>"},{"location":"lessons/day-46-intro-to-neural-networks/#how-neural-networks-learn","title":"How Neural Networks Learn","text":"<p>They learn through a process called backpropagation and gradient descent.</p> <ol> <li>The network makes a prediction (forward pass).</li> <li>The prediction error is calculated using a loss function.</li> <li>The backpropagation algorithm calculates the gradient of the loss function with respect to the network's weights.</li> <li>The weights are updated using gradient descent to minimize the error.</li> </ol>"},{"location":"lessons/day-46-intro-to-neural-networks/#deep-learning-frameworks","title":"Deep Learning Frameworks","text":"<p>Building neural networks from scratch is complex. We use specialized libraries to handle the heavy lifting.</p> <ul> <li>TensorFlow: Developed by Google, it's a powerful and flexible ecosystem for machine learning. It's known for its production-readiness and scalability.</li> <li>PyTorch: Developed by Facebook's AI Research lab, it's known for its simplicity, ease of use, and Pythonic nature, making it a favorite in the research community.</li> </ul>"},{"location":"lessons/day-46-intro-to-neural-networks/#practice-exercise","title":"Practice Exercise","text":"<ul> <li>The <code>solutions.py</code> module now exposes dedicated helper functions for each step of the workflow (data preparation, model construction, training, and evaluation). Import the pieces you need instead of running everything on import.</li> <li>The code covers:</li> <li>Loading and preparing the Iris dataset.</li> <li>Building a sequential model with dense (fully connected) layers.</li> <li>Compiling the model (specifying the optimizer, loss function, and metrics).</li> <li>Training the model.</li> <li>Evaluating its performance and making predictions.</li> </ul>"},{"location":"lessons/day-46-intro-to-neural-networks/#running-the-example-and-tests","title":"Running the example and tests","text":"<ul> <li>Execute the full walkthrough from the command line with <code>python Day_46_Intro_to_Neural_Networks/solutions.py</code>. The script prints the data shapes, model summary, and evaluation metrics.</li> <li>Need a quick confidence check without the full 50-epoch run? The automated test performs a single, deterministic epoch: <code>pytest tests/test_day_46.py</code>.</li> <li>Training on GPU hardware is optional for this small dataset, but TensorFlow will automatically leverage your GPU if it's available and correctly configured.</li> </ul>"},{"location":"lessons/day-46-intro-to-neural-networks/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Utility functions for training a simple neural network on the Iris dataset.\n\nThe original tutorial for Day 46 walked through the full workflow of preparing\ndata, building a model, fitting it, and evaluating the results using\nprint statements.  This refactor exposes each major step as a reusable\nfunction so that the workflow can be unit tested and reused from other\nscripts without executing expensive training at import time.\n\nThe helper functions are intentionally lightweight: callers control the\nnumber of training epochs, verbosity, and whether validation data is used.\nEach function returns rich objects (e.g. `History` instances or metric\ndictionaries) so tests can assert on their contents.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterable, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nDEFAULT_SEED = 42\n\n\n@dataclass\nclass IrisData:\n    \"\"\"Container for the Iris dataset splits and fitted preprocessors.\"\"\"\n\n    X_train: np.ndarray\n    X_test: np.ndarray\n    y_train: np.ndarray\n    y_test: np.ndarray\n    scaler: StandardScaler\n    encoder: OneHotEncoder\n    target_names: Iterable[str]\n\n\ndef set_global_seed(seed: int = DEFAULT_SEED) -&gt; None:\n    \"\"\"Ensure reproducible results across NumPy and TensorFlow.\"\"\"\n\n    np.random.seed(seed)\n    tf.keras.utils.set_random_seed(seed)\n\n\ndef prepare_iris_data(\n    test_size: float = 0.2, random_state: int = DEFAULT_SEED\n) -&gt; IrisData:\n    \"\"\"Load, scale, and encode the Iris dataset.\n\n    Args:\n        test_size: Fraction of samples to allocate to the test split.\n        random_state: Deterministic seed used for the train/test split.\n\n    Returns:\n        An :class:`IrisData` instance containing the dataset splits along with\n        the fitted preprocessing objects.\n    \"\"\"\n\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # ``sparse_output`` is available in newer scikit-learn releases; fall back\n    # to the legacy ``sparse`` flag when running on older versions.\n    try:\n        encoder = OneHotEncoder(sparse_output=False)  # type: ignore[arg-type]\n    except TypeError:  # pragma: no cover - executed on older scikit-learn\n        encoder = OneHotEncoder(sparse=False)\n    y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y_onehot, test_size=test_size, random_state=random_state\n    )\n\n    return IrisData(\n        X_train=X_train,\n        X_test=X_test,\n        y_train=y_train,\n        y_test=y_test,\n        scaler=scaler,\n        encoder=encoder,\n        target_names=iris.target_names,\n    )\n\n\ndef build_iris_model(\n    input_shape: int, num_classes: int, hidden_units: Tuple[int, ...] = (10, 10)\n) -&gt; tf.keras.Model:\n    \"\"\"Create and compile the neural network used for Iris classification.\"\"\"\n\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Input(shape=(input_shape,)))\n    for units in hidden_units:\n        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n\n    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n\n    return model\n\n\ndef train_iris_model(\n    model: tf.keras.Model,\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    *,\n    epochs: int = 50,\n    batch_size: int = 8,\n    validation_split: float = 0.2,\n    verbose: int = 0,\n    shuffle: bool = True,\n) -&gt; tf.keras.callbacks.History:\n    \"\"\"Fit the neural network and return the resulting history object.\"\"\"\n\n    history = model.fit(\n        X_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_split=validation_split,\n        verbose=verbose,\n        shuffle=shuffle,\n    )\n    return history\n\n\ndef evaluate_iris_model(\n    model: tf.keras.Model, X_test: np.ndarray, y_test: np.ndarray, *, verbose: int = 0\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate the trained model and return the metrics as a dictionary.\"\"\"\n\n    return model.evaluate(X_test, y_test, verbose=verbose, return_dict=True)\n\n\ndef run_full_workflow(\n    *,\n    epochs: int = 50,\n    batch_size: int = 8,\n    validation_split: float = 0.2,\n    verbose: int = 0,\n    seed: int = DEFAULT_SEED,\n) -&gt; Tuple[tf.keras.callbacks.History, Dict[str, float], tf.keras.Model, IrisData]:\n    \"\"\"Execute the end-to-end Iris training workflow.\n\n    This helper is convenient for interactive exploration while keeping the\n    individual steps separately testable.\n    \"\"\"\n\n    set_global_seed(seed)\n    data = prepare_iris_data(random_state=seed)\n    model = build_iris_model(data.X_train.shape[1], data.y_train.shape[1])\n    history = train_iris_model(\n        model,\n        data.X_train,\n        data.y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_split=validation_split,\n        verbose=verbose,\n        shuffle=True,\n    )\n    metrics = evaluate_iris_model(model, data.X_test, data.y_test, verbose=verbose)\n    return history, metrics, model, data\n\n\nif __name__ == \"__main__\":\n    history, metrics, model, data = run_full_workflow(verbose=1)\n\n    print(\"--- Neural Network for Iris Classification ---\")\n    print(f\"Training set shape: {data.X_train.shape}\")\n    print(f\"One-hot encoded target shape: {data.y_train.shape}\")\n    print(\"-\" * 30)\n\n    print(\"Model Summary:\")\n    model.summary()\n    print(\"-\" * 30)\n\n    print(\n        \"Training complete. Final validation accuracy:\"\n        f\" {history.history['val_accuracy'][-1]:.3f}\"\n    )\n    print(\"Test metrics:\")\n    for name, value in metrics.items():\n        print(f\"  {name}: {value:.4f}\")\n    print(\"-\" * 30)\n\n    # Demonstrate an example prediction using the fitted scaler.\n    sample = np.array([[5.1, 3.5, 1.4, 0.2]])\n    sample_scaled = data.scaler.transform(sample)\n    prediction = model.predict(sample_scaled)\n    predicted_class = np.argmax(prediction, axis=1)[0]\n\n    print(\"Prediction for a new sample:\")\n    print(f\"Probabilities: {prediction[0]}\")\n    print(f\"Predicted class index: {predicted_class}\")\n    print(f\"Predicted class name: {data.target_names[predicted_class]}\")\n</code></pre>"},{"location":"lessons/day-47-convolutional-neural-networks/","title":"Day 47: Convolutional Neural Networks (CNNs) for Computer Vision","text":"<p>Welcome to Day 47! Today, we dive into Convolutional Neural Networks (CNNs), a specialized type of neural network that has revolutionized the field of Computer Vision.</p> <p>Prerequisites: Ensure TensorFlow is installed with <code>pip install tensorflow</code> (CPU build by default; use TensorFlow's GPU installation guide if you have compatible hardware). Need to review package installation basics? Revisit the Day 20 Python Package Manager lesson.</p>"},{"location":"lessons/day-47-convolutional-neural-networks/#key-concepts","title":"Key Concepts","text":""},{"location":"lessons/day-47-convolutional-neural-networks/#what-are-cnns","title":"What are CNNs?","text":"<p>CNNs are designed to automatically and adaptively learn spatial hierarchies of features from images. Unlike standard neural networks, which treat inputs as flat vectors, CNNs preserve the spatial relationship between pixels.</p>"},{"location":"lessons/day-47-convolutional-neural-networks/#core-components-of-a-cnn","title":"Core Components of a CNN","text":"<ol> <li> <p>Convolutional Layer (<code>Conv2D</code>)</p> </li> <li> <p>This is the main building block of a CNN. It uses filters (or kernels) to slide over the input image and perform a convolution operation.</p> </li> <li> <p>This process creates feature maps that highlight specific patterns like edges, corners, or textures in the image. The network learns the optimal values for these filters during training.</p> </li> <li> <p>Pooling Layer (<code>MaxPooling2D</code>)</p> </li> <li> <p>The pooling layer is used to downsample the feature maps, reducing their spatial dimensions.</p> </li> <li>This reduces the number of parameters and computation in the network, helping to control overfitting.</li> <li> <p>Max Pooling is the most common type, where a filter slides over the feature map and takes the maximum value from each region.</p> </li> <li> <p>Flatten Layer</p> </li> <li> <p>After the convolutional and pooling layers have extracted features, the resulting multi-dimensional feature maps are flattened into a single one-dimensional vector.</p> </li> <li>This vector is then fed into a standard fully connected neural network (like the one from Day 46) for classification.</li> </ol>"},{"location":"lessons/day-47-convolutional-neural-networks/#a-typical-cnn-architecture","title":"A Typical CNN Architecture","text":"<p>A common CNN architecture consists of a stack of <code>Conv2D</code> and <code>MaxPooling2D</code> layers, followed by one or more <code>Dense</code> layers for classification.</p> <ol> <li>Input Image</li> <li>[Conv2D -&gt; ReLU Activation -&gt; MaxPooling2D] (This block can be repeated multiple times)</li> <li>Flatten Layer</li> <li>Dense Layer (with ReLU)</li> <li>Output Dense Layer (with Softmax for classification)</li> </ol>"},{"location":"lessons/day-47-convolutional-neural-networks/#practice-exercise","title":"Practice Exercise","text":"<ul> <li>The <code>solutions.py</code> module now exports granular helpers (<code>prepare_mnist_data</code>, <code>build_cnn_model</code>, <code>train_cnn_model</code>, etc.) so you can mix and match pieces in notebooks or tests without kicking off a full five-epoch training run on import.</li> <li>The code covers:</li> <li>Loading and preprocessing the MNIST image data. (Images are normalized to be between 0 and 1).</li> <li>Building a sequential CNN model with <code>Conv2D</code>, <code>MaxPooling2D</code>, <code>Flatten</code>, and <code>Dense</code> layers.</li> <li>Compiling and training the CNN.</li> <li>Evaluating its performance on the test set. A well-trained CNN can achieve very high accuracy on this task.</li> </ul>"},{"location":"lessons/day-47-convolutional-neural-networks/#running-the-example-and-tests","title":"Running the example and tests","text":"<ul> <li>For the full demonstration run <code>python Day_47_Convolutional_Neural_Networks/solutions.py</code>. Expect a few epochs of training output plus the final metrics.</li> <li>To verify the pipeline quickly (and without downloading the entire MNIST dataset), use the short smoke test: <code>pytest tests/test_day_47.py</code>. The test swaps in a tiny synthetic dataset and trains for a single epoch.</li> <li>CNN training benefits from GPU acceleration. TensorFlow will automatically use your GPU if the drivers and CUDA/cuDNN stack are configured; otherwise the CPU-only run will simply take longer.</li> </ul>"},{"location":"lessons/day-47-convolutional-neural-networks/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Reusable helpers for building and training a small CNN on MNIST images.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\n\nDEFAULT_SEED = 42\n\n\ndef set_global_seed(seed: int = DEFAULT_SEED) -&gt; None:\n    \"\"\"Synchronise NumPy and TensorFlow RNGs for deterministic runs.\"\"\"\n\n    np.random.seed(seed)\n    tf.keras.utils.set_random_seed(seed)\n\n\ndef prepare_mnist_data(\n    normalize: bool = True,\n) -&gt; Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Load MNIST images, optionally normalise pixels, and add a channel axis.\"\"\"\n\n    (train_images, train_labels), (test_images, test_labels) = (\n        datasets.mnist.load_data()\n    )\n\n    train_images = train_images.astype(\"float32\")\n    test_images = test_images.astype(\"float32\")\n\n    if normalize:\n        train_images /= 255.0\n        test_images /= 255.0\n\n    train_images = train_images[..., tf.newaxis]\n    test_images = test_images[..., tf.newaxis]\n\n    return (train_images, train_labels), (test_images, test_labels)\n\n\ndef build_cnn_model(\n    input_shape: Tuple[int, int, int] = (28, 28, 1),\n    num_classes: int = 10,\n    conv_filters: Tuple[int, int, int] = (32, 64, 64),\n    dense_units: int = 64,\n) -&gt; tf.keras.Model:\n    \"\"\"Create an MNIST classifier mirroring the tutorial architecture.\"\"\"\n\n    model = models.Sequential()\n    model.add(layers.Input(shape=input_shape))\n    for index, filters in enumerate(conv_filters):\n        model.add(layers.Conv2D(filters, (3, 3), activation=\"relu\"))\n        if index &lt; len(conv_filters) - 1:\n            model.add(layers.MaxPooling2D((2, 2)))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(dense_units, activation=\"relu\"))\n    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n\n    return model\n\n\ndef compile_cnn_model(\n    model: tf.keras.Model,\n    optimizer: str = \"adam\",\n    loss: str = \"sparse_categorical_crossentropy\",\n    metrics: Tuple[str, ...] = (\"accuracy\",),\n) -&gt; tf.keras.Model:\n    \"\"\"Compile the CNN with sensible defaults for classification.\"\"\"\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=list(metrics))\n    return model\n\n\ndef train_cnn_model(\n    model: tf.keras.Model,\n    train_images: np.ndarray,\n    train_labels: np.ndarray,\n    *,\n    epochs: int = 5,\n    batch_size: int = 64,\n    validation_data: Tuple[np.ndarray, np.ndarray] | None = None,\n    validation_split: float = 0.0,\n    verbose: int = 1,\n    shuffle: bool = True,\n) -&gt; tf.keras.callbacks.History:\n    \"\"\"Fit the CNN and return the training history.\"\"\"\n\n    history = model.fit(\n        train_images,\n        train_labels,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=validation_data,\n        validation_split=validation_split,\n        verbose=verbose,\n        shuffle=shuffle,\n    )\n    return history\n\n\ndef evaluate_cnn_model(\n    model: tf.keras.Model,\n    test_images: np.ndarray,\n    test_labels: np.ndarray,\n    *,\n    verbose: int = 2,\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate the trained CNN on the test split.\"\"\"\n\n    return model.evaluate(test_images, test_labels, verbose=verbose, return_dict=True)\n\n\ndef run_full_workflow(\n    *,\n    epochs: int = 5,\n    batch_size: int = 64,\n    verbose: int = 1,\n    seed: int = DEFAULT_SEED,\n) -&gt; Tuple[tf.keras.callbacks.History, Dict[str, float], tf.keras.Model]:\n    \"\"\"Train and evaluate the CNN end-to-end, returning the artifacts.\"\"\"\n\n    set_global_seed(seed)\n    (train_images, train_labels), (test_images, test_labels) = prepare_mnist_data()\n    model = build_cnn_model(input_shape=train_images.shape[1:])\n    compile_cnn_model(model)\n    history = train_cnn_model(\n        model,\n        train_images,\n        train_labels,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(test_images, test_labels),\n        verbose=verbose,\n    )\n    metrics = evaluate_cnn_model(model, test_images, test_labels, verbose=verbose)\n    return history, metrics, model\n\n\nif __name__ == \"__main__\":\n    history, metrics, model = run_full_workflow()\n\n    print(\"--- CNN for MNIST Classification ---\")\n    model.summary()\n    print(\"-\" * 30)\n    print(\"Final training accuracy:\", history.history[\"accuracy\"][-1])\n    print(\"Test metrics:\")\n    for name, value in metrics.items():\n        print(f\"  {name}: {value:.4f}\")\n</code></pre>"},{"location":"lessons/day-48-recurrent-neural-networks/","title":"Day 48: Recurrent Neural Networks (RNNs) for Sequence Data","text":"<p>Welcome to Day 48! Today, we explore Recurrent Neural Networks (RNNs), a class of neural networks designed specifically for handling sequential data, such as time series, text, or audio.</p> <p>Prerequisites: Install TensorFlow with <code>pip install tensorflow</code> so you can build and train the RNN examples (CPU build by default; follow TensorFlow's GPU instructions if you have compatible hardware). Need to brush up on <code>pip</code> usage? Revisit the Day 20 Python Package Manager lesson.</p>"},{"location":"lessons/day-48-recurrent-neural-networks/#key-concepts","title":"Key Concepts","text":""},{"location":"lessons/day-48-recurrent-neural-networks/#what-are-rnns","title":"What are RNNs?","text":"<p>Unlike feedforward networks (like ANNs and CNNs), which process inputs independently, RNNs have loops in them, allowing information to persist. This \"memory\" lets them use information from prior inputs to influence the current input and output.</p> <ul> <li>The Loop: An RNN processes a sequence one element at a time. At each step, the output from the previous step is fed back as an input to the current step. This creates a hidden state that acts as a memory of the sequence seen so far.</li> </ul>"},{"location":"lessons/day-48-recurrent-neural-networks/#the-vanishing-gradient-problem","title":"The Vanishing Gradient Problem","text":"<p>Simple RNNs struggle to learn long-range dependencies (patterns over long sequences). This is due to the vanishing gradient problem, where the gradients used to update the network's weights become very small during backpropagation, effectively stopping the learning process for earlier time steps.</p>"},{"location":"lessons/day-48-recurrent-neural-networks/#advanced-rnn-architectures","title":"Advanced RNN Architectures","text":"<p>To solve this problem, more sophisticated RNN variants were developed:</p> <ol> <li> <p>Long Short-Term Memory (LSTM)</p> </li> <li> <p>LSTMs are a special kind of RNN that are explicitly designed to avoid the long-term dependency problem.</p> </li> <li> <p>They have a more complex internal structure called a cell, which includes three gates (forget, input, and output gates). These gates regulate the flow of information, allowing the network to remember or forget information over long periods.</p> </li> <li> <p>Gated Recurrent Unit (GRU)</p> </li> <li> <p>GRUs are a simplified version of LSTMs. They combine the forget and input gates into a single \"update gate\" and have fewer parameters.</p> </li> <li>They often perform similarly to LSTMs but are computationally more efficient.</li> </ol>"},{"location":"lessons/day-48-recurrent-neural-networks/#typical-rnn-architecture-for-classification","title":"Typical RNN Architecture for Classification","text":"<ol> <li>Input / Embedding Layer: For text data, an <code>Embedding</code> layer is often used first to convert integer indices (representing words) into dense vectors.</li> <li>Recurrent Layer (LSTM or GRU): This layer processes the sequence of vectors.</li> <li>Dense Layer: A standard fully connected layer for classification.</li> <li>Output Layer: Produces the final prediction.</li> </ol>"},{"location":"lessons/day-48-recurrent-neural-networks/#practice-exercise","title":"Practice Exercise","text":"<ul> <li>The refactored <code>solutions.py</code> module exposes building blocks such as <code>prepare_imdb_data</code>, <code>build_rnn_model</code>, and <code>train_rnn_model</code>. Import what you need for notebooks or experiments; nothing trains automatically when the module is imported.</li> <li>The goal remains to classify IMDB reviews as <code>positive</code> or <code>negative</code>, and the workflow now mirrors the modular structure used for other deep-learning lessons.</li> <li>The code covers:</li> <li>Loading and preprocessing the IMDB text data. (Reviews are pre-processed into sequences of integers).</li> <li>Padding the sequences to ensure they all have the same length.</li> <li>Building a sequential model with an <code>Embedding</code> layer, an <code>LSTM</code> layer, and <code>Dense</code> layers.</li> <li>Compiling and training the RNN.</li> <li>Evaluating its performance on the test set.</li> </ul>"},{"location":"lessons/day-48-recurrent-neural-networks/#running-the-example-and-tests","title":"Running the example and tests","text":"<ul> <li>Launch the full script with <code>python Day_48_Recurrent_Neural_Networks/solutions.py</code> to download IMDB, train the LSTM, and print evaluation metrics.</li> <li>For a very fast check, use <code>pytest tests/test_day_48.py</code>. The test stubs out the dataset loader with a handful of synthetic sequences and runs a single epoch so it finishes quickly.</li> <li>LSTM models benefit significantly from GPU acceleration. If you have CUDA/cuDNN configured, TensorFlow will pick it up automatically; otherwise the CPU execution path will still work (just slower).</li> </ul>"},{"location":"lessons/day-48-recurrent-neural-networks/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Utility functions for building and training a small LSTM on the IMDB dataset.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nDEFAULT_SEED = 42\n\n\ndef set_global_seed(seed: int = DEFAULT_SEED) -&gt; None:\n    \"\"\"Synchronise NumPy and TensorFlow RNGs for deterministic runs.\"\"\"\n\n    np.random.seed(seed)\n    tf.keras.utils.set_random_seed(seed)\n\n\ndef prepare_imdb_data(\n    *, vocab_size: int = 10_000, max_length: int = 256\n) -&gt; Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Load the IMDB sentiment dataset and pad sequences to a uniform length.\"\"\"\n\n    (train_data, train_labels), (test_data, test_labels) = datasets.imdb.load_data(\n        num_words=vocab_size\n    )\n\n    train_data_padded = pad_sequences(train_data, maxlen=max_length, padding=\"post\")\n    test_data_padded = pad_sequences(test_data, maxlen=max_length, padding=\"post\")\n\n    return (train_data_padded, np.array(train_labels)), (\n        test_data_padded,\n        np.array(test_labels),\n    )\n\n\ndef build_rnn_model(\n    *,\n    vocab_size: int = 10_000,\n    embedding_dim: int = 16,\n    max_length: int = 256,\n    lstm_units: int = 64,\n    dense_units: int = 64,\n) -&gt; tf.keras.Model:\n    \"\"\"Create an LSTM-based classifier mirroring the tutorial architecture.\"\"\"\n\n    model = models.Sequential(\n        [\n            layers.Input(shape=(max_length,)),\n            layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n            layers.LSTM(lstm_units),\n            layers.Dense(dense_units, activation=\"relu\"),\n            layers.Dense(1, activation=\"sigmoid\"),\n        ]\n    )\n\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n\ndef train_rnn_model(\n    model: tf.keras.Model,\n    train_data: np.ndarray,\n    train_labels: np.ndarray,\n    *,\n    epochs: int = 5,\n    batch_size: int = 128,\n    validation_split: float = 0.2,\n    verbose: int = 1,\n    shuffle: bool = True,\n) -&gt; tf.keras.callbacks.History:\n    \"\"\"Fit the RNN and return the training history.\"\"\"\n\n    history = model.fit(\n        train_data,\n        train_labels,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_split=validation_split,\n        verbose=verbose,\n        shuffle=shuffle,\n    )\n    return history\n\n\ndef evaluate_rnn_model(\n    model: tf.keras.Model,\n    test_data: np.ndarray,\n    test_labels: np.ndarray,\n    *,\n    verbose: int = 2,\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate the trained RNN on the held-out test data.\"\"\"\n\n    return model.evaluate(test_data, test_labels, verbose=verbose, return_dict=True)\n\n\ndef run_full_workflow(\n    *,\n    vocab_size: int = 10_000,\n    max_length: int = 256,\n    epochs: int = 5,\n    batch_size: int = 128,\n    verbose: int = 1,\n    seed: int = DEFAULT_SEED,\n) -&gt; Tuple[tf.keras.callbacks.History, Dict[str, float], tf.keras.Model]:\n    \"\"\"Train and evaluate the IMDB LSTM classifier end-to-end.\"\"\"\n\n    set_global_seed(seed)\n    (train_data, train_labels), (test_data, test_labels) = prepare_imdb_data(\n        vocab_size=vocab_size, max_length=max_length\n    )\n    model = build_rnn_model(\n        vocab_size=vocab_size, embedding_dim=16, max_length=max_length\n    )\n    history = train_rnn_model(\n        model,\n        train_data,\n        train_labels,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_split=0.2,\n        verbose=verbose,\n    )\n    metrics = evaluate_rnn_model(model, test_data, test_labels, verbose=verbose)\n    return history, metrics, model\n\n\nif __name__ == \"__main__\":\n    history, metrics, model = run_full_workflow()\n\n    print(\"--- RNN (LSTM) for IMDB Sentiment Classification ---\")\n    model.summary()\n    print(\"-\" * 30)\n    print(\"Final training accuracy:\", history.history[\"accuracy\"][-1])\n    print(\"Test metrics:\")\n    for name, value in metrics.items():\n        print(f\"  {name}: {value:.4f}\")\n</code></pre>"},{"location":"lessons/day-49-nlp/","title":"Day 49: Natural Language Processing (NLP)","text":""},{"location":"lessons/day-49-nlp/#overview","title":"Overview","text":"<p>Day 49 introduces the classic feature-extraction techniques that turn raw text into numeric matrices. Bag-of-words counts and TF-IDF scores are the foundations for many traditional NLP pipelines and remain useful for quick baselines or lightweight models.</p>"},{"location":"lessons/day-49-nlp/#whats-in-this-folder","title":"What's in this folder?","text":"<ul> <li><code>solutions.py</code> \u2013 exposes <code>build_count_matrix</code> and <code>build_tfidf_matrix</code>   helper functions plus a small demo script that prints both   representations for a sample corpus.</li> </ul>"},{"location":"lessons/day-49-nlp/#running-the-lesson-script","title":"Running the lesson script","text":"<p>Ensure the lesson dependencies are installed (in particular <code>scikit-learn</code>, <code>pandas</code>, and <code>numpy</code>). Then execute the walkthrough:</p> <pre><code>python Day_49_NLP/solutions.py\n</code></pre>"},{"location":"lessons/day-49-nlp/#running-the-tests","title":"Running the tests","text":"<p>The automated checks validate the helper functions against a miniature corpus. Run them with:</p> <pre><code>pytest tests/test_day_49.py\n</code></pre> <p>All tests expect to be run from the repository root so that imports resolve correctly.</p>"},{"location":"lessons/day-49-nlp/#next-steps","title":"Next steps","text":"<ul> <li>Jump to <code>Day_64_Modern_NLP_Pipelines</code>   for transformer fine-tuning, retrieval-augmented generation, and robust   evaluation workflows that build on the vectorization foundations covered   here.</li> </ul>"},{"location":"lessons/day-49-nlp/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Utility functions and a demo for bag-of-words and TF-IDF vectorization.\"\"\"\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n\ndef build_count_matrix(corpus):\n    \"\"\"Return a document-term matrix of raw counts for the given corpus.\"\"\"\n\n    vectorizer = CountVectorizer()\n    matrix = vectorizer.fit_transform(corpus)\n    return pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n\ndef build_tfidf_matrix(corpus):\n    \"\"\"Return a document-term matrix of TF-IDF scores for the given corpus.\"\"\"\n\n    vectorizer = TfidfVectorizer()\n    matrix = vectorizer.fit_transform(corpus)\n    return pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n\ndef demo():\n    \"\"\"Print a walkthrough of bag-of-words and TF-IDF representations.\"\"\"\n\n    corpus = [\n        \"The quick brown fox jumped over the lazy dog.\",\n        \"The dog was not lazy.\",\n        \"The fox is quick.\",\n    ]\n\n    print(\"--- NLP Vectorization Demo ---\")\n    print(\"Sample Corpus:\")\n    for doc in corpus:\n        print(f\"- '{doc}'\")\n    print(\"-\" * 30)\n\n    print(\"\\n--- 1. Bag-of-Words (CountVectorizer) ---\")\n    df_count = build_count_matrix(corpus)\n    print(\"Vocabulary (Feature Names):\")\n    print(df_count.columns.to_list())\n    print(\"\\nDocument-Term Matrix (Counts):\")\n    print(df_count)\n    print(\"This matrix shows the count of each word in each document.\")\n    print(\"-\" * 30)\n\n    print(\"\\n--- 2. TF-IDF (TfidfVectorizer) ---\")\n    df_tfidf = build_tfidf_matrix(corpus)\n    print(\"Vocabulary (Feature Names):\")\n    print(df_tfidf.columns.to_list())\n    print(\"\\nTF-IDF Matrix:\")\n    print(df_tfidf.round(2))\n    print(\n        \"This matrix shows the TF-IDF score for each word, highlighting important words.\"\n    )\n    print(\"-\" * 30)\n\n\nif __name__ == \"__main__\":\n    demo()\n</code></pre>"},{"location":"lessons/day-50-mlops/","title":"Day 50: MLOps - Model Deployment","text":"<p>Welcome to our final day, Day 50! Today, we touch upon MLOps (Machine Learning Operations), which focuses on the practical side of deploying, monitoring, and maintaining machine learning models in production environments. Our focus will be on a crucial first step: saving and loading a trained model. Later lessons deepen each pillar:</p> <ul> <li>Day 65 \u2013 MLOps Pipelines and CI/CD scales persistence into feature stores, model registries, and GitHub Actions workflows.</li> <li>Day 66 \u2013 Model Deployment and Serving compares REST, gRPC, batch, streaming, and edge endpoints with FastAPI/BentoML-inspired adapters.</li> <li>Day 67 \u2013 Model Monitoring and Reliability adds drift detection, retraining triggers, and observability instrumentation.</li> </ul>"},{"location":"lessons/day-50-mlops/#key-concepts","title":"Key Concepts","text":""},{"location":"lessons/day-50-mlops/#what-is-mlops","title":"What is MLOps?","text":"<p>MLOps is a set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently. It's the intersection of Machine Learning, DevOps, and Data Engineering.</p> <p>Key MLOps concerns include:</p> <ul> <li>Model Deployment: How do you make your trained model available to users or other systems?</li> <li>Automation: Automating the ML workflow from data ingestion to model training and deployment.</li> <li>Monitoring: Tracking the model's performance in production to detect degradation or drift.</li> <li>Scalability &amp; Reproducibility: Ensuring your system can handle load and that your experiments can be reproduced.</li> </ul>"},{"location":"lessons/day-50-mlops/#saving-and-loading-models","title":"Saving and Loading Models","text":"<p>Before you can deploy a model, you need to save its learned state (the weights, parameters, etc.). This process is often called serialization. Once saved, the model can be loaded into another environment (like a web server) to make predictions without needing to be retrained.</p> <ul> <li> <p>For Scikit-learn: The standard way is to use Python's <code>pickle</code> module or the more efficient <code>joblib</code> library. <code>joblib</code> is often preferred for objects that carry large NumPy arrays.</p> </li> <li> <p>For TensorFlow/Keras: TensorFlow has its own built-in saving and loading functionality (<code>model.save()</code> and <code>tf.keras.models.load_model()</code>). It can save the model's architecture, weights, and training configuration all in one place.</p> </li> </ul>"},{"location":"lessons/day-50-mlops/#serving-a-model-via-a-web-api","title":"Serving a Model via a Web API","text":"<p>A common way to deploy a model is to wrap it in a web API.</p> <ol> <li>A client (e.g., a web or mobile app) sends a POST request with new data to an API endpoint (e.g., <code>/predict</code>).</li> <li>The server, running a web framework like Flask or FastAPI, receives the data.</li> <li>The server loads the saved model and uses it to make a prediction on the data.</li> <li>The prediction is sent back to the client in the API response (usually in JSON format).</li> </ol>"},{"location":"lessons/day-50-mlops/#practice-exercise","title":"Practice Exercise","text":"<ul> <li>The <code>solutions.py</code> file demonstrates the basic workflow of saving and loading a <code>scikit-learn</code> model using <code>joblib</code>.</li> <li>The <code>bonus_flask_api.py</code> file provides a simple example of how you could serve this saved model using the Flask web framework.</li> <li>The code covers:</li> <li>Training a simple model on the Iris dataset.</li> <li>Saving the trained model to a file (<code>iris_model.joblib</code>).</li> <li>Loading the model back from the file and using it to make a new prediction.</li> <li>(Bonus) A minimal Flask app that loads the model and exposes a <code>/predict</code> endpoint.</li> </ul> <p>Review the code to understand the fundamental step of model persistence, which is the gateway to model deployment. When you are ready to automate full pipelines, deploy APIs, and monitor production behaviour, continue into Days 65\u201367 for the dedicated deep dives.</p>"},{"location":"lessons/day-50-mlops/#additional-materials","title":"Additional Materials","text":"<ul> <li>bonus_flask_api.ipynb</li> <li>solutions.ipynb</li> </ul> bonus_flask_api.py <p>View on GitHub</p> bonus_flask_api.py<pre><code>import joblib\nfrom flask import Flask, jsonify, request\n\n# NOTE: To run this script, you need to install Flask:\n# pip install Flask\n\n# 1. Initialize the Flask app\napp = Flask(__name__)\n\n# 2. Load the trained model\n# This is done once when the app starts.\nmodel_filename = \"iris_model.joblib\"\ntry:\n    model = joblib.load(model_filename)\n    print(f\"Model '{model_filename}' loaded successfully.\")\nexcept FileNotFoundError:\n    print(\n        f\"Error: Model file '{model_filename}' not found. Please run solutions.py first.\"\n    )\n    model = None\n\n\n# 3. Define the prediction endpoint\n@app.route(\"/predict\", methods=[\"POST\"])\ndef predict():\n    \"\"\"\n    Receives a POST request with JSON data and returns a prediction.\n    \"\"\"\n    if model is None:\n        return jsonify({\"error\": \"Model not loaded\"}), 500\n\n    # Get the JSON data from the request\n    data = request.get_json()\n\n    # Basic validation\n    if not data or \"features\" not in data:\n        return jsonify({\"error\": \"Invalid input: 'features' key not found\"}), 400\n\n    try:\n        # Extract features and convert to a list of lists for prediction\n        features = [data[\"features\"]]\n\n        # Make a prediction\n        prediction_index = model.predict(features)\n\n        # Get the class name from the index\n        iris_target_names = [\n            \"setosa\",\n            \"versicolor\",\n            \"virginica\",\n        ]  # In a real app, load this properly\n        predicted_class = iris_target_names[prediction_index[0]]\n\n        # Return the result as JSON\n        return jsonify(\n            {\n                \"prediction\": predicted_class,\n                \"prediction_index\": int(prediction_index[0]),\n            }\n        )\n\n    except Exception as e:\n        return jsonify({\"error\": f\"An error occurred: {str(e)}\"}), 500\n\n\n# --- How to Run This API ---\n# 1. Make sure 'iris_model.joblib' exists by running solutions.py.\n# 2. Run this script from your terminal: `python bonus_flask_api.py`\n# 3. The server will start, usually on http://127.0.0.1:5000.\n#\n# --- How to Test This API ---\n# You can use a tool like 'curl' from another terminal:\n#\n# curl -X POST http://127.0.0.1:5000/predict \\\n# -H \"Content-Type: application/json\" \\\n# -d '{\"features\": [6.0, 2.5, 4.5, 1.5]}'\n#\n# Expected Response:\n# {\n#   \"prediction\": \"versicolor\",\n#   \"prediction_index\": 1\n# }\n\nif __name__ == \"__main__\":\n    # The `debug=True` flag allows for auto-reloading when you save changes.\n    # Do not use `debug=True` in a production environment.\n    app.run(debug=True)\n</code></pre> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Reusable helpers for the Day 50 MLOps lesson.\n\nThe original script demonstrated how to train, persist, load, and reuse a\nscikit-learn model.  The logic now lives in functions that can be imported from\ntests or other projects.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections import Counter\nfrom math import ceil\nfrom pathlib import Path\nfrom typing import Iterable, Optional, Sequence, Tuple\n\nimport joblib\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef train_iris_model(\n    *,\n    test_size: float = 0.2,\n    random_state: int = 42,\n    max_iter: int = 200,\n    subset_size: Optional[int] = None,\n) -&gt; Tuple[LogisticRegression, float, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Train a Logistic Regression model on the Iris dataset.\n\n    Parameters\n    ----------\n    test_size:\n        Fraction of the dataset to hold back for evaluation.\n    random_state:\n        Seed that controls the train/test split and optional sub-sampling.\n    max_iter:\n        Maximum number of iterations for the Logistic Regression solver.\n    subset_size:\n        If provided, draw a deterministic subset of this size before\n        training. This is useful for demonstrations and tests where you want a\n        smaller, reproducible dataset.\n\n    Returns\n    -------\n    model:\n        The trained scikit-learn estimator.\n    accuracy:\n        Accuracy on the held-out test set.\n    X_test, y_test:\n        The evaluation features and labels so callers can verify behaviour.\n    target_names:\n        Names of the Iris species corresponding to prediction indices.\n    \"\"\"\n\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    full_classes = np.unique(iris.target)\n\n    if subset_size is not None:\n        if subset_size &lt;= 0:\n            raise ValueError(\"subset_size must be a positive integer\")\n        if subset_size &gt; len(X):\n            raise ValueError(\"subset_size cannot exceed the dataset size\")\n        rng = np.random.RandomState(random_state)\n        indices = rng.choice(len(X), subset_size, replace=False)\n        X = X[indices]\n        y = y[indices]\n\n    present_classes = np.unique(y)\n    missing_classes = sorted(set(full_classes) - set(present_classes))\n    if missing_classes:\n        raise ValueError(\n            \"Sub-sampled dataset is missing the following Iris classes: \"\n            f\"{missing_classes}. Increase the subset_size to include all classes.\"\n        )\n\n    class_counts = Counter(int(label) for label in np.asarray(y))\n    y_length = len(y)\n    effective_test_ratio: Optional[float] = None\n    if isinstance(test_size, (float, int)):\n        if isinstance(test_size, float):\n            if not 0 &lt; test_size &lt; 1:\n                raise ValueError(\n                    \"test_size must be between 0 and 1 when provided as a float\"\n                )\n            effective_test_ratio = test_size\n        else:\n            test_size_int = int(test_size)\n            if test_size_int &lt;= 0 or test_size_int &gt;= y_length:\n                raise ValueError(\n                    \"test_size must leave at least one sample in both the train and test sets\"\n                )\n            effective_test_ratio = test_size_int / y_length\n\n    for label in full_classes:\n        count = class_counts[int(label)]\n        if count &lt; 2:\n            raise ValueError(\n                \"Sub-sampled dataset must contain at least two samples per class to allow a stratified split.\"\n            )\n        if effective_test_ratio is not None:\n            test_allocation = ceil(count * effective_test_ratio)\n            if test_allocation == 0 or test_allocation &gt;= count:\n                raise ValueError(\n                    \"Sub-sampled dataset does not have enough samples of class \"\n                    f\"{label} to satisfy test_size={test_size}. \"\n                    \"Increase the subset_size or adjust test_size.\"\n                )\n\n    try:\n        X_train, X_test, y_train, y_test = train_test_split(\n            X,\n            y,\n            test_size=test_size,\n            random_state=random_state,\n            stratify=y,\n        )\n    except ValueError as exc:\n        raise ValueError(\n            \"Unable to perform a stratified train/test split with the requested \"\n            \"parameters. Ensure each class has sufficient samples for the chosen \"\n            \"test_size.\"\n        ) from exc\n\n    model = LogisticRegression(max_iter=max_iter)\n    model.fit(X_train, y_train)\n\n    accuracy = accuracy_score(y_test, model.predict(X_test))\n    return model, accuracy, X_test, y_test, iris.target_names\n\n\ndef save_model(model: LogisticRegression, path: Path | str) -&gt; Path:\n    \"\"\"Persist the trained model to disk and return the resolved path.\"\"\"\n\n    path = Path(path).expanduser().resolve()\n    path.parent.mkdir(parents=True, exist_ok=True)\n    joblib.dump(model, path)\n    return path\n\n\ndef load_model(path: Path | str) -&gt; LogisticRegression:\n    \"\"\"Load a persisted model from disk.\"\"\"\n\n    path = Path(path)\n    return joblib.load(path)\n\n\ndef predict_sample(\n    model: LogisticRegression,\n    sample: Iterable[float],\n    target_names: Optional[Sequence[str]] = None,\n) -&gt; Tuple[int, Optional[str]]:\n    \"\"\"Predict the Iris class for a single feature vector.\n\n    Parameters\n    ----------\n    model:\n        A trained scikit-learn estimator.\n    sample:\n        Iterable of feature values (sepal length, sepal width, petal length,\n        petal width).\n    target_names:\n        Optional sequence of class labels. If provided, the corresponding name\n        is returned alongside the numeric prediction.\n    \"\"\"\n\n    array = np.asarray(sample, dtype=float)\n    if array.ndim == 1:\n        array = array.reshape(1, -1)\n    prediction = model.predict(array)[0]\n    name = None\n    if target_names is not None:\n        name = target_names[prediction]\n    return prediction, name\n\n\ndef _demo() -&gt; None:\n    \"\"\"Replicate the original script as a simple CLI demonstration.\"\"\"\n\n    print(\"--- Model Persistence Example ---\")\n    model, accuracy, X_test, y_test, target_names = train_iris_model()\n    print(f\"Model trained with an accuracy of: {accuracy * 100:.2f}%\")\n    print(\"-\" * 30)\n\n    model_filename = save_model(model, \"iris_model.joblib\")\n    print(f\"Model saved to '{model_filename}'\")\n    print(\"-\" * 30)\n\n    print(\"Loading model from file...\")\n    loaded_model = load_model(model_filename)\n    print(\"Model loaded successfully.\")\n    print(\"-\" * 30)\n\n    new_sample = np.array([6.0, 2.5, 4.5, 1.5])\n    prediction, predicted_class_name = predict_sample(\n        loaded_model, new_sample, target_names\n    )\n\n    print(\"--- Making a Prediction with the Loaded Model ---\")\n    print(f\"New sample data: {new_sample.tolist()}\")\n    print(f\"Predicted class index: {prediction}\")\n    print(f\"Predicted class name: '{predicted_class_name}'\")\n    print(\"This demonstrates that our saved model retains its knowledge.\")\n    print(\"-\" * 30)\n    print(\n        \"\\nCheck out 'bonus_flask_api.py' for an example of how to serve this model in a web API.\"\n    )\n\n\nif __name__ == \"__main__\":\n    _demo()\n</code></pre>"},{"location":"lessons/day-51-regularized-models/","title":"Day 51 \u2013 Regularised Models","text":"<p>This lesson extends the regression toolkit with L2 (ridge), L1 (lasso), and elastic net penalties before introducing generalised linear models (GLMs). The core notebook and <code>solutions.py</code> module walk through the following:</p> <ul> <li>Building a reproducible synthetic regression dataset and benchmarking ridge,   lasso, and elastic net pipelines with cross-validation.</li> <li>Measuring coefficient shrinkage to understand how regularisation combats   overfitting and highlights the most important predictors.</li> <li>Training a Poisson regression GLM for count outcomes so you can generalise   linear modelling concepts beyond ordinary least squares.</li> </ul> <p>Run <code>python Day_51_Regularized_Models/solutions.py</code> to execute the full demo and review the printed comparison table.</p>"},{"location":"lessons/day-51-regularized-models/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Utilities for exploring regularised linear models in Day 51.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Tuple\n\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import (\n    ElasticNet,\n    Lasso,\n    LinearRegression,\n    PoissonRegressor,\n    Ridge,\n)\nfrom sklearn.metrics import mean_poisson_deviance\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n@dataclass(frozen=True)\nclass RegularisedModelResult:\n    \"\"\"Container for a fitted regularised model and its metrics.\"\"\"\n\n    name: str\n    pipeline: Pipeline\n    cv_score: float\n\n\ndef load_synthetic_regression(\n    n_samples: int = 200,\n    n_features: int = 12,\n    n_informative: int = 6,\n    noise: float = 8.0,\n    random_state: int = 51,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Return a deterministic regression dataset with informative coefficients.\"\"\"\n\n    X, y, true_coef = make_regression(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_informative=n_informative,\n        noise=noise,\n        coef=True,\n        random_state=random_state,\n    )\n    return X, y, true_coef\n\n\ndef build_regularised_pipeline(\n    model: str,\n    alpha: float = 1.0,\n    l1_ratio: float = 0.5,\n    random_state: int | None = 51,\n) -&gt; Pipeline:\n    \"\"\"Create a standardised pipeline for the requested regularised estimator.\"\"\"\n\n    if model == \"ridge\":\n        estimator = Ridge(alpha=alpha, random_state=random_state)\n    elif model == \"lasso\":\n        estimator = Lasso(alpha=alpha, random_state=random_state, max_iter=5000)\n    elif model == \"elastic_net\":\n        estimator = ElasticNet(\n            alpha=alpha, l1_ratio=l1_ratio, random_state=random_state, max_iter=5000\n        )\n    elif model == \"linear\":\n        estimator = LinearRegression()\n    else:\n        raise ValueError(f\"Unsupported model '{model}'.\")\n    return make_pipeline(StandardScaler(), estimator)\n\n\ndef evaluate_models_with_cv(\n    pipelines: Dict[str, Pipeline],\n    X: np.ndarray,\n    y: np.ndarray,\n    cv: int = 5,\n    scoring: str = \"neg_mean_squared_error\",\n) -&gt; Dict[str, RegularisedModelResult]:\n    \"\"\"Fit each pipeline with cross-validation and capture their scores.\"\"\"\n\n    splitter = KFold(n_splits=cv, shuffle=True, random_state=42)\n    results: Dict[str, RegularisedModelResult] = {}\n    for name, pipeline in pipelines.items():\n        scores = cross_val_score(pipeline, X, y, scoring=scoring, cv=splitter)\n        pipeline.fit(X, y)\n        results[name] = RegularisedModelResult(\n            name=name, pipeline=pipeline, cv_score=float(np.mean(scores))\n        )\n    return results\n\n\ndef summarise_coefficients(\n    results: Dict[str, RegularisedModelResult],\n) -&gt; Dict[str, Dict[str, float]]:\n    \"\"\"Report coefficient shrinkage statistics for fitted regularised models.\"\"\"\n\n    summary: Dict[str, Dict[str, float]] = {}\n    for name, result in results.items():\n        estimator = result.pipeline[-1]\n        if not hasattr(estimator, \"coef_\"):\n            continue\n        coef = estimator.coef_\n        summary[name] = {\n            \"l1_norm\": float(np.sum(np.abs(coef))),\n            \"l2_norm\": float(np.sqrt(np.sum(coef**2))),\n            \"non_zero\": int(np.count_nonzero(np.abs(coef) &gt; 1e-8)),\n        }\n    return summary\n\n\ndef fit_poisson_glm(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 0.0,\n    max_iter: int = 500,\n    random_state: int | None = 51,\n) -&gt; Tuple[Pipeline, float]:\n    \"\"\"Fit a Poisson regression GLM and return the model with its deviance.\"\"\"\n\n    pipeline = make_pipeline(\n        StandardScaler(with_mean=False),\n        PoissonRegressor(alpha=alpha, max_iter=max_iter, fit_intercept=True),\n    )\n    pipeline.fit(X, y)\n    preds = pipeline.predict(X)\n    deviance = mean_poisson_deviance(y, preds)\n    return pipeline, float(deviance)\n\n\ndef run_day51_demo() -&gt; Dict[str, RegularisedModelResult]:\n    \"\"\"Train ridge, lasso, and elastic net pipelines on the synthetic dataset.\"\"\"\n\n    X, y, _ = load_synthetic_regression()\n    models = {\n        \"linear\": build_regularised_pipeline(\"linear\"),\n        \"ridge\": build_regularised_pipeline(\"ridge\", alpha=1.0),\n        \"lasso\": build_regularised_pipeline(\"lasso\", alpha=0.05),\n        \"elastic_net\": build_regularised_pipeline(\n            \"elastic_net\", alpha=0.08, l1_ratio=0.5\n        ),\n    }\n    results = evaluate_models_with_cv(models, X, y)\n    return results\n\n\nif __name__ == \"__main__\":\n    fitted = run_day51_demo()\n    coefficient_summary = summarise_coefficients(fitted)\n    print(\"Day 51 Regularised Models Demo\")\n    for name, result in fitted.items():\n        print(f\"- {name}: CV score (neg MSE) = {result.cv_score:.3f}\")\n    print(\"\\nCoefficient summary:\")\n    for name, stats in coefficient_summary.items():\n        print(\n            f\"- {name}: L1 {stats['l1_norm']:.2f}, L2 {stats['l2_norm']:.2f}, non-zero {stats['non_zero']}\"\n        )\n</code></pre>"},{"location":"lessons/day-52-ensemble-methods/","title":"Day 52 \u2013 Ensemble Methods","text":"<p>Day 52 highlights how bagging, boosting, and stacking unlock better accuracy than single estimators. Use the notebook or <code>solutions.py</code> helpers to:</p> <ul> <li>Generate a balanced synthetic dataset for comparing ensemble families.</li> <li>Train a random forest with out-of-bag (OOB) scoring and export feature   importances for stakeholder-ready summaries.</li> <li>Fit a gradient boosting model, combine learners through stacking, and calibrate   probabilities so the predictions can power downstream decision rules.</li> </ul> <p>Execute <code>python Day_52_Ensemble_Methods/solutions.py</code> to print validation scores for each ensemble configuration.</p>"},{"location":"lessons/day-52-ensemble-methods/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Reusable ensemble helpers for Day 52.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Sequence, Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import (\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n    StackingClassifier,\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, brier_score_loss\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n@dataclass\nclass EnsembleResult:\n    \"\"\"Summary of an ensemble model and its validation score.\"\"\"\n\n    name: str\n    model: object\n    score: float\n\n\ndef generate_classification_data(\n    n_samples: int = 400,\n    n_features: int = 12,\n    n_informative: int = 6,\n    class_sep: float = 1.8,\n    random_state: int = 52,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return a deterministic classification dataset suitable for ensembles.\"\"\"\n\n    X, y = make_classification(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_informative=n_informative,\n        n_redundant=0,\n        class_sep=class_sep,\n        random_state=random_state,\n    )\n    return X, y\n\n\ndef train_random_forest(\n    X: np.ndarray,\n    y: np.ndarray,\n    n_estimators: int = 200,\n    max_depth: int | None = None,\n    random_state: int = 52,\n) -&gt; RandomForestClassifier:\n    \"\"\"Fit a random forest classifier with out-of-bag scoring enabled.\"\"\"\n\n    model = RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        oob_score=True,\n        random_state=random_state,\n        bootstrap=True,\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n    return model\n\n\ndef train_gradient_boosting(\n    X: np.ndarray,\n    y: np.ndarray,\n    learning_rate: float = 0.05,\n    n_estimators: int = 300,\n    max_depth: int = 2,\n    random_state: int = 52,\n) -&gt; GradientBoostingClassifier:\n    \"\"\"Train a gradient boosting classifier with mild regularisation.\"\"\"\n\n    model = GradientBoostingClassifier(\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        random_state=random_state,\n    )\n    model.fit(X, y)\n    return model\n\n\ndef build_stacking_classifier(\n    estimators: List[Tuple[str, Pipeline]] | None = None,\n    random_state: int = 52,\n) -&gt; StackingClassifier:\n    \"\"\"Create a stacking classifier with logistic regression as the final estimator.\"\"\"\n\n    if estimators is None:\n        estimators = [\n            (\n                \"rf\",\n                make_pipeline(\n                    StandardScaler(with_mean=False),\n                    RandomForestClassifier(\n                        n_estimators=150,\n                        max_depth=None,\n                        random_state=random_state,\n                        n_jobs=-1,\n                    ),\n                ),\n            ),\n            (\n                \"gb\",\n                make_pipeline(\n                    StandardScaler(),\n                    GradientBoostingClassifier(\n                        learning_rate=0.05,\n                        n_estimators=200,\n                        max_depth=2,\n                        random_state=random_state,\n                    ),\n                ),\n            ),\n        ]\n    final_estimator = LogisticRegression(max_iter=1000, random_state=random_state)\n    stacking = StackingClassifier(\n        estimators=estimators,\n        final_estimator=final_estimator,\n        passthrough=False,\n        stack_method=\"predict_proba\",\n        n_jobs=-1,\n    )\n    return stacking\n\n\ndef calibrate_classifier(\n    model,\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    method: str = \"isotonic\",\n    cv: int = 3,\n) -&gt; CalibratedClassifierCV:\n    \"\"\"Wrap a fitted classifier with probability calibration.\"\"\"\n\n    calibrated = CalibratedClassifierCV(estimator=model, method=method, cv=cv)\n    calibrated.fit(X_train, y_train)\n    return calibrated\n\n\ndef evaluate_classifier(\n    model, X_test: np.ndarray, y_test: np.ndarray\n) -&gt; Dict[str, float]:\n    \"\"\"Return accuracy and Brier score for the provided classifier.\"\"\"\n\n    probs = model.predict_proba(X_test)[:, 1]\n    preds = (probs &gt;= 0.5).astype(int)\n    return {\n        \"accuracy\": float(accuracy_score(y_test, preds)),\n        \"brier\": float(brier_score_loss(y_test, probs)),\n    }\n\n\ndef export_feature_importance(\n    model: RandomForestClassifier,\n    feature_names: Sequence[str],\n    output_path: str | Path | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Return and optionally persist feature importances as a DataFrame.\"\"\"\n\n    importances = pd.DataFrame(\n        {\n            \"feature\": list(feature_names),\n            \"importance\": model.feature_importances_,\n        }\n    ).sort_values(\"importance\", ascending=False)\n    if output_path is not None:\n        output_path = Path(output_path)\n        importances.to_csv(output_path, index=False)\n    return importances.reset_index(drop=True)\n\n\ndef evaluate_with_cross_validation(\n    model,\n    X: np.ndarray,\n    y: np.ndarray,\n    cv: int = 5,\n    scoring: str = \"roc_auc\",\n) -&gt; float:\n    \"\"\"Return the mean cross-validated score for a classifier.\"\"\"\n\n    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n    return float(np.mean(scores))\n\n\ndef run_day52_demo() -&gt; Dict[str, EnsembleResult]:\n    \"\"\"Train and evaluate the featured ensemble models.\"\"\"\n\n    X, y = generate_classification_data()\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.25, random_state=52, stratify=y\n    )\n\n    rf = train_random_forest(X_train, y_train)\n    gb = train_gradient_boosting(X_train, y_train)\n\n    stacking = build_stacking_classifier()\n    stacking.fit(X_train, y_train)\n    calibrated = calibrate_classifier(stacking, X_train, y_train)\n\n    results = {\n        \"random_forest\": EnsembleResult(\n            name=\"random_forest\",\n            model=rf,\n            score=float(rf.oob_score_),\n        ),\n        \"gradient_boosting\": EnsembleResult(\n            name=\"gradient_boosting\",\n            model=gb,\n            score=float(evaluate_with_cross_validation(gb, X, y)),\n        ),\n        \"stacking_calibrated\": EnsembleResult(\n            name=\"stacking_calibrated\",\n            model=calibrated,\n            score=evaluate_classifier(calibrated, X_test, y_test)[\"accuracy\"],\n        ),\n    }\n    return results\n\n\nif __name__ == \"__main__\":\n    results = run_day52_demo()\n    for name, result in results.items():\n        print(f\"{name}: validation score = {result.score:.3f}\")\n</code></pre>"},{"location":"lessons/day-53-model-tuning-and-feature-selection/","title":"Day 53 \u2013 Model Tuning and Feature Selection","text":"<p>Optimisation is the bridge between baseline models and production-grade performance. Day 53 introduces reproducible workflows for:</p> <ul> <li>Running grid search and Bayesian optimisation (via <code>skopt.BayesSearchCV</code>) to   tune hyperparameters efficiently.</li> <li>Calculating permutation importance scores to quantify feature contributions.</li> <li>Applying recursive feature elimination (RFE) and evaluating the reduced   feature set with cross-validation to check that accuracy holds steady.</li> </ul> <p>Execute <code>python Day_53_Model_Tuning_and_Feature_Selection/solutions.py</code> to see both search strategies in action alongside feature importance diagnostics.</p>"},{"location":"lessons/day-53-model-tuning-and-feature-selection/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Model tuning and feature selection utilities for Day 53.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterable, Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.feature_selection import RFE\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom skopt import BayesSearchCV\nfrom skopt.space import Categorical, Real\n\n\n@dataclass\nclass TuningResult:\n    \"\"\"Lightweight container for fitted search objects.\"\"\"\n\n    name: str\n    search: object\n    best_params: Dict[str, object]\n    best_score: float\n\n\ndef generate_tuning_dataset(\n    n_samples: int = 300,\n    n_features: int = 10,\n    n_informative: int = 5,\n    class_sep: float = 2.0,\n    random_state: int = 53,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Create a deterministic binary classification dataset for tuning demos.\"\"\"\n\n    X, y = make_classification(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_informative=n_informative,\n        n_redundant=0,\n        n_repeated=0,\n        class_sep=class_sep,\n        flip_y=0.01,\n        random_state=random_state,\n    )\n    return X, y\n\n\ndef build_logistic_pipeline(random_state: int = 53) -&gt; Pipeline:\n    \"\"\"Return a scaled logistic regression pipeline.\"\"\"\n\n    return make_pipeline(\n        StandardScaler(),\n        LogisticRegression(max_iter=2000, solver=\"lbfgs\", random_state=random_state),\n    )\n\n\ndef run_grid_search(\n    X: np.ndarray,\n    y: np.ndarray,\n    param_grid: Dict[str, Iterable[float | int]] | None = None,\n    cv: int = 5,\n    scoring: str = \"roc_auc\",\n    random_state: int = 53,\n) -&gt; TuningResult:\n    \"\"\"Execute a deterministic grid search for logistic regression hyperparameters.\"\"\"\n\n    pipeline = build_logistic_pipeline(random_state=random_state)\n    if param_grid is None:\n        param_grid = {\n            \"logisticregression__C\": [0.1, 1.0, 10.0],\n            \"logisticregression__penalty\": [\"l2\"],\n        }\n    cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n    grid = GridSearchCV(\n        pipeline,\n        param_grid=param_grid,\n        cv=cv_strategy,\n        scoring=scoring,\n        n_jobs=-1,\n    )\n    grid.fit(X, y)\n    return TuningResult(\n        name=\"grid_search\",\n        search=grid,\n        best_params=grid.best_params_,\n        best_score=float(grid.best_score_),\n    )\n\n\ndef run_bayesian_optimisation(\n    X: np.ndarray,\n    y: np.ndarray,\n    search_spaces: Dict[str, Iterable] | None = None,\n    n_iter: int = 16,\n    cv: int = 5,\n    scoring: str = \"roc_auc\",\n    random_state: int = 53,\n) -&gt; TuningResult:\n    \"\"\"Perform Bayesian optimisation using scikit-optimize's BayesSearchCV.\"\"\"\n\n    pipeline = build_logistic_pipeline(random_state=random_state)\n    if search_spaces is None:\n        search_spaces = {\n            \"logisticregression__C\": Real(1e-2, 1e2, prior=\"log-uniform\"),\n            \"logisticregression__penalty\": Categorical([\"l2\"]),\n        }\n    cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n    bayes = BayesSearchCV(\n        pipeline,\n        search_spaces=search_spaces,\n        n_iter=n_iter,\n        cv=cv_strategy,\n        scoring=scoring,\n        random_state=random_state,\n        n_jobs=-1,\n        refit=True,\n    )\n    bayes.fit(X, y)\n    return TuningResult(\n        name=\"bayesian_search\",\n        search=bayes,\n        best_params=bayes.best_params_,\n        best_score=float(bayes.best_score_),\n    )\n\n\ndef compute_permutation_importance(\n    model,\n    X: np.ndarray,\n    y: np.ndarray,\n    n_repeats: int = 15,\n    random_state: int = 53,\n) -&gt; pd.DataFrame:\n    \"\"\"Return permutation importance scores as a DataFrame.\"\"\"\n\n    result = permutation_importance(\n        model,\n        X,\n        y,\n        n_repeats=n_repeats,\n        random_state=random_state,\n        scoring=\"accuracy\",\n    )\n    df = pd.DataFrame(\n        {\n            \"feature\": [f\"feature_{i}\" for i in range(X.shape[1])],\n            \"importance_mean\": result.importances_mean,\n            \"importance_std\": result.importances_std,\n        }\n    ).sort_values(\"importance_mean\", ascending=False)\n    return df.reset_index(drop=True)\n\n\ndef run_recursive_feature_elimination(\n    X: np.ndarray,\n    y: np.ndarray,\n    n_features_to_select: int = 5,\n    random_state: int = 53,\n) -&gt; Tuple[RFE, np.ndarray]:\n    \"\"\"Perform recursive feature elimination with logistic regression.\"\"\"\n\n    estimator = LogisticRegression(\n        max_iter=2000, solver=\"lbfgs\", random_state=random_state\n    )\n    selector = RFE(estimator, n_features_to_select=n_features_to_select)\n    selector.fit(X, y)\n    support = selector.support_\n    return selector, support\n\n\ndef evaluate_selected_features(\n    selector: RFE,\n    X: np.ndarray,\n    y: np.ndarray,\n    cv: int = 5,\n) -&gt; float:\n    \"\"\"Evaluate the selected features with cross-validation accuracy.\"\"\"\n\n    X_selected = selector.transform(X)\n    model = LogisticRegression(max_iter=2000)\n    scores = cross_val_score(model, X_selected, y, cv=cv, scoring=\"accuracy\")\n    return float(np.mean(scores))\n\n\ndef run_day53_demo() -&gt; Dict[str, TuningResult]:\n    \"\"\"Execute grid search and Bayesian optimisation workflows.\"\"\"\n\n    X, y = generate_tuning_dataset()\n    grid = run_grid_search(X, y)\n    bayes = run_bayesian_optimisation(X, y)\n\n    pipeline = grid.search.best_estimator_\n    permutation_df = compute_permutation_importance(pipeline, X, y)\n    selector, support = run_recursive_feature_elimination(X, y)\n    selected_score = evaluate_selected_features(selector, X, y)\n\n    print(\"Permutation importance head:\\n\", permutation_df.head())\n    print(\"Selected features:\", np.where(support)[0])\n    print(f\"Cross-validated accuracy with selected features: {selected_score:.3f}\")\n\n    return {\n        \"grid\": grid,\n        \"bayes\": bayes,\n    }\n\n\nif __name__ == \"__main__\":\n    results = run_day53_demo()\n    for name, result in results.items():\n        print(f\"{name}: best score = {result.best_score:.3f}\")\n</code></pre>"},{"location":"lessons/day-54-probabilistic-modeling/","title":"Day 54 \u2013 Probabilistic Modeling","text":"<p>Gaussian mixtures, Bayesian classifiers, expectation-maximisation, and hidden Markov models power probabilistic reasoning for ambiguous business signals. Use the notebook or <code>solutions.py</code> helpers to:</p> <ul> <li>Simulate multi-modal customer cohorts and recover their structure with Gaussian mixtures.</li> <li>Train Gaussian Naive Bayes classifiers that expose posterior log-probabilities for fast decision rules.</li> <li>Run expectation-maximisation loops to maximise mixture log-likelihoods on noisy, partially labelled data.</li> <li>Implement a numerically stable hidden Markov forward pass to evaluate sequence likelihoods under state   transitions and Gaussian emissions.</li> </ul> <p>Execute <code>python Day_54_Probabilistic_Modeling/solutions.py</code> to print representative log-likelihood outputs for the reproducible toy datasets.</p>"},{"location":"lessons/day-54-probabilistic-modeling/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Reusable probabilistic modelling utilities for Day 54.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Tuple\n\nimport numpy as np\nfrom numpy.typing import ArrayLike, NDArray\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.naive_bayes import GaussianNB\n\n\n@dataclass\nclass HiddenMarkovModel:\n    \"\"\"Container for Gaussian-emission HMM parameters.\"\"\"\n\n    transition: NDArray[np.float64]\n    startprob: NDArray[np.float64]\n    means: NDArray[np.float64]\n    covariances: NDArray[np.float64]\n\n    def __post_init__(self) -&gt; None:\n        if self.transition.shape[0] != self.transition.shape[1]:\n            msg = \"Transition matrix must be square.\"\n            raise ValueError(msg)\n        if not np.allclose(self.transition.sum(axis=1), 1.0):\n            msg = \"Rows of the transition matrix must sum to 1.\"\n            raise ValueError(msg)\n        if not np.isclose(self.startprob.sum(), 1.0):\n            msg = \"Start probabilities must sum to 1.\"\n            raise ValueError(msg)\n        if len(self.means) != self.transition.shape[0]:\n            msg = \"Means must match number of hidden states.\"\n            raise ValueError(msg)\n        if self.covariances.shape[0] != self.transition.shape[0]:\n            msg = \"Covariances must match number of hidden states.\"\n            raise ValueError(msg)\n\n\ndef generate_probabilistic_dataset(\n    n_samples: int = 400,\n    random_state: int = 54,\n) -&gt; Tuple[NDArray[np.float64], NDArray[np.int_]]:\n    \"\"\"Create a reproducible Gaussian mixture dataset with component labels.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    weights = np.array([0.55, 0.45])\n    means = np.array([[0.0, 0.0], [3.5, 2.8]])\n    covariances = np.array(\n        [\n            [[0.8, 0.2], [0.2, 0.6]],\n            [[0.5, -0.15], [-0.15, 0.7]],\n        ]\n    )\n    assignments = rng.choice(len(weights), size=n_samples, p=weights)\n    observations = np.vstack(\n        [\n            rng.multivariate_normal(\n                means[idx], covariances[idx], size=(assignments == idx).sum()\n            )\n            for idx in range(len(weights))\n        ]\n    )\n    labels = np.concatenate(\n        [\n            np.full((assignments == idx).sum(), idx, dtype=int)\n            for idx in range(len(weights))\n        ]\n    )\n    ordering = rng.permutation(n_samples)\n    return observations[ordering], labels[ordering]\n\n\ndef fit_gaussian_mixture(\n    X: ArrayLike,\n    n_components: int = 2,\n    covariance_type: str = \"full\",\n    random_state: int = 54,\n    max_iter: int = 300,\n) -&gt; GaussianMixture:\n    \"\"\"Fit a Gaussian mixture model using expectation-maximisation.\"\"\"\n\n    model = GaussianMixture(\n        n_components=n_components,\n        covariance_type=covariance_type,\n        random_state=random_state,\n        max_iter=max_iter,\n        tol=1e-4,\n        reg_covar=1e-6,\n    )\n    model.fit(np.asarray(X))\n    return model\n\n\ndef mixture_log_likelihood(model: GaussianMixture, X: ArrayLike) -&gt; float:\n    \"\"\"Return the total data log-likelihood under the fitted mixture.\"\"\"\n\n    X = np.asarray(X)\n    return float(np.sum(model.score_samples(X)))\n\n\ndef run_expectation_maximisation(\n    X: ArrayLike,\n    n_components: int = 2,\n    random_state: int = 54,\n    max_iter: int = 300,\n) -&gt; Tuple[GaussianMixture, float]:\n    \"\"\"Fit a Gaussian mixture and return the average log-likelihood bound.\"\"\"\n\n    model = fit_gaussian_mixture(\n        X,\n        n_components=n_components,\n        random_state=random_state,\n        max_iter=max_iter,\n    )\n    return model, float(model.lower_bound_)\n\n\ndef train_bayesian_classifier(X: ArrayLike, y: ArrayLike) -&gt; GaussianNB:\n    \"\"\"Train a Gaussian Naive Bayes classifier.\"\"\"\n\n    model = GaussianNB()\n    model.fit(np.asarray(X), np.asarray(y))\n    return model\n\n\ndef bayesian_log_posterior(model: GaussianNB, X: ArrayLike) -&gt; NDArray[np.float64]:\n    \"\"\"Return class log-posterior probabilities for the given observations.\"\"\"\n\n    return np.asarray(model.predict_log_proba(np.asarray(X)))\n\n\ndef _gaussian_log_pdf(\n    x: NDArray[np.float64], mean: NDArray[np.float64], cov: NDArray[np.float64]\n) -&gt; float:\n    \"\"\"Log probability density of a multivariate normal distribution.\"\"\"\n\n    x = np.atleast_1d(x)\n    mean = np.atleast_1d(mean)\n    cov = np.asarray(cov)\n    diff = x - mean\n    sign, logdet = np.linalg.slogdet(cov)\n    if sign &lt;= 0:\n        msg = \"Covariance matrix must be positive definite.\"\n        raise ValueError(msg)\n    inv_cov = np.linalg.inv(cov)\n    dim = mean.size\n    exponent = float(diff.T @ inv_cov @ diff)\n    return -0.5 * (dim * np.log(2.0 * np.pi) + logdet + exponent)\n\n\ndef _logsumexp(\n    arr: NDArray[np.float64], axis: int | None = None\n) -&gt; NDArray[np.float64]:\n    \"\"\"Compute log-sum-exp in a numerically stable fashion.\"\"\"\n\n    arr = np.asarray(arr, dtype=float)\n    max_val = np.max(arr, axis=axis, keepdims=True)\n    stabilized = np.exp(arr - max_val)\n    summed = np.sum(stabilized, axis=axis, keepdims=True)\n    result = max_val + np.log(summed)\n    if axis is None:\n        return np.squeeze(result)\n    return np.squeeze(result, axis=axis)\n\n\ndef hmm_log_likelihood(model: HiddenMarkovModel, observations: ArrayLike) -&gt; float:\n    \"\"\"Compute the log-likelihood of observations under a Gaussian HMM.\"\"\"\n\n    obs = np.asarray(observations, dtype=float)\n    n_states = model.transition.shape[0]\n    n_obs = obs.shape[0]\n    emission_log_probs = np.empty((n_obs, n_states))\n    for state in range(n_states):\n        emission_log_probs[:, state] = [\n            _gaussian_log_pdf(point, model.means[state], model.covariances[state])\n            for point in obs\n        ]\n\n    log_alpha = np.log(model.startprob) + emission_log_probs[0]\n    for t in range(1, n_obs):\n        log_alpha = (\n            _logsumexp(log_alpha[:, np.newaxis] + np.log(model.transition), axis=0)\n            + emission_log_probs[t]\n        )\n    return float(_logsumexp(log_alpha))\n\n\ndef build_demo_hmm(random_state: int = 54) -&gt; HiddenMarkovModel:\n    \"\"\"Return a small two-state Gaussian HMM for demonstrations.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    transition = np.array([[0.85, 0.15], [0.2, 0.8]])\n    startprob = np.array([0.6, 0.4])\n    means = np.array([[0.0], [3.0]])\n    covariances = np.array([[[0.5]], [[0.7]]])\n    _ = rng  # Reserved for future extensions; keeps signature consistent.\n    return HiddenMarkovModel(\n        transition=transition, startprob=startprob, means=means, covariances=covariances\n    )\n\n\ndef demo_log_likelihoods() -&gt; dict[str, float]:\n    \"\"\"Train baseline models and return key log-likelihood metrics.\"\"\"\n\n    X, labels = generate_probabilistic_dataset()\n    gmm = fit_gaussian_mixture(X)\n    total_log_like = mixture_log_likelihood(gmm, X)\n\n    bayes = train_bayesian_classifier(X, labels)\n    avg_bayes_log_like = float(\n        np.mean(bayesian_log_posterior(bayes, X)[np.arange(len(labels)), labels])\n    )\n\n    hmm = build_demo_hmm()\n    demo_sequence = np.array([[0.2], [-0.1], [2.8], [3.4], [2.5]])\n    hmm_log_like = hmm_log_likelihood(hmm, demo_sequence)\n\n    return {\n        \"gmm_log_likelihood\": total_log_like,\n        \"bayes_average_log_posterior\": avg_bayes_log_like,\n        \"hmm_log_likelihood\": hmm_log_like,\n    }\n\n\nif __name__ == \"__main__\":\n    metrics = demo_log_likelihoods()\n    for name, value in metrics.items():\n        print(f\"{name}: {value:.3f}\")\n</code></pre>"},{"location":"lessons/day-55-advanced-unsupervised-learning/","title":"Day 55 \u2013 Advanced Unsupervised Learning","text":"<p>Density-based clustering, hierarchical approaches, and modern embeddings unlock structure within messy unlabelled datasets. Use the resources in this folder to:</p> <ul> <li>Compare DBSCAN and agglomerative clustering on reproducible customer segmentation datasets.</li> <li>Generate t-SNE and UMAP-style embeddings for storytelling-ready visualisations of high-dimensional data.</li> <li>Train compact autoencoders that reconstruct core signal while surfacing anomalies through reconstruction error.</li> <li>Combine reconstruction-based methods with classic isolation forests for a practical anomaly detection workflow.</li> </ul> <p>Run <code>python Day_55_Advanced_Unsupervised_Learning/solutions.py</code> to reproduce the cluster assignments, embeddings, and anomaly scores featured in the lesson.</p>"},{"location":"lessons/day-55-advanced-unsupervised-learning/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Advanced unsupervised learning helpers for Day 55.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom numpy.typing import ArrayLike, NDArray\nfrom sklearn.cluster import DBSCAN, AgglomerativeClustering\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n@dataclass\nclass ClusteringResult:\n    \"\"\"Simple container for clustering outputs.\"\"\"\n\n    labels: NDArray[np.int_]\n    model: object\n\n\ndef generate_clustering_data(\n    n_samples: int = 450,\n    random_state: int = 55,\n) -&gt; Tuple[NDArray[np.float64], NDArray[np.int_]]:\n    \"\"\"Return a reproducible blob dataset for clustering experiments.\"\"\"\n\n    X, y = make_blobs(\n        n_samples=n_samples,\n        centers=[[0, 0], [4, 4], [-4, 4]],\n        cluster_std=[0.55, 0.6, 0.7],\n        random_state=random_state,\n    )\n    return X, y\n\n\ndef run_dbscan(\n    X: ArrayLike,\n    eps: float = 0.55,\n    min_samples: int = 8,\n    scale: bool = True,\n) -&gt; ClusteringResult:\n    \"\"\"Cluster the dataset with DBSCAN and optional feature scaling.\"\"\"\n\n    X = np.asarray(X, dtype=float)\n    if scale:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n    model = DBSCAN(eps=eps, min_samples=min_samples)\n    labels = model.fit_predict(X)\n    return ClusteringResult(labels=labels, model=model)\n\n\ndef run_agglomerative(\n    X: ArrayLike,\n    n_clusters: int = 3,\n    linkage: str = \"ward\",\n) -&gt; ClusteringResult:\n    \"\"\"Perform agglomerative clustering and return labels and the fitted estimator.\"\"\"\n\n    X = np.asarray(X, dtype=float)\n    model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n    labels = model.fit_predict(X)\n    return ClusteringResult(labels=labels, model=model)\n\n\ndef compute_tsne_embedding(\n    X: ArrayLike,\n    n_components: int = 2,\n    perplexity: float = 30.0,\n    random_state: int = 55,\n) -&gt; NDArray[np.float64]:\n    \"\"\"Return a deterministic t-SNE embedding for visualisation.\"\"\"\n\n    X = np.asarray(X, dtype=float)\n    tsne = TSNE(\n        n_components=n_components,\n        perplexity=perplexity,\n        random_state=random_state,\n        init=\"pca\",\n        learning_rate=\"auto\",\n        max_iter=1500,\n    )\n    return tsne.fit_transform(X)\n\n\ndef build_autoencoder(\n    input_dim: int,\n    encoding_dim: int = 2,\n    random_state: int = 55,\n) -&gt; keras.Model:\n    \"\"\"Create a compact dense autoencoder.\"\"\"\n\n    tf.keras.utils.set_random_seed(random_state)\n    encoder_inputs = keras.Input(shape=(input_dim,))\n    encoded = layers.Dense(encoding_dim, activation=\"relu\")(encoder_inputs)\n    decoded = layers.Dense(input_dim, activation=\"linear\")(encoded)\n    autoencoder = keras.Model(inputs=encoder_inputs, outputs=decoded)\n    autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=\"mse\")\n    return autoencoder\n\n\ndef train_autoencoder(\n    model: keras.Model,\n    X_train: ArrayLike,\n    epochs: int = 80,\n    batch_size: int = 32,\n) -&gt; keras.callbacks.History:\n    \"\"\"Train the autoencoder on the provided dataset.\"\"\"\n\n    X_train = np.asarray(X_train, dtype=float)\n    history = model.fit(\n        X_train,\n        X_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        shuffle=True,\n        verbose=0,\n    )\n    return history\n\n\ndef reconstruction_errors(model: keras.Model, X: ArrayLike) -&gt; NDArray[np.float64]:\n    \"\"\"Return mean squared reconstruction error per sample.\"\"\"\n\n    X = np.asarray(X, dtype=float)\n    reconstructions = model.predict(X, verbose=0)\n    return np.mean((X - reconstructions) ** 2, axis=1)\n\n\ndef autoencoder_anomaly_threshold(errors: ArrayLike, quantile: float = 0.95) -&gt; float:\n    \"\"\"Return an empirical anomaly threshold from reconstruction errors.\"\"\"\n\n    errors = np.asarray(errors, dtype=float)\n    return float(np.quantile(errors, quantile))\n\n\ndef detect_anomalies_with_autoencoder(\n    model: keras.Model,\n    X: ArrayLike,\n    threshold: float,\n) -&gt; NDArray[np.int_]:\n    \"\"\"Return a binary mask where 1 indicates an anomaly.\"\"\"\n\n    errors = reconstruction_errors(model, X)\n    return (errors &gt; threshold).astype(int)\n\n\ndef isolation_forest_scores(\n    X: ArrayLike,\n    contamination: float = 0.05,\n    random_state: int = 55,\n) -&gt; NDArray[np.float64]:\n    \"\"\"Return anomaly scores from an isolation forest.\"\"\"\n\n    X = np.asarray(X, dtype=float)\n    model = IsolationForest(\n        contamination=contamination,\n        random_state=random_state,\n        n_estimators=200,\n    )\n    model.fit(X)\n    return -model.decision_function(X)\n\n\ndef lof_anomaly_scores(\n    X: ArrayLike, n_neighbors: int = 20\n) -&gt; Tuple[NDArray[np.float64], NDArray[np.int_]]:\n    \"\"\"Return Local Outlier Factor scores (larger implies more anomalous).\"\"\"\n\n    X = np.asarray(X, dtype=float)\n    lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=False)\n    labels = lof.fit_predict(X)\n    return -lof.negative_outlier_factor_, labels\n\n\ndef tsne_distance_preservation(X: ArrayLike, embedding: ArrayLike) -&gt; float:\n    \"\"\"Compute Spearman correlation between pairwise distances before and after embedding.\"\"\"\n\n    original = pairwise_distances(np.asarray(X, dtype=float))\n    embedded = pairwise_distances(np.asarray(embedding, dtype=float))\n    original_flat = original[np.triu_indices_from(original, k=1)]\n    embedded_flat = embedded[np.triu_indices_from(embedded, k=1)]\n    if original_flat.size == 0:\n        return 1.0\n    return float(np.corrcoef(original_flat, embedded_flat)[0, 1])\n\n\ndef demo_unsupervised_pipeline(random_state: int = 55) -&gt; Dict[str, float]:\n    \"\"\"Run the featured unsupervised workflow and return summary statistics.\"\"\"\n\n    X, _ = generate_clustering_data(random_state=random_state)\n    dbscan_result = run_dbscan(X)\n    agg_result = run_agglomerative(X)\n    embedding = compute_tsne_embedding(X, random_state=random_state)\n    distance_corr = tsne_distance_preservation(X, embedding)\n\n    auto = build_autoencoder(input_dim=X.shape[1], random_state=random_state)\n    train_autoencoder(auto, X, epochs=60)\n    errors = reconstruction_errors(auto, X)\n    threshold = autoencoder_anomaly_threshold(errors, 0.9)\n    anomaly_rate = float(np.mean(errors &gt; threshold))\n\n    if_scores = isolation_forest_scores(X)\n\n    return {\n        \"dbscan_clusters\": float(\n            len(set(dbscan_result.labels)) - (1 if -1 in dbscan_result.labels else 0)\n        ),\n        \"agglomerative_clusters\": float(len(np.unique(agg_result.labels))),\n        \"tsne_distance_corr\": distance_corr,\n        \"autoencoder_threshold\": threshold,\n        \"autoencoder_anomaly_rate\": anomaly_rate,\n        \"isolation_forest_score_mean\": float(np.mean(if_scores)),\n    }\n\n\nif __name__ == \"__main__\":\n    summary = demo_unsupervised_pipeline()\n    for key, value in summary.items():\n        print(f\"{key}: {value:.3f}\")\n</code></pre>"},{"location":"lessons/day-56-time-series-and-forecasting/","title":"Day 56 \u2013 Time Series and Forecasting","text":"<p>Seasonality-aware forecasting keeps plans grounded in data rather than gut feel. The lesson materials show how to:</p> <ul> <li>Generate seeded seasonal datasets for reproducible ARIMA and exponential smoothing experiments.</li> <li>Fit classic ARIMA/SARIMAX models alongside Prophet-style trend/seasonality decompositions built with statsmodels.</li> <li>Evaluate forecasts with rolling-origin backtests that compute MAE, RMSE, MAPE, and sMAPE simultaneously.</li> <li>Visualise forecast intervals and compare competing models on demand and revenue scenarios.</li> </ul> <p>Launch <code>python Day_56_Time_Series_and_Forecasting/solutions.py</code> to train the baseline models and print metric summaries for the synthetic workloads.</p>"},{"location":"lessons/day-56-time-series-and-forecasting/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Time series forecasting utilities for Day 56.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Callable, Dict, Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom numpy.typing import ArrayLike\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\n@dataclass\nclass ForecastResult:\n    \"\"\"Container for forecast outputs and residual metrics.\"\"\"\n\n    forecast: pd.Series\n    lower: pd.Series | None\n    upper: pd.Series | None\n\n\ndef generate_seasonal_series(\n    periods: int = 96,\n    seasonal_period: int = 12,\n    trend: float = 0.3,\n    noise: float = 0.5,\n    random_state: int = 56,\n) -&gt; pd.Series:\n    \"\"\"Return a synthetic seasonal time series suitable for forecasting demos.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    time = np.arange(periods)\n    seasonal_pattern = np.sin(2 * np.pi * time / seasonal_period)\n    series = (\n        5 + trend * time + 2.5 * seasonal_pattern + rng.normal(0, noise, size=periods)\n    )\n    index = pd.date_range(\"2020-01-01\", periods=periods, freq=\"MS\")\n    return pd.Series(series, index=index, name=\"demand\")\n\n\ndef train_test_split_series(\n    series: pd.Series, test_size: int = 12\n) -&gt; Tuple[pd.Series, pd.Series]:\n    \"\"\"Split a series into train and test suffixes.\"\"\"\n\n    if test_size &lt;= 0:\n        msg = \"test_size must be positive\"\n        raise ValueError(msg)\n    return series.iloc[:-test_size], series.iloc[-test_size:]\n\n\ndef fit_arima_forecast(\n    train: pd.Series,\n    order: Tuple[int, int, int] = (1, 1, 1),\n    steps: int = 12,\n) -&gt; ForecastResult:\n    \"\"\"Fit an ARIMA model and forecast the specified number of steps.\"\"\"\n\n    model = ARIMA(train, order=order)\n    fitted = model.fit()\n    forecast_res = fitted.get_forecast(steps=steps)\n    mean = forecast_res.predicted_mean\n    conf_int = forecast_res.conf_int(alpha=0.05)\n    return ForecastResult(\n        forecast=mean,\n        lower=conf_int.iloc[:, 0],\n        upper=conf_int.iloc[:, 1],\n    )\n\n\ndef fit_sarimax_forecast(\n    train: pd.Series,\n    order: Tuple[int, int, int] = (1, 0, 0),\n    seasonal_order: Tuple[int, int, int, int] = (0, 1, 1, 12),\n    steps: int = 12,\n) -&gt; ForecastResult:\n    \"\"\"Fit a SARIMAX model that captures seasonal structure.\"\"\"\n\n    model = SARIMAX(\n        train,\n        order=order,\n        seasonal_order=seasonal_order,\n        trend=\"c\",\n        enforce_stationarity=False,\n        enforce_invertibility=False,\n    )\n    fitted = model.fit(disp=False)\n    forecast_res = fitted.get_forecast(steps=steps)\n    mean = forecast_res.predicted_mean\n    conf_int = forecast_res.conf_int(alpha=0.05)\n    return ForecastResult(\n        forecast=mean,\n        lower=conf_int.iloc[:, 0],\n        upper=conf_int.iloc[:, 1],\n    )\n\n\ndef fit_exponential_smoothing(\n    train: pd.Series,\n    seasonal_periods: int = 12,\n    trend: str = \"add\",\n    seasonal: str = \"add\",\n    steps: int = 12,\n) -&gt; ForecastResult:\n    \"\"\"Fit Holt-Winters exponential smoothing and forecast.\"\"\"\n\n    model = ExponentialSmoothing(\n        train,\n        seasonal_periods=seasonal_periods,\n        trend=trend,\n        seasonal=seasonal,\n    )\n    fitted = model.fit(optimized=True)\n    forecast = fitted.forecast(steps)\n    return ForecastResult(forecast=forecast, lower=None, upper=None)\n\n\ndef forecast_metrics(y_true: ArrayLike, y_pred: ArrayLike) -&gt; Dict[str, float]:\n    \"\"\"Return common time-series error metrics.\"\"\"\n\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    mae = np.mean(np.abs(y_true - y_pred))\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n        smape = 100 * np.mean(\n            np.abs(y_true - y_pred) / ((np.abs(y_true) + np.abs(y_pred)) / 2)\n        )\n    return {\n        \"mae\": float(mae),\n        \"rmse\": rmse,\n        \"mape\": float(np.nan_to_num(mape)),\n        \"smape\": float(np.nan_to_num(smape)),\n    }\n\n\ndef rolling_origin_backtest(\n    series: pd.Series,\n    initial_train_size: int,\n    horizon: int,\n    model_builder: Callable[[pd.Series, int], ForecastResult],\n) -&gt; pd.DataFrame:\n    \"\"\"Perform a rolling-origin backtest returning metrics per split.\"\"\"\n\n    if initial_train_size + horizon &gt; len(series):\n        msg = \"Not enough observations for the specified horizon.\"\n        raise ValueError(msg)\n    rows = []\n    for start in range(len(series) - initial_train_size - horizon + 1):\n        train_end = initial_train_size + start\n        train = series.iloc[:train_end]\n        test = series.iloc[train_end : train_end + horizon]\n        forecast = model_builder(train, horizon)\n        metrics = forecast_metrics(test.values, forecast.forecast.values)\n        metrics[\"start\"] = series.index[train_end]\n        rows.append(metrics)\n    return pd.DataFrame(rows)\n\n\ndef prophet_style_forecast(train: pd.Series, steps: int = 12) -&gt; ForecastResult:\n    \"\"\"Approximate a Prophet-like decomposition using statsmodels.\"\"\"\n\n    # Prophet combines trend and seasonality; emulate with additive Holt-Winters\n    result = fit_exponential_smoothing(\n        train, seasonal_periods=12, trend=\"add\", seasonal=\"add\", steps=steps\n    )\n    return result\n\n\ndef demo_forecasting_pipeline(random_state: int = 56) -&gt; Dict[str, float]:\n    \"\"\"Generate a dataset, fit multiple models, and return evaluation metrics.\"\"\"\n\n    series = generate_seasonal_series(random_state=random_state)\n    train, test = train_test_split_series(series, test_size=12)\n\n    arima_forecast = fit_arima_forecast(train, steps=len(test))\n    sarimax_forecast = fit_sarimax_forecast(train, steps=len(test))\n    prophet_like = prophet_style_forecast(train, steps=len(test))\n\n    arima_metrics = forecast_metrics(test.values, arima_forecast.forecast.values)\n    sarimax_metrics = forecast_metrics(test.values, sarimax_forecast.forecast.values)\n    prophet_metrics = forecast_metrics(test.values, prophet_like.forecast.values)\n\n    return {\n        \"arima_mae\": arima_metrics[\"mae\"],\n        \"sarimax_mae\": sarimax_metrics[\"mae\"],\n        \"prophet_mae\": prophet_metrics[\"mae\"],\n        \"test_mean\": float(test.mean()),\n    }\n\n\nif __name__ == \"__main__\":\n    summary = demo_forecasting_pipeline()\n    for key, value in summary.items():\n        print(f\"{key}: {value:.3f}\")\n</code></pre>"},{"location":"lessons/day-57-recommender-systems/","title":"Day 57 \u2013 Recommender Systems","text":"<p>Recommender systems pair users with relevant products when catalogues explode. This lesson covers how to:</p> <ul> <li>Build user\u2013item matrices and run neighbourhood-based collaborative filtering with cosine similarity.</li> <li>Perform low-rank matrix factorisation to uncover latent tastes and fill sparse feedback grids.</li> <li>Adapt ranking pipelines for implicit-feedback data by combining confidence weights and popularity priors.</li> <li>Evaluate recommenders with hit rate, precision@K, and mean average precision to keep stakeholders aligned on outcomes.</li> </ul> <p>Execute <code>python Day_57_Recommender_Systems/solutions.py</code> to generate rankings, matrix factorisation reconstructions, and evaluation metrics on compact demo datasets.</p>"},{"location":"lessons/day-57-recommender-systems/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Collaborative filtering helpers for Day 57.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterable, List, Sequence\n\nimport numpy as np\nimport pandas as pd\nfrom numpy.typing import ArrayLike\n\n\n@dataclass\nclass Recommendation:\n    \"\"\"Container storing recommendations for a single user.\"\"\"\n\n    user: str\n    ranked_items: List[str]\n    scores: List[float]\n\n\ndef build_demo_user_item_matrix(random_state: int = 57) -&gt; pd.DataFrame:\n    \"\"\"Return a reproducible user\u2013item ratings matrix.\"\"\"\n\n    users = [\"A\", \"B\", \"C\", \"D\"]\n    items = [\"Item1\", \"Item2\", \"Item3\", \"Item4\", \"Item5\"]\n    base = np.array(\n        [\n            [5, 4, 0, 1, 0],\n            [4, 0, 4, 1, 0],\n            [1, 1, 0, 5, 4],\n            [0, 1, 5, 4, 0],\n        ],\n        dtype=float,\n    )\n    rng = np.random.default_rng(random_state)\n    noise = rng.normal(0, 0.05, size=base.shape)\n    ratings = base + noise\n    ratings[base == 0] = 0  # keep missing entries at zero\n    return pd.DataFrame(ratings, index=users, columns=items)\n\n\ndef cosine_similarity_matrix(matrix: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Compute cosine similarity between rows of the given matrix.\"\"\"\n\n    X = np.asarray(matrix, dtype=float)\n    norms = np.linalg.norm(X, axis=1, keepdims=True)\n    norms[norms == 0] = 1\n    normalised = X / norms\n    return normalised @ normalised.T\n\n\ndef user_based_scores(\n    ratings: pd.DataFrame,\n    target_user: str,\n    k_neighbors: int = 2,\n) -&gt; pd.Series:\n    \"\"\"Return item scores for a target user using user-based collaborative filtering.\"\"\"\n\n    if target_user not in ratings.index:\n        msg = f\"Unknown user: {target_user}\"\n        raise KeyError(msg)\n    similarity = cosine_similarity_matrix(ratings.values)\n    user_index = list(ratings.index).index(target_user)\n    user_sim = similarity[user_index]\n    np.fill_diagonal(similarity, 0.0)\n    top_indices = np.argsort(user_sim)[::-1][:k_neighbors]\n    neighbour_weights = user_sim[top_indices]\n    neighbour_ratings = ratings.iloc[top_indices]\n    weighted = neighbour_weights[:, np.newaxis] * neighbour_ratings.values\n    denom = np.sum(np.abs(neighbour_weights)) + 1e-9\n    scores = np.sum(weighted, axis=0) / denom\n    scores_series = pd.Series(scores, index=ratings.columns)\n    already_rated = ratings.loc[target_user] &gt; 0\n    return scores_series.mask(already_rated, other=-np.inf)\n\n\ndef svd_matrix_factorisation(\n    ratings: pd.DataFrame,\n    n_factors: int = 2,\n    regularisation: float = 0.0,\n) -&gt; pd.DataFrame:\n    \"\"\"Approximate the ratings matrix with truncated SVD.\"\"\"\n\n    matrix = ratings.values.astype(float)\n    user_means = np.where(\n        matrix.sum(axis=1) &gt; 0, matrix.sum(axis=1) / np.count_nonzero(matrix, axis=1), 0\n    )\n    demeaned = matrix - user_means[:, np.newaxis]\n    demeaned[np.isnan(demeaned)] = 0\n    U, s, Vt = np.linalg.svd(demeaned, full_matrices=False)\n    s = np.diag(s[:n_factors])\n    U = U[:, :n_factors]\n    Vt = Vt[:n_factors, :]\n    if regularisation &gt; 0:\n        s = s / (1.0 + regularisation)\n    approx = U @ s @ Vt\n    recon = approx + user_means[:, np.newaxis]\n    return pd.DataFrame(recon, index=ratings.index, columns=ratings.columns)\n\n\ndef implicit_confidence_matrix(\n    interactions: pd.DataFrame, alpha: float = 20.0\n) -&gt; pd.DataFrame:\n    \"\"\"Convert implicit interactions into confidence scores.\"\"\"\n\n    confidence = 1 + alpha * interactions\n    return confidence\n\n\ndef rank_items(scores: pd.Series, top_n: int = 3) -&gt; Recommendation:\n    \"\"\"Return the highest-scoring unrated items for a user.\"\"\"\n\n    user = scores.name if scores.name is not None else \"user\"\n    sorted_items = scores.sort_values(ascending=False)\n    filtered = sorted_items[sorted_items &gt; -np.inf]\n    top_items = filtered.head(top_n)\n    return Recommendation(\n        user=user, ranked_items=list(top_items.index), scores=list(top_items.values)\n    )\n\n\ndef mask_known_items(predictions: pd.Series, original_ratings: pd.Series) -&gt; pd.Series:\n    \"\"\"Mask items that already have explicit feedback.\"\"\"\n\n    return predictions.mask(original_ratings &gt; 0, other=-np.inf)\n\n\ndef precision_at_k(\n    recommended: Sequence[str], relevant: Iterable[str], k: int\n) -&gt; float:\n    \"\"\"Compute precision@k for the provided recommendation list.\"\"\"\n\n    if k &lt;= 0:\n        return 0.0\n    recommended_at_k = recommended[:k]\n    relevant_set = set(relevant)\n    hits = sum(1 for item in recommended_at_k if item in relevant_set)\n    return hits / min(k, len(recommended_at_k) or k)\n\n\ndef recall_at_k(recommended: Sequence[str], relevant: Iterable[str], k: int) -&gt; float:\n    \"\"\"Compute recall@k.\"\"\"\n\n    relevant_list = list(relevant)\n    if not relevant_list:\n        return 0.0\n    recommended_at_k = recommended[:k]\n    hits = sum(1 for item in recommended_at_k if item in set(relevant_list))\n    return hits / len(relevant_list)\n\n\ndef average_precision(\n    recommended: Sequence[str], relevant: Iterable[str], k: int\n) -&gt; float:\n    \"\"\"Compute average precision for a single user.\"\"\"\n\n    relevant_set = set(relevant)\n    if not relevant_set:\n        return 0.0\n    score = 0.0\n    hits = 0\n    for idx, item in enumerate(recommended[:k], start=1):\n        if item in relevant_set:\n            hits += 1\n            score += hits / idx\n    return score / len(relevant_set)\n\n\ndef mean_average_precision(\n    recommendations: Iterable[Sequence[str]], relevants: Iterable[Iterable[str]], k: int\n) -&gt; float:\n    \"\"\"Compute MAP@k across multiple users.\"\"\"\n\n    ap_scores = [\n        average_precision(rec, rel, k) for rec, rel in zip(recommendations, relevants)\n    ]\n    if not ap_scores:\n        return 0.0\n    return float(np.mean(ap_scores))\n\n\ndef demo_recommender_workflow(random_state: int = 57) -&gt; Dict[str, float]:\n    \"\"\"Run a small recommender pipeline and return evaluation metrics.\"\"\"\n\n    ratings = build_demo_user_item_matrix(random_state=random_state)\n    target_user = \"A\"\n    scores = user_based_scores(ratings, target_user, k_neighbors=2)\n    rec = rank_items(scores, top_n=3)\n\n    svd_preds = svd_matrix_factorisation(ratings, n_factors=2)\n    masked_svd = mask_known_items(svd_preds.loc[target_user], ratings.loc[target_user])\n    svd_rec = rank_items(masked_svd, top_n=3)\n\n    implicit = (ratings &gt; 3).astype(int)\n    confidence = implicit_confidence_matrix(implicit)\n    implicit_scores = mask_known_items(\n        confidence.loc[target_user], ratings.loc[target_user]\n    )\n    implicit_rec = rank_items(implicit_scores, top_n=3)\n\n    relevant = {\"Item3\", \"Item5\"}\n    precision = precision_at_k(rec.ranked_items, relevant, k=3)\n    recall = recall_at_k(rec.ranked_items, relevant, k=3)\n    map_score = mean_average_precision(\n        [rec.ranked_items, svd_rec.ranked_items, implicit_rec.ranked_items],\n        [relevant, relevant, relevant],\n        k=3,\n    )\n\n    return {\n        \"precision_at_3\": precision,\n        \"recall_at_3\": recall,\n        \"map_at_3\": map_score,\n        \"svd_top_item\": float(svd_rec.scores[0]),\n    }\n\n\nif __name__ == \"__main__\":\n    metrics = demo_recommender_workflow()\n    for name, value in metrics.items():\n        print(f\"{name}: {value:.3f}\")\n</code></pre>"},{"location":"lessons/day-58-transformers-and-attention/","title":"Day 58 \u2013 Transformers and Attention","text":"<p>Transformers dominate modern sequence modelling. This lesson demonstrates how to:</p> <ul> <li>Assemble encoder\u2013decoder stacks with multi-head self-attention, cross-attention, and position-wise feed-forward layers.</li> <li>Fine-tune pretrained checkpoints (Hugging Face style) with layer-freezing schedules, discriminative learning rates, and LoRA adapters.</li> <li>Visualise token-to-token attention patterns to interpret model focus during inference.</li> <li>Deploy a deterministic tiny transformer classifier for reproducible experiments on compact datasets.</li> </ul> <p>Run <code>python Day_58_Transformers_and_Attention/solutions.py</code> to simulate encoder\u2013decoder passes, generate fine-tuning playbooks, and score demo texts with attention heatmaps.</p>"},{"location":"lessons/day-58-transformers-and-attention/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Transformer helpers and deterministic classifier for Day 58.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterable, List, Sequence, Tuple\n\nimport numpy as np\n\n\n@dataclass\nclass TransformerConfig:\n    \"\"\"Configuration for the tiny encoder\u2013decoder demonstration.\"\"\"\n\n    vocab_size: int = 16\n    d_model: int = 8\n    num_heads: int = 2\n    ff_dim: int = 16\n\n\n@dataclass\nclass EncoderDecoderStates:\n    \"\"\"Container capturing intermediate encoder/decoder representations.\"\"\"\n\n    encoder_hidden: np.ndarray\n    decoder_hidden: np.ndarray\n    cross_attention: np.ndarray\n\n\nDEFAULT_VOCAB: Tuple[str, ...] = (\n    \"&lt;pad&gt;\",\n    \"&lt;unk&gt;\",\n    \"great\",\n    \"bad\",\n    \"product\",\n    \"service\",\n    \"love\",\n    \"hate\",\n    \"fast\",\n    \"slow\",\n    \"support\",\n    \"terrible\",\n    \"amazing\",\n    \"not\",\n    \"boring\",\n    \"exciting\",\n)\nDEFAULT_LABELS: Tuple[str, ...] = (\"negative\", \"positive\")\n\n\nclass TinyTransformerClassifier:\n    \"\"\"Deterministic self-attention classifier for miniature datasets.\"\"\"\n\n    def __init__(\n        self,\n        vocab: Sequence[str] | None = None,\n        labels: Sequence[str] | None = None,\n        config: TransformerConfig | None = None,\n        random_state: int = 58,\n    ) -&gt; None:\n        self.config = config or TransformerConfig(vocab_size=len(DEFAULT_VOCAB))\n        self.vocab_tokens = tuple(vocab) if vocab is not None else DEFAULT_VOCAB\n        self.labels = tuple(labels) if labels is not None else DEFAULT_LABELS\n        self.token_to_id: Dict[str, int] = {\n            token: idx for idx, token in enumerate(self.vocab_tokens)\n        }\n        if \"&lt;unk&gt;\" not in self.token_to_id:\n            self.token_to_id[\"&lt;unk&gt;\"] = len(self.token_to_id)\n        rng = np.random.default_rng(random_state)\n\n        vocab_size = len(self.token_to_id)\n        d_model = self.config.d_model\n        ff_dim = self.config.ff_dim\n        self.embed = rng.normal(0.0, 0.2, size=(vocab_size, d_model))\n        self.W_q = rng.normal(0.0, 0.3, size=(d_model, d_model))\n        self.W_k = rng.normal(0.0, 0.3, size=(d_model, d_model))\n        self.W_v = rng.normal(0.0, 0.3, size=(d_model, d_model))\n        self.W_o = rng.normal(0.0, 0.2, size=(d_model, d_model))\n        self.ff_w1 = rng.normal(0.0, 0.2, size=(d_model, ff_dim))\n        self.ff_b1 = rng.normal(0.0, 0.1, size=(ff_dim,))\n        self.ff_w2 = rng.normal(0.0, 0.2, size=(ff_dim, d_model))\n        self.ff_b2 = rng.normal(0.0, 0.05, size=(d_model,))\n        self.classifier_w = rng.normal(0.0, 0.4, size=(d_model, len(self.labels)))\n        self.classifier_b = rng.normal(0.0, 0.1, size=(len(self.labels),))\n        sentiment = {\n            \"great\": 1.1,\n            \"amazing\": 1.0,\n            \"love\": 1.2,\n            \"fast\": 0.6,\n            \"support\": 0.5,\n            \"bad\": -1.0,\n            \"terrible\": -1.3,\n            \"hate\": -1.2,\n            \"slow\": -0.8,\n            \"boring\": -0.7,\n            \"not\": -0.4,\n            \"service\": -0.1,\n        }\n        self.sentiment_vector = np.zeros(vocab_size)\n        for token, weight in sentiment.items():\n            idx = self.token_to_id.get(token)\n            if idx is not None:\n                self.sentiment_vector[idx] = weight\n        self.lexicon_scale = 0.6\n\n    # ------------------------------------------------------------------\n    # Tokenisation utilities\n    # ------------------------------------------------------------------\n    def tokenize(self, text: str) -&gt; List[int]:\n        \"\"\"Convert raw text into token ids.\"\"\"\n\n        tokens = text.lower().replace(\"!\", \" \").replace(\"?\", \" \").split()\n        unk_id = self.token_to_id[\"&lt;unk&gt;\"]\n        return [self.token_to_id.get(token, unk_id) for token in tokens]\n\n    def pad(self, token_ids: Sequence[int], length: int) -&gt; np.ndarray:\n        \"\"\"Pad or truncate token ids to the provided length.\"\"\"\n\n        pad_id = self.token_to_id.get(\"&lt;pad&gt;\", 0)\n        output = np.full(length, pad_id, dtype=int)\n        seq = np.asarray(token_ids[:length], dtype=int)\n        output[: seq.size] = seq\n        return output\n\n    # ------------------------------------------------------------------\n    # Transformer block\n    # ------------------------------------------------------------------\n    def _reshape_for_heads(self, array: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Reshape (seq, d_model) into (num_heads, seq, head_dim).\"\"\"\n\n        seq_len, d_model = array.shape\n        head_dim = d_model // self.config.num_heads\n        reshaped = array.reshape(seq_len, self.config.num_heads, head_dim)\n        return np.transpose(reshaped, (1, 0, 2))\n\n    def _combine_heads(self, array: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Combine (num_heads, seq, head_dim) into (seq, d_model).\"\"\"\n\n        num_heads, seq_len, head_dim = array.shape\n        combined = np.transpose(array, (1, 0, 2)).reshape(seq_len, num_heads * head_dim)\n        return combined\n\n    def _scaled_dot_product(\n        self, q: np.ndarray, k: np.ndarray, v: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Compute scaled dot-product attention for a single head.\"\"\"\n\n        scale = np.sqrt(q.shape[-1]).astype(float)\n        scores = (q @ k.T) / scale\n        scores -= scores.max(axis=-1, keepdims=True)\n        weights = np.exp(scores)\n        weights /= weights.sum(axis=-1, keepdims=True)\n        attended = weights @ v\n        return attended, weights\n\n    def _self_attention(self, embeddings: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Apply multi-head self-attention to the token embeddings.\"\"\"\n\n        query = embeddings @ self.W_q\n        key = embeddings @ self.W_k\n        value = embeddings @ self.W_v\n\n        q_heads = self._reshape_for_heads(query)\n        k_heads = self._reshape_for_heads(key)\n        v_heads = self._reshape_for_heads(value)\n\n        outputs = []\n        attn_scores = []\n        for head in range(self.config.num_heads):\n            attended, weights = self._scaled_dot_product(\n                q_heads[head], k_heads[head], v_heads[head]\n            )\n            outputs.append(attended)\n            attn_scores.append(weights)\n        concat = self._combine_heads(np.stack(outputs, axis=0))\n        attn_matrix = np.stack(attn_scores, axis=0)\n        transformed = concat @ self.W_o\n        return transformed, attn_matrix\n\n    def _feed_forward(self, tensor: np.ndarray) -&gt; np.ndarray:\n        hidden = np.maximum(0.0, tensor @ self.ff_w1 + self.ff_b1)\n        return hidden @ self.ff_w2 + self.ff_b2\n\n    def forward(self, token_ids: Sequence[int]) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Run the transformer block and return logits and attention.\"\"\"\n\n        if not token_ids:\n            token_ids = [self.token_to_id.get(\"&lt;pad&gt;\", 0)]\n        ids = np.asarray(token_ids, dtype=int)\n        embeddings = self.embed[ids]\n        attn_output, attn_weights = self._self_attention(embeddings)\n        transformed = self._feed_forward(attn_output)\n        pooled = transformed.mean(axis=0)\n        logits = pooled @ self.classifier_w + self.classifier_b\n        lexicon_boost = float(np.sum(self.sentiment_vector[ids]))\n        if logits.shape[0] &gt;= 2:\n            logits = logits.copy()\n            logits[0] -= self.lexicon_scale * lexicon_boost\n            logits[1] += self.lexicon_scale * lexicon_boost\n        return logits, attn_weights\n\n    # ------------------------------------------------------------------\n    # Public API\n    # ------------------------------------------------------------------\n    def predict_proba(self, text: str) -&gt; Dict[str, float]:\n        \"\"\"Return class probabilities for the given text.\"\"\"\n\n        token_ids = self.tokenize(text)\n        logits, _ = self.forward(token_ids)\n        logits = logits - logits.max()\n        probs = np.exp(logits)\n        probs /= probs.sum()\n        return {label: float(prob) for label, prob in zip(self.labels, probs)}\n\n    def classify(self, text: str) -&gt; str:\n        \"\"\"Return the most likely label for a text sequence.\"\"\"\n\n        probs = self.predict_proba(text)\n        return max(probs, key=probs.get)\n\n    def attention_heatmap(self, text: str) -&gt; np.ndarray:\n        \"\"\"Return average attention weights across heads for inspection.\"\"\"\n\n        token_ids = self.tokenize(text)\n        _, attn = self.forward(token_ids)\n        return attn.mean(axis=0)\n\n\ndef build_encoder_decoder_stack(\n    source_tokens: Sequence[int],\n    target_tokens: Sequence[int],\n    config: TransformerConfig | None = None,\n    random_state: int = 58,\n) -&gt; EncoderDecoderStates:\n    \"\"\"Run a compact encoder\u2013decoder simulation and return states.\"\"\"\n\n    cfg = config or TransformerConfig()\n    rng = np.random.default_rng(random_state)\n    d_model = cfg.d_model\n    num_heads = cfg.num_heads\n\n    def multi_head(\n        x: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray\n    ) -&gt; np.ndarray:\n        seq_len = x.shape[0]\n        q = x @ W_q\n        k = x @ W_k\n        v = x @ W_v\n        head_dim = d_model // num_heads\n        q = q.reshape(seq_len, num_heads, head_dim).transpose(1, 0, 2)\n        k = k.reshape(seq_len, num_heads, head_dim).transpose(1, 0, 2)\n        v = v.reshape(seq_len, num_heads, head_dim).transpose(1, 0, 2)\n        outputs = []\n        for head in range(num_heads):\n            scale = np.sqrt(head_dim)\n            weights = (q[head] @ k[head].T) / scale\n            weights -= weights.max(axis=-1, keepdims=True)\n            prob = np.exp(weights)\n            prob /= prob.sum(axis=-1, keepdims=True)\n            outputs.append(prob @ v[head])\n        concat = np.stack(outputs, axis=1).reshape(seq_len, d_model)\n        return concat\n\n    vocab_size = cfg.vocab_size\n    encoder_embed = rng.normal(0.0, 0.4, size=(vocab_size, d_model))\n    decoder_embed = rng.normal(0.0, 0.4, size=(vocab_size, d_model))\n\n    encoder_inp = encoder_embed[np.asarray(source_tokens, dtype=int)]\n    decoder_inp = decoder_embed[np.asarray(target_tokens, dtype=int)]\n\n    W_q = rng.normal(0.0, 0.3, size=(d_model, d_model))\n    W_k = rng.normal(0.0, 0.3, size=(d_model, d_model))\n    W_v = rng.normal(0.0, 0.3, size=(d_model, d_model))\n    encoder_hidden = multi_head(encoder_inp, W_q, W_k, W_v)\n\n    cross_W_q = rng.normal(0.0, 0.3, size=(d_model, d_model))\n    cross_W_k = rng.normal(0.0, 0.3, size=(d_model, d_model))\n    cross_W_v = rng.normal(0.0, 0.3, size=(d_model, d_model))\n\n    decoder_self = multi_head(decoder_inp, W_q, W_k, W_v)\n    seq_len_t = decoder_inp.shape[0]\n    q = (\n        (decoder_self @ cross_W_q)\n        .reshape(seq_len_t, num_heads, d_model // num_heads)\n        .transpose(1, 0, 2)\n    )\n    k = (\n        (encoder_hidden @ cross_W_k)\n        .reshape(encoder_hidden.shape[0], num_heads, d_model // num_heads)\n        .transpose(1, 0, 2)\n    )\n    v = (\n        (encoder_hidden @ cross_W_v)\n        .reshape(encoder_hidden.shape[0], num_heads, d_model // num_heads)\n        .transpose(1, 0, 2)\n    )\n\n    cross_outputs = []\n    for head in range(num_heads):\n        scale = np.sqrt(d_model // num_heads)\n        weights = (q[head] @ k[head].T) / scale\n        weights -= weights.max(axis=-1, keepdims=True)\n        prob = np.exp(weights)\n        prob /= prob.sum(axis=-1, keepdims=True)\n        cross_outputs.append(prob @ v[head])\n    cross_attention = np.stack(cross_outputs, axis=0)\n    decoder_hidden = cross_attention.transpose(1, 0, 2).reshape(seq_len_t, d_model)\n\n    return EncoderDecoderStates(\n        encoder_hidden=encoder_hidden,\n        decoder_hidden=decoder_hidden,\n        cross_attention=cross_attention,\n    )\n\n\ndef fine_tuning_playbook(\n    base_model: str = \"distilbert-base-uncased\",\n    lr: float = 2e-5,\n    weight_decay: float = 0.01,\n    epochs: int = 3,\n) -&gt; Dict[str, object]:\n    \"\"\"Return a Hugging Face style fine-tuning recipe for documentation.\"\"\"\n\n    schedule = [\n        {\n            \"phase\": 1,\n            \"frozen_layers\": \"embeddings+encoder[:2]\",\n            \"learning_rate\": lr / 10,\n        },\n        {\"phase\": 2, \"frozen_layers\": \"encoder[:1]\", \"learning_rate\": lr},\n        {\"phase\": 3, \"adapter\": \"LoRA rank=4\", \"learning_rate\": lr * 1.5},\n    ]\n    return {\n        \"model\": base_model,\n        \"epochs\": epochs,\n        \"weight_decay\": weight_decay,\n        \"discriminative_lrs\": schedule,\n        \"evaluation_strategy\": \"epoch\",\n        \"gradient_accumulation_steps\": 2,\n    }\n\n\ndef demo_attention_visualisation(\n    text: str, classifier: TinyTransformerClassifier | None = None\n) -&gt; Dict[str, object]:\n    \"\"\"Return attention weights and tokens for plotting.\"\"\"\n\n    clf = classifier or TinyTransformerClassifier()\n    token_ids = clf.tokenize(text)\n    heatmap = clf.attention_heatmap(text)\n    tokens = [\n        clf.vocab_tokens[idx] if idx &lt; len(clf.vocab_tokens) else \"&lt;extra&gt;\"\n        for idx in token_ids\n    ]\n    return {\"tokens\": tokens, \"attention\": heatmap}\n\n\ndef run_demo_classification(\n    texts: Iterable[str], classifier: TinyTransformerClassifier | None = None\n) -&gt; List[Dict[str, object]]:\n    \"\"\"Score a batch of texts with deterministic predictions and attention.\"\"\"\n\n    clf = classifier or TinyTransformerClassifier()\n    outputs: List[Dict[str, object]] = []\n    for text in texts:\n        probs = clf.predict_proba(text)\n        heatmap = clf.attention_heatmap(text)\n        outputs.append(\n            {\n                \"text\": text,\n                \"prediction\": clf.classify(text),\n                \"probs\": probs,\n                \"attention\": heatmap,\n            }\n        )\n    return outputs\n\n\ndef _demo() -&gt; None:\n    classifier = TinyTransformerClassifier()\n    texts = [\"Great product and amazing support\", \"Terrible and slow service\"]\n    reports = run_demo_classification(texts, classifier)\n    for report in reports:\n        print(f\"Text: {report['text']}\")\n        print(f\"Prediction: {report['prediction']} \u2013 probs: {report['probs']}\")\n        print(f\"Attention shape: {report['attention'].shape}\\n\")\n\n\nif __name__ == \"__main__\":\n    _demo()\n</code></pre>"},{"location":"lessons/day-59-generative-models/","title":"Day 59 \u2013 Generative Models","text":"<p>Generative models synthesise data, compress signals, and enable controllable creativity. In this lesson you will:</p> <ul> <li>Contrast autoencoders, variational autoencoders, GANs, and diffusion models across objectives and sampling procedures.</li> <li>Optimise lightweight autoencoders and VAEs on synthetic data to observe reconstruction loss curves.</li> <li>Understand GAN training dynamics with simplified generator\u2013discriminator updates and stability heuristics.</li> <li>Explore diffusion process fundamentals: forward noising, denoising score matching, and scheduler design.</li> </ul> <p>Execute <code>python Day_59_Generative_Models/solutions.py</code> to run miniature training loops that log decreasing reconstruction losses and summarise practical tuning tips.</p>"},{"location":"lessons/day-59-generative-models/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Synthetic generative modelling routines for Day 59.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\nimport numpy as np\n\n\n@dataclass\nclass TrainingLog:\n    \"\"\"Container for training statistics collected per iteration.\"\"\"\n\n    losses: List[float]\n    reconstructions: np.ndarray\n\n\ndef generate_swiss_roll(n_samples: int = 128, random_state: int = 59) -&gt; np.ndarray:\n    \"\"\"Return a 2D swiss-roll style dataset for reconstruction demos.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    theta = rng.uniform(0, 3 * np.pi, size=n_samples)\n    height = rng.uniform(-1.0, 1.0, size=n_samples)\n    x = theta * np.cos(theta)\n    y = theta * np.sin(theta)\n    data = np.column_stack((x, y))\n    data -= data.mean(axis=0, keepdims=True)\n    data /= np.abs(data).max()\n    data += 0.05 * height[:, None]\n    return data.astype(np.float64)\n\n\ndef _tanh(x: np.ndarray) -&gt; np.ndarray:\n    return np.tanh(x)\n\n\ndef _tanh_grad(x: np.ndarray) -&gt; np.ndarray:\n    t = np.tanh(x)\n    return 1.0 - t**2\n\n\ndef train_autoencoder_synthetic(\n    data: np.ndarray | None = None,\n    hidden_dim: int = 3,\n    epochs: int = 200,\n    lr: float = 0.05,\n    random_state: int = 59,\n) -&gt; TrainingLog:\n    \"\"\"Train a deterministic autoencoder on synthetic data.\"\"\"\n\n    X = data if data is not None else generate_swiss_roll(random_state=random_state)\n    rng = np.random.default_rng(random_state)\n    n_features = X.shape[1]\n    W1 = rng.normal(0.0, 0.2, size=(n_features, hidden_dim))\n    b1 = np.zeros(hidden_dim)\n    W2 = rng.normal(0.0, 0.2, size=(hidden_dim, n_features))\n    b2 = np.zeros(n_features)\n\n    losses: List[float] = []\n    for _ in range(epochs):\n        z_lin = X @ W1 + b1\n        z = _tanh(z_lin)\n        recon = z @ W2 + b2\n        diff = recon - X\n        loss = float(np.mean(diff**2))\n        losses.append(loss)\n\n        grad_recon = (2.0 / X.shape[0]) * diff\n        grad_W2 = z.T @ grad_recon\n        grad_b2 = grad_recon.sum(axis=0)\n        grad_hidden = (grad_recon @ W2.T) * _tanh_grad(z_lin)\n        grad_W1 = X.T @ grad_hidden\n        grad_b1 = grad_hidden.sum(axis=0)\n\n        W2 -= lr * grad_W2\n        b2 -= lr * grad_b2\n        W1 -= lr * grad_W1\n        b1 -= lr * grad_b1\n\n    final_recon = _tanh(X @ W1 + b1) @ W2 + b2\n    return TrainingLog(losses=losses, reconstructions=final_recon)\n\n\ndef train_variational_autoencoder_synthetic(\n    data: np.ndarray | None = None,\n    latent_dim: int = 2,\n    epochs: int = 200,\n    lr: float = 0.05,\n    kl_weight: float = 0.01,\n    random_state: int = 59,\n) -&gt; TrainingLog:\n    \"\"\"Run a minimal VAE with reparameterisation on synthetic data.\"\"\"\n\n    X = data if data is not None else generate_swiss_roll(random_state=random_state + 1)\n    rng = np.random.default_rng(random_state)\n    n_features = X.shape[1]\n    W_mu = rng.normal(0.0, 0.2, size=(n_features, latent_dim))\n    b_mu = np.zeros(latent_dim)\n    W_logvar = rng.normal(0.0, 0.2, size=(n_features, latent_dim))\n    b_logvar = np.zeros(latent_dim)\n    W_dec = rng.normal(0.0, 0.2, size=(latent_dim, n_features))\n    b_dec = np.zeros(n_features)\n\n    losses: List[float] = []\n    for _ in range(epochs):\n        mu = X @ W_mu + b_mu\n        logvar = X @ W_logvar + b_logvar\n        std = np.exp(0.5 * logvar)\n        eps = rng.normal(0.0, 1.0, size=mu.shape)\n        z = mu + eps * std\n        recon = _tanh(z) @ W_dec + b_dec\n        diff = recon - X\n        recon_loss = np.mean(diff**2)\n        kl_div = -0.5 * np.mean(1 + logvar - mu**2 - np.exp(logvar))\n        loss = float(recon_loss + kl_weight * kl_div)\n        losses.append(loss)\n\n        grad_recon = (2.0 / X.shape[0]) * diff\n        grad_W_dec = (_tanh(z)).T @ grad_recon\n        grad_b_dec = grad_recon.sum(axis=0)\n        grad_hidden = (grad_recon @ W_dec.T) * (1 - np.tanh(z) ** 2)\n\n        grad_mu = grad_hidden + kl_weight * (mu / X.shape[0])\n        grad_logvar = (\n            grad_hidden * eps * std * 0.5\n            + kl_weight * 0.5 * (np.exp(logvar) - 1) / X.shape[0]\n        )\n\n        grad_W_mu = X.T @ grad_mu\n        grad_b_mu = grad_mu.sum(axis=0)\n        grad_W_logvar = X.T @ grad_logvar\n        grad_b_logvar = grad_logvar.sum(axis=0)\n\n        W_dec -= lr * grad_W_dec\n        b_dec -= lr * grad_b_dec\n        W_mu -= lr * grad_W_mu\n        b_mu -= lr * grad_b_mu\n        W_logvar -= lr * grad_W_logvar\n        b_logvar -= lr * grad_b_logvar\n\n    final_z = X @ W_mu + b_mu\n    final_recon = _tanh(final_z) @ W_dec + b_dec\n    return TrainingLog(losses=losses, reconstructions=final_recon)\n\n\ndef train_diffusion_denoiser(\n    data: np.ndarray | None = None,\n    timesteps: int = 10,\n    epochs: int = 200,\n    lr: float = 0.05,\n    random_state: int = 59,\n) -&gt; TrainingLog:\n    \"\"\"Train a denoiser to recover clean data from a simple diffusion step.\"\"\"\n\n    X = data if data is not None else generate_swiss_roll(random_state=random_state + 2)\n    rng = np.random.default_rng(random_state)\n    n_features = X.shape[1]\n    W = rng.normal(0.0, 0.2, size=(n_features, n_features))\n    b = np.zeros(n_features)\n    losses: List[float] = []\n\n    betas = np.linspace(1e-3, 5e-2, timesteps)\n    alphas = 1.0 - betas\n    alpha_bar = np.cumprod(alphas)\n\n    for _ in range(epochs):\n        t = rng.integers(0, timesteps)\n        noise = rng.normal(0.0, 1.0, size=X.shape)\n        noisy = np.sqrt(alpha_bar[t]) * X + np.sqrt(1 - alpha_bar[t]) * noise\n        pred_noise = noisy @ W + b\n        diff = pred_noise - noise\n        loss = float(np.mean(diff**2))\n        losses.append(loss)\n\n        grad = (2.0 / X.shape[0]) * diff\n        grad_W = noisy.T @ grad\n        grad_b = grad.sum(axis=0)\n        W -= lr * grad_W\n        b -= lr * grad_b\n\n    final_noise = X @ W + b\n    return TrainingLog(losses=losses, reconstructions=final_noise)\n\n\ndef gan_training_summary(\n    steps: int = 100, random_state: int = 59\n) -&gt; List[Dict[str, float]]:\n    \"\"\"Simulate GAN training metrics to illustrate convergence heuristics.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    real_mean = 1.5\n    gen_mean = rng.normal(-1.0, 0.1)\n    log: List[Dict[str, float]] = []\n    for step in range(steps):\n        gen_mean += 0.03 * (real_mean - gen_mean)\n        discriminator_loss = float(np.exp(-abs(real_mean - gen_mean)))\n        generator_loss = float(abs(real_mean - gen_mean))\n        log.append(\n            {\n                \"step\": step,\n                \"gen_loss\": generator_loss,\n                \"disc_loss\": discriminator_loss,\n                \"gen_mean\": gen_mean,\n            }\n        )\n    return log\n\n\ndef summarise_generative_objectives() -&gt; Dict[str, str]:\n    \"\"\"Return cheat-sheet style descriptions of key generative objectives.\"\"\"\n\n    return {\n        \"autoencoder\": \"Minimise reconstruction error with deterministic encoder/decoder.\",\n        \"vae\": \"Optimise ELBO = reconstruction + KL divergence to prior.\",\n        \"gan\": \"Adversarial min-max between generator and discriminator losses.\",\n        \"diffusion\": \"Score matching/denoising losses across noisy timesteps.\",\n    }\n\n\ndef run_all_demos(random_state: int = 59) -&gt; Dict[str, object]:\n    \"\"\"Convenience entrypoint mirroring the CLI behaviour.\"\"\"\n\n    data = generate_swiss_roll(random_state=random_state)\n    ae = train_autoencoder_synthetic(data=data, random_state=random_state)\n    vae = train_variational_autoencoder_synthetic(data=data, random_state=random_state)\n    diffusion = train_diffusion_denoiser(data=data, random_state=random_state)\n    gan_log = gan_training_summary(random_state=random_state)\n    return {\n        \"autoencoder\": ae,\n        \"vae\": vae,\n        \"diffusion\": diffusion,\n        \"gan\": gan_log,\n        \"objectives\": summarise_generative_objectives(),\n    }\n\n\ndef _demo() -&gt; None:\n    stats = run_all_demos()\n    print(\n        f\"Autoencoder start/end loss: {stats['autoencoder'].losses[0]:.4f} -&gt; {stats['autoencoder'].losses[-1]:.4f}\"\n    )\n    print(\n        f\"VAE start/end loss: {stats['vae'].losses[0]:.4f} -&gt; {stats['vae'].losses[-1]:.4f}\"\n    )\n    print(\n        f\"Diffusion start/end loss: {stats['diffusion'].losses[0]:.4f} -&gt; {stats['diffusion'].losses[-1]:.4f}\"\n    )\n    print(f\"GAN terminal generator mean: {stats['gan'][-1]['gen_mean']:.3f}\")\n\n\nif __name__ == \"__main__\":\n    _demo()\n</code></pre>"},{"location":"lessons/day-60-graph-and-geometric-learning/","title":"Day 60 \u2013 Graph and Geometric Learning","text":"<p>Graph neural networks capture relational structure beyond Euclidean grids. This lesson focuses on:</p> <ul> <li>Building GraphSAGE neighbourhood aggregators and graph attention networks (GAT) from first principles.</li> <li>Preparing toy graphs and feature matrices compatible with PyTorch Geometric or DGL workflows.</li> <li>Training node classifiers with message passing, skip connections, and softmax heads on miniature datasets.</li> <li>Evaluating accuracy, attention weights, and representation quality for stakeholder-ready reporting.</li> </ul> <p>Run <code>python Day_60_Graph_and_Geometric_Learning/solutions.py</code> to inspect handcrafted GraphSAGE/GAT layers, monitor training metrics on a toy citation-style graph, and export feature embeddings.</p>"},{"location":"lessons/day-60-graph-and-geometric-learning/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Graph neural network helpers for Day 60.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\nimport numpy as np\n\n\n@dataclass\nclass GraphData:\n    \"\"\"Simple container for toy graph node classification tasks.\"\"\"\n\n    features: np.ndarray\n    adjacency: np.ndarray\n    labels: np.ndarray\n\n\ndef build_toy_graph() -&gt; GraphData:\n    \"\"\"Create a reproducible toy graph with two communities.\"\"\"\n\n    features = np.array(\n        [\n            [1.0, 0.2, 0.8],\n            [0.9, 0.1, 0.7],\n            [1.1, 0.25, 0.9],\n            [-0.2, 1.0, 0.1],\n            [-0.1, 0.9, 0.2],\n            [-0.3, 1.1, 0.15],\n        ],\n        dtype=float,\n    )\n    labels = np.array([0, 0, 0, 1, 1, 1], dtype=int)\n    adjacency = np.array(\n        [\n            [1, 1, 1, 0, 0, 0],\n            [1, 1, 1, 0, 0, 0],\n            [1, 1, 1, 0, 0, 1],\n            [0, 0, 0, 1, 1, 1],\n            [0, 0, 0, 1, 1, 1],\n            [0, 0, 1, 1, 1, 1],\n        ],\n        dtype=float,\n    )\n    adjacency = adjacency + np.eye(adjacency.shape[0]) * 0.0  # ensure float copy\n    return GraphData(features=features, adjacency=adjacency, labels=labels)\n\n\ndef _softmax(logits: np.ndarray) -&gt; np.ndarray:\n    logits = logits - logits.max(axis=1, keepdims=True)\n    exp = np.exp(logits)\n    exp /= exp.sum(axis=1, keepdims=True)\n    return exp\n\n\nclass GraphSAGEClassifier:\n    \"\"\"Mean-aggregator GraphSAGE classifier with manual gradients.\"\"\"\n\n    def __init__(\n        self, hidden_dim: int = 6, num_classes: int = 2, random_state: int = 60\n    ) -&gt; None:\n        self.hidden_dim = hidden_dim\n        self.num_classes = num_classes\n        self.random_state = random_state\n        self.W_self: np.ndarray | None = None\n        self.W_neigh: np.ndarray | None = None\n        self.b_hidden: np.ndarray | None = None\n        self.W_out: np.ndarray | None = None\n        self.b_out: np.ndarray | None = None\n\n    def _ensure_params(self, input_dim: int) -&gt; None:\n        if self.W_self is not None:\n            return\n        rng = np.random.default_rng(self.random_state)\n        self.W_self = rng.normal(0.0, 0.4, size=(input_dim, self.hidden_dim))\n        self.W_neigh = rng.normal(0.0, 0.4, size=(input_dim, self.hidden_dim))\n        self.b_hidden = np.zeros(self.hidden_dim)\n        self.W_out = rng.normal(0.0, 0.4, size=(self.hidden_dim, self.num_classes))\n        self.b_out = np.zeros(self.num_classes)\n\n    def forward(self, data: GraphData) -&gt; Dict[str, np.ndarray]:\n        assert self.W_self is not None and self.W_neigh is not None\n        assert (\n            self.b_hidden is not None\n            and self.W_out is not None\n            and self.b_out is not None\n        )\n        features = data.features\n        adjacency = data.adjacency\n        degrees = adjacency.sum(axis=1, keepdims=True)\n        degrees[degrees == 0] = 1.0\n        neigh_mean = adjacency @ features / degrees\n        hidden_pre = features @ self.W_self + neigh_mean @ self.W_neigh + self.b_hidden\n        hidden = np.maximum(0.0, hidden_pre)\n        logits = hidden @ self.W_out + self.b_out\n        probs = _softmax(logits)\n        return {\n            \"logits\": logits,\n            \"hidden\": hidden,\n            \"hidden_pre\": hidden_pre,\n            \"neigh\": neigh_mean,\n            \"probs\": probs,\n        }\n\n    def train(self, data: GraphData, epochs: int = 200, lr: float = 0.1) -&gt; List[float]:\n        self._ensure_params(data.features.shape[1])\n        assert self.W_self is not None and self.W_neigh is not None\n        assert (\n            self.b_hidden is not None\n            and self.W_out is not None\n            and self.b_out is not None\n        )\n        y = data.labels\n        y_onehot = np.eye(self.num_classes)[y]\n        losses: List[float] = []\n        for _ in range(epochs):\n            forward = self.forward(data)\n            probs = forward[\"probs\"]\n            loss = float(-np.sum(y_onehot * np.log(probs + 1e-9)) / y.shape[0])\n            losses.append(loss)\n\n            grad_logits = (probs - y_onehot) / y.shape[0]\n            grad_W_out = forward[\"hidden\"].T @ grad_logits\n            grad_b_out = grad_logits.sum(axis=0)\n            grad_hidden = grad_logits @ self.W_out.T\n            grad_hidden_pre = grad_hidden * (forward[\"hidden_pre\"] &gt; 0)\n\n            grad_W_self = data.features.T @ grad_hidden_pre\n            grad_W_neigh = forward[\"neigh\"].T @ grad_hidden_pre\n            grad_b_hidden = grad_hidden_pre.sum(axis=0)\n\n            self.W_out -= lr * grad_W_out\n            self.b_out -= lr * grad_b_out\n            self.W_self -= lr * grad_W_self\n            self.W_neigh -= lr * grad_W_neigh\n            self.b_hidden -= lr * grad_b_hidden\n        return losses\n\n    def predict(self, data: GraphData) -&gt; np.ndarray:\n        probs = self.forward(data)[\"probs\"]\n        return probs.argmax(axis=1)\n\n    def accuracy(self, data: GraphData) -&gt; float:\n        preds = self.predict(data)\n        return float((preds == data.labels).mean())\n\n\nclass GraphAttentionClassifier:\n    \"\"\"Attention-based aggregator with trainable linear head.\"\"\"\n\n    def __init__(\n        self, temperature: float = 0.5, num_classes: int = 2, random_state: int = 60\n    ) -&gt; None:\n        self.temperature = temperature\n        self.num_classes = num_classes\n        self.random_state = random_state\n        self.W_out: np.ndarray | None = None\n        self.b_out: np.ndarray | None = None\n        self._embeddings: np.ndarray | None = None\n        self._attention: np.ndarray | None = None\n\n    def _attention_matrix(\n        self, features: np.ndarray, adjacency: np.ndarray\n    ) -&gt; np.ndarray:\n        sim = (features @ features.T) / self.temperature\n        sim -= sim.max(axis=1, keepdims=True)\n        weights = np.exp(sim)\n        masked = weights * adjacency\n        normaliser = masked.sum(axis=1, keepdims=True)\n        normaliser[normaliser == 0] = 1.0\n        return masked / normaliser\n\n    def encode(self, data: GraphData) -&gt; np.ndarray:\n        adjacency = data.adjacency.copy()\n        np.fill_diagonal(adjacency, 1.0)\n        attn = self._attention_matrix(data.features, adjacency)\n        self._attention = attn\n        embeddings = attn @ data.features\n        self._embeddings = embeddings\n        return embeddings\n\n    def train(self, data: GraphData, epochs: int = 200, lr: float = 0.1) -&gt; List[float]:\n        embeddings = self.encode(data)\n        if self.W_out is None or self.b_out is None:\n            rng = np.random.default_rng(self.random_state)\n            self.W_out = rng.normal(\n                0.0, 0.4, size=(embeddings.shape[1], self.num_classes)\n            )\n            self.b_out = np.zeros(self.num_classes)\n        assert self.W_out is not None and self.b_out is not None\n        y = data.labels\n        y_onehot = np.eye(self.num_classes)[y]\n        losses: List[float] = []\n        for _ in range(epochs):\n            logits = embeddings @ self.W_out + self.b_out\n            probs = _softmax(logits)\n            loss = float(-np.sum(y_onehot * np.log(probs + 1e-9)) / y.shape[0])\n            losses.append(loss)\n\n            grad_logits = (probs - y_onehot) / y.shape[0]\n            grad_W_out = embeddings.T @ grad_logits\n            grad_b_out = grad_logits.sum(axis=0)\n            self.W_out -= lr * grad_W_out\n            self.b_out -= lr * grad_b_out\n        return losses\n\n    def predict(self, data: GraphData) -&gt; np.ndarray:\n        embeddings = self.encode(data) if self._embeddings is None else self._embeddings\n        assert self.W_out is not None and self.b_out is not None\n        logits = embeddings @ self.W_out + self.b_out\n        probs = _softmax(logits)\n        return probs.argmax(axis=1)\n\n    def attention_matrix(self, data: GraphData) -&gt; np.ndarray:\n        if self._attention is None:\n            self.encode(data)\n        assert self._attention is not None\n        return self._attention\n\n    def accuracy(self, data: GraphData) -&gt; float:\n        preds = self.predict(data)\n        return float((preds == data.labels).mean())\n\n\ndef train_node_classifiers(random_state: int = 60) -&gt; Dict[str, object]:\n    \"\"\"Train both GraphSAGE and graph attention classifiers on the toy graph.\"\"\"\n\n    data = build_toy_graph()\n    sage = GraphSAGEClassifier(random_state=random_state)\n    gat = GraphAttentionClassifier(random_state=random_state)\n    sage_losses = sage.train(data, epochs=200, lr=0.1)\n    gat_losses = gat.train(data, epochs=200, lr=0.1)\n    results = {\n        \"graphsage_accuracy\": sage.accuracy(data),\n        \"gat_accuracy\": gat.accuracy(data),\n        \"graphsage_losses\": sage_losses,\n        \"gat_losses\": gat_losses,\n        \"attention_matrix\": gat.attention_matrix(data),\n    }\n    return results\n\n\ndef _demo() -&gt; None:\n    results = train_node_classifiers()\n    print(\n        f\"GraphSAGE accuracy: {results['graphsage_accuracy']:.3f} | GAT accuracy: {results['gat_accuracy']:.3f}\"\n    )\n    print(f\"Attention matrix row sums: {results['attention_matrix'].sum(axis=1)}\")\n\n\nif __name__ == \"__main__\":\n    _demo()\n</code></pre>"},{"location":"lessons/day-61-reinforcement-and-offline-learning/","title":"Day 61 \u2013 Reinforcement and Offline Learning","text":"<p>Reinforcement learning (RL) balances exploration and exploitation while offline evaluation keeps policies safe. After this lesson you can:</p> <ul> <li>Compare value-based, policy-based, and actor\u2013critic methods across episodic control problems.</li> <li>Simulate contextual bandits and conservative offline policy evaluation with replay buffers and importance sampling.</li> <li>Analyse stability tricks: entropy bonuses, target networks, batch-constrained Q-learning, and doubly robust estimators.</li> <li>Reproduce seeded experiments that converge to expected reward thresholds for regression tests.</li> </ul> <p>Execute <code>python Day_61_Reinforcement_and_Offline_Learning/solutions.py</code> to walk through deterministic policy optimisation, offline evaluation diagnostics, and bandit baselines.</p>"},{"location":"lessons/day-61-reinforcement-and-offline-learning/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Reinforcement learning utilities for Day 61.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\nimport numpy as np\n\n\n@dataclass\nclass EpisodeLog:\n    \"\"\"Track rewards and moving averages for RL experiments.\"\"\"\n\n    rewards: List[float]\n    moving_average: List[float]\n    policy_parameter: float\n\n\ndef _sigmoid(x: float) -&gt; float:\n    return 1.0 / (1.0 + np.exp(-x))\n\n\ndef run_policy_gradient_bandit(\n    episodes: int = 200,\n    lr: float = 0.2,\n    random_state: int = 61,\n) -&gt; EpisodeLog:\n    \"\"\"Train a REINFORCE-style policy on a two-armed bandit.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    theta = 0.0\n    baseline = 0.0\n    rewards: List[float] = []\n    moving_avg: List[float] = []\n    for episode in range(episodes):\n        prob_action_one = _sigmoid(theta)\n        action = 1 if rng.random() &lt; prob_action_one else 0\n        reward = float(rng.normal(1.2, 0.05) if action == 1 else rng.normal(0.2, 0.05))\n        rewards.append(reward)\n        baseline = 0.9 * baseline + 0.1 * reward\n        grad = (reward - baseline) * (action - prob_action_one)\n        theta += lr * grad\n        moving_avg.append(float(np.mean(rewards[max(0, episode - 19) : episode + 1])))\n    return EpisodeLog(\n        rewards=rewards, moving_average=moving_avg, policy_parameter=float(theta)\n    )\n\n\n@dataclass\nclass QLearningResult:\n    \"\"\"Container for Q-learning progress on a deterministic MDP.\"\"\"\n\n    q_values: np.ndarray\n    rewards: List[float]\n\n\ndef run_q_learning(\n    episodes: int = 200,\n    gamma: float = 0.9,\n    lr: float = 0.3,\n    epsilon: float = 0.2,\n    random_state: int = 61,\n) -&gt; QLearningResult:\n    \"\"\"Run tabular Q-learning on a two-state MDP.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    q_values = np.zeros((2, 2))\n    rewards: List[float] = []\n    transition = {\n        (0, 0): (0, 0.5),\n        (0, 1): (1, 1.0),\n        (1, 0): (0, 0.4),\n        (1, 1): (1, 1.2),\n    }\n    state = 0\n    for _ in range(episodes):\n        if rng.random() &lt; epsilon:\n            action = rng.integers(0, 2)\n        else:\n            action = int(np.argmax(q_values[state]))\n        next_state, reward = transition[(state, action)]\n        rewards.append(reward)\n        best_next = np.max(q_values[next_state])\n        td_target = reward + gamma * best_next\n        td_error = td_target - q_values[state, action]\n        q_values[state, action] += lr * td_error\n        state = next_state\n    return QLearningResult(q_values=q_values, rewards=rewards)\n\n\n@dataclass\nclass BanditSummary:\n    \"\"\"Summary statistics for epsilon-greedy contextual bandit.\"\"\"\n\n    action_counts: np.ndarray\n    cumulative_reward: float\n    average_reward: float\n\n\ndef run_contextual_bandit(\n    steps: int = 300,\n    epsilon: float = 0.1,\n    random_state: int = 61,\n) -&gt; BanditSummary:\n    \"\"\"Execute epsilon-greedy strategy on a contextual bandit.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    action_values = np.zeros(3)\n    action_counts = np.zeros(3, dtype=int)\n    reward_means = np.array([0.3, 0.8, 1.1])\n    total_reward = 0.0\n    for step in range(steps):\n        if rng.random() &lt; epsilon:\n            action = rng.integers(0, 3)\n        else:\n            action = int(np.argmax(action_values))\n        reward = float(rng.normal(reward_means[action], 0.05))\n        action_counts[action] += 1\n        total_reward += reward\n        step_size = 1.0 / action_counts[action]\n        action_values[action] += step_size * (reward - action_values[action])\n    return BanditSummary(\n        action_counts=action_counts,\n        cumulative_reward=total_reward,\n        average_reward=total_reward / steps,\n    )\n\n\ndef offline_evaluation(\n    num_samples: int = 500,\n    random_state: int = 61,\n) -&gt; Dict[str, float]:\n    \"\"\"Estimate evaluation policy performance with weighted importance sampling.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    reward_means = np.array([0.2, 0.5, 1.0])\n    behaviour_policy = np.array([0.5, 0.4, 0.1])\n    evaluation_policy = np.array([0.1, 0.2, 0.7])\n    weights: List[float] = []\n    weighted_rewards: List[float] = []\n    for _ in range(num_samples):\n        action = rng.choice(3, p=behaviour_policy)\n        reward = float(rng.normal(reward_means[action], 0.1))\n        importance = evaluation_policy[action] / behaviour_policy[action]\n        weights.append(importance)\n        weighted_rewards.append(importance * reward)\n    weights_arr = np.array(weights)\n    weighted_rewards_arr = np.array(weighted_rewards)\n    estimate = float(weighted_rewards_arr.sum() / (weights_arr.sum() + 1e-9))\n    ess = float((weights_arr.sum() ** 2) / (np.sum(weights_arr**2) + 1e-9))\n    return {\"estimate\": estimate, \"effective_sample_size\": ess}\n\n\ndef run_rl_suite(random_state: int = 61) -&gt; Dict[str, object]:\n    \"\"\"Run policy/value/bandit/offline learning experiments and aggregate metrics.\"\"\"\n\n    pg = run_policy_gradient_bandit(random_state=random_state)\n    ql = run_q_learning(random_state=random_state)\n    bandit = run_contextual_bandit(random_state=random_state)\n    offline = offline_evaluation(random_state=random_state)\n    return {\n        \"policy_gradient\": pg,\n        \"q_learning\": ql,\n        \"bandit\": bandit,\n        \"offline\": offline,\n    }\n\n\ndef _demo() -&gt; None:\n    results = run_rl_suite()\n    print(\n        f\"Policy gradient final avg reward: {results['policy_gradient'].moving_average[-1]:.3f}\"\n    )\n    print(f\"Q-learning Q-values: {results['q_learning'].q_values}\")\n    print(f\"Bandit average reward: {results['bandit'].average_reward:.3f}\")\n    print(\n        f\"Offline estimate: {results['offline']['estimate']:.3f} (ESS={results['offline']['effective_sample_size']:.1f})\"\n    )\n\n\nif __name__ == \"__main__\":\n    _demo()\n</code></pre>"},{"location":"lessons/day-62-model-interpretability-and-fairness/","title":"Day 62 \u2013 Model Interpretability and Fairness","text":"<p>Explainable and responsible AI practices underpin trustworthy analytics. After this lesson you will:</p> <ul> <li>Compute additive SHAP-style attributions for linear models and verify they sum to the predicted score.</li> <li>Fit lightweight LIME surrogates around individual observations using locally weighted regression.</li> <li>Produce counterfactual examples that respect feature bounds to meet target outcomes.</li> <li>Quantify bias with statistical parity, disparate impact, and equal opportunity metrics.</li> <li>Apply simple reweighing mitigation to close gaps in simulated lending data.</li> </ul> <p>Run <code>python Day_62_Model_Interpretability_and_Fairness/solutions.py</code> to walk through interpretability utilities, fairness diagnostics, and mitigation experiments on deterministic toy datasets.</p>"},{"location":"lessons/day-62-model-interpretability-and-fairness/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Interpretability and fairness helpers for Day 62.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Mapping, Sequence, Tuple\n\nimport numpy as np\nimport pandas as pd\n\n\n@dataclass\nclass LinearModel:\n    \"\"\"Simple linear model container.\"\"\"\n\n    coefficients: np.ndarray\n    intercept: float\n    feature_names: Sequence[str]\n\n    def predict(self, features: Sequence[float]) -&gt; float:\n        vector = np.asarray(features, dtype=float)\n        return float(self.intercept + np.dot(self.coefficients, vector))\n\n\n@dataclass\nclass ShapExplanation:\n    \"\"\"Container for SHAP-style additive explanations.\"\"\"\n\n    base_value: float\n    contributions: np.ndarray\n\n    def reconstructed_prediction(self) -&gt; float:\n        return float(self.base_value + self.contributions.sum())\n\n\n@dataclass\nclass LimeExplanation:\n    \"\"\"Container for LIME-style local linear approximations.\"\"\"\n\n    intercept: float\n    weights: np.ndarray\n    prediction: float\n\n    def local_prediction(self, instance: Sequence[float]) -&gt; float:\n        vector = np.asarray(instance, dtype=float)\n        return float(self.intercept + np.dot(self.weights, vector))\n\n\n@dataclass\nclass CounterfactualResult:\n    \"\"\"Result of a counterfactual search.\"\"\"\n\n    original_prediction: float\n    target: float\n    counterfactual_features: np.ndarray\n    counterfactual_prediction: float\n    delta: np.ndarray\n\n\n@dataclass\nclass FairnessReport:\n    \"\"\"Bias metrics calculated on binary outcomes.\"\"\"\n\n    statistical_parity: float\n    disparate_impact: float\n    equal_opportunity: float\n\n\ndef load_credit_dataset() -&gt; pd.DataFrame:\n    \"\"\"Return a deterministic lending dataset for fairness experiments.\"\"\"\n\n    rng = np.random.default_rng(62)\n    records: List[Dict[str, float]] = []\n    for credit_score in (580, 620, 660, 700, 740):\n        for income in (42_000, 58_000, 74_000):\n            for gender in (\"F\", \"M\"):\n                base_prob = (\n                    0.15 + 0.0006 * (credit_score - 600) + 0.000002 * (income - 50_000)\n                )\n                shift = -0.04 if gender == \"F\" else 0.0\n                approval = rng.random() &lt; (base_prob + shift)\n                records.append(\n                    {\n                        \"credit_score\": credit_score,\n                        \"income\": income,\n                        \"gender\": gender,\n                        \"approved\": float(approval),\n                        \"default_risk\": 0.35\n                        - 0.0004 * credit_score\n                        - 0.0000015 * income\n                        + (0.02 if gender == \"F\" else 0.0),\n                    }\n                )\n    return pd.DataFrame.from_records(records)\n\n\ndef train_default_risk_model() -&gt; LinearModel:\n    \"\"\"Fit a closed-form linear regression on the credit dataset.\"\"\"\n\n    df = load_credit_dataset()\n    feature_names = [\"credit_score\", \"income\", \"is_female\"]\n    X = np.column_stack(\n        [\n            df[\"credit_score\"].to_numpy(dtype=float),\n            df[\"income\"].to_numpy(dtype=float),\n            (df[\"gender\"] == \"F\").to_numpy(dtype=float),\n        ]\n    )\n    y = df[\"default_risk\"].to_numpy(dtype=float)\n    X_design = np.column_stack([np.ones(len(df)), X])\n    coefficients, *_ = np.linalg.lstsq(X_design, y, rcond=None)\n    intercept = float(coefficients[0])\n    weights = np.asarray(coefficients[1:], dtype=float)\n    return LinearModel(\n        coefficients=weights, intercept=intercept, feature_names=feature_names\n    )\n\n\ndef _baseline_from_dataset(\n    model: LinearModel, dataset: pd.DataFrame | None = None\n) -&gt; Tuple[float, np.ndarray]:\n    if dataset is None:\n        dataset = load_credit_dataset()\n    baseline_features = np.column_stack(\n        [\n            dataset[\"credit_score\"].to_numpy(dtype=float),\n            dataset[\"income\"].to_numpy(dtype=float),\n            (dataset[\"gender\"] == \"F\").to_numpy(dtype=float),\n        ]\n    ).mean(axis=0)\n    base_value = model.predict(baseline_features)\n    return base_value, baseline_features\n\n\ndef compute_shap_values(\n    model: LinearModel,\n    instance: Sequence[float],\n    dataset: pd.DataFrame | None = None,\n) -&gt; ShapExplanation:\n    \"\"\"Return additive SHAP-style contributions for a linear model.\"\"\"\n\n    base_value, baseline_features = _baseline_from_dataset(model, dataset)\n    instance_arr = np.asarray(instance, dtype=float)\n    contributions = model.coefficients * (instance_arr - baseline_features)\n    return ShapExplanation(base_value=base_value, contributions=contributions)\n\n\ndef _kernel_weights(distances: np.ndarray, kernel_width: float) -&gt; np.ndarray:\n    weights = np.exp(-(distances**2) / (kernel_width**2))\n    return weights / (weights.sum() + 1e-12)\n\n\ndef lime_explanation(\n    model: LinearModel,\n    instance: Sequence[float],\n    num_samples: int = 200,\n    kernel_width: float = 0.75,\n    random_state: int = 62,\n) -&gt; LimeExplanation:\n    \"\"\"Fit a locally weighted surrogate model around an instance.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    instance_arr = np.asarray(instance, dtype=float)\n    noise = rng.normal(\n        scale=[20.0, 5_000.0, 0.2], size=(num_samples, instance_arr.size)\n    )\n    samples = instance_arr + noise\n    predictions = np.apply_along_axis(model.predict, 1, samples)\n    distances = np.linalg.norm(samples - instance_arr, axis=1)\n    weights = _kernel_weights(distances, kernel_width)\n    X_design = np.column_stack([np.ones(num_samples), samples])\n    W = np.diag(weights)\n    beta = np.linalg.pinv(X_design.T @ W @ X_design) @ (X_design.T @ W @ predictions)\n    coefs = np.asarray(beta[1:], dtype=float)\n    prediction = model.predict(instance_arr)\n    intercept = float(prediction - np.dot(coefs, instance_arr))\n    return LimeExplanation(intercept=intercept, weights=coefs, prediction=prediction)\n\n\ndef generate_counterfactual(\n    model: LinearModel,\n    instance: Sequence[float],\n    target: float,\n    bounds: Mapping[str, Tuple[float, float]],\n) -&gt; CounterfactualResult:\n    \"\"\"Compute a counterfactual by moving along the coefficient direction.\"\"\"\n\n    features = np.asarray(instance, dtype=float)\n    original_pred = model.predict(features)\n    direction = model.coefficients\n    scale = (target - original_pred) / (np.dot(direction, direction) + 1e-12)\n    raw_cf = features + scale * direction\n    ordered_bounds = np.array(\n        [bounds[name] for name in model.feature_names], dtype=float\n    )\n    clipped_cf = np.clip(raw_cf, ordered_bounds[:, 0], ordered_bounds[:, 1])\n    cf_prediction = model.predict(clipped_cf)\n    return CounterfactualResult(\n        original_prediction=original_pred,\n        target=target,\n        counterfactual_features=clipped_cf,\n        counterfactual_prediction=cf_prediction,\n        delta=clipped_cf - features,\n    )\n\n\ndef fairness_metrics(dataset: pd.DataFrame) -&gt; FairnessReport:\n    \"\"\"Compute key bias metrics for the approval outcome.\"\"\"\n\n    grouped = dataset.groupby(\"gender\")\n    approval_rate = grouped[\"approved\"].mean()\n    female_rate = float(approval_rate.get(\"F\", np.nan))\n    male_rate = float(approval_rate.get(\"M\", np.nan))\n    statistical_parity = female_rate - male_rate\n    disparate_impact = female_rate / male_rate if male_rate &gt; 0 else np.nan\n\n    # Equal opportunity: P(approval=1 | default risk below threshold)\n    low_risk = dataset[dataset[\"default_risk\"] &lt; dataset[\"default_risk\"].median()]\n    eq_grouped = low_risk.groupby(\"gender\")[\"approved\"].mean()\n    female_eq = float(eq_grouped.get(\"F\", np.nan))\n    male_eq = float(eq_grouped.get(\"M\", np.nan))\n    equal_opportunity = female_eq - male_eq\n\n    return FairnessReport(\n        statistical_parity=statistical_parity,\n        disparate_impact=disparate_impact,\n        equal_opportunity=equal_opportunity,\n    )\n\n\ndef apply_reweighing(dataset: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Return a dataframe with sample weights that mitigate statistical parity gaps.\"\"\"\n\n    df = dataset.copy()\n    grouped = df.groupby(\"gender\")\n    approval_rate = grouped[\"approved\"].mean()\n    target_rate = float(df[\"approved\"].mean())\n    weights = []\n    for _, row in df.iterrows():\n        group_rate = float(approval_rate[row[\"gender\"]])\n        weights.append(target_rate / (group_rate + 1e-12))\n    df[\"sample_weight\"] = weights\n    return df\n\n\ndef mitigation_effect(dataset: pd.DataFrame) -&gt; FairnessReport:\n    \"\"\"Recompute fairness metrics after reweighing.\"\"\"\n\n    reweighted = apply_reweighing(dataset)\n    weighted = {\n        gender: float(np.average(grp[\"approved\"], weights=grp[\"sample_weight\"]))\n        for gender, grp in reweighted.groupby(\"gender\")\n    }\n    female_rate = float(weighted.get(\"F\", np.nan))\n    male_rate = float(weighted.get(\"M\", np.nan))\n    statistical_parity = female_rate - male_rate\n    disparate_impact = female_rate / male_rate if male_rate &gt; 0 else np.nan\n\n    low_risk = reweighted[\n        reweighted[\"default_risk\"] &lt; reweighted[\"default_risk\"].median()\n    ]\n    eq_weighted = {\n        gender: float(np.average(grp[\"approved\"], weights=grp[\"sample_weight\"]))\n        for gender, grp in low_risk.groupby(\"gender\")\n    }\n    female_eq = float(eq_weighted.get(\"F\", np.nan))\n    male_eq = float(eq_weighted.get(\"M\", np.nan))\n    equal_opportunity = female_eq - male_eq\n\n    return FairnessReport(\n        statistical_parity=statistical_parity,\n        disparate_impact=disparate_impact,\n        equal_opportunity=equal_opportunity,\n    )\n\n\ndef run_interpretability_suite() -&gt; Dict[str, object]:\n    \"\"\"Execute interpretability and fairness utilities for documentation demos.\"\"\"\n\n    model = train_default_risk_model()\n    dataset = load_credit_dataset()\n    instance = dataset.loc[0, [\"credit_score\", \"income\", \"gender\"]]\n    encoded_instance = np.array(\n        [instance[\"credit_score\"], instance[\"income\"], float(instance[\"gender\"] == \"F\")]\n    )\n    shap = compute_shap_values(model, encoded_instance, dataset)\n    lime = lime_explanation(model, encoded_instance)\n    bounds = {\n        \"credit_score\": (500, 850),\n        \"income\": (30_000, 120_000),\n        \"is_female\": (0.0, 1.0),\n    }\n    counterfactual = generate_counterfactual(\n        model, encoded_instance, target=0.05, bounds=bounds\n    )\n    fairness = fairness_metrics(dataset)\n    mitigated = mitigation_effect(dataset)\n    return {\n        \"model\": model,\n        \"shap\": shap,\n        \"lime\": lime,\n        \"counterfactual\": counterfactual,\n        \"fairness\": fairness,\n        \"mitigated\": mitigated,\n    }\n\n\nif __name__ == \"__main__\":\n    report = run_interpretability_suite()\n    print(\"Base prediction:\", report[\"shap\"].reconstructed_prediction())\n    print(\n        \"LIME local prediction:\",\n        report[\"lime\"].local_prediction(\n            report[\"counterfactual\"].counterfactual_features\n        ),\n    )\n    print(\"Counterfactual delta:\", report[\"counterfactual\"].delta)\n    print(\"Fairness metrics:\", report[\"fairness\"])\n    print(\"After mitigation:\", report[\"mitigated\"])\n</code></pre>"},{"location":"lessons/day-63-causal-inference-and-uplift/","title":"Day 63 \u2013 Causal Inference and Uplift Modeling","text":"<p>Understand how experimentation and counterfactual reasoning quantify impact. After this lesson you will:</p> <ul> <li>Estimate average treatment effects (ATE) from randomized A/B tests.</li> <li>Learn propensity score workflows for observational studies.</li> <li>Implement double machine learning with cross-fitted residualization.</li> <li>Build two-model uplift estimators to target incremental responders.</li> </ul> <p>Run <code>python Day_63_Causal_Inference_and_Uplift/solutions.py</code> to generate synthetic treatment data, estimate effects with multiple techniques, and visualise uplift segmentations.</p>"},{"location":"lessons/day-63-causal-inference-and-uplift/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Causal inference utilities for Day 63.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\n\n\n@dataclass\nclass ABTestResult:\n    \"\"\"Summary statistics for a difference-in-means test.\"\"\"\n\n    lift: float\n    stderr: float\n    ci_low: float\n    ci_high: float\n\n\n@dataclass\nclass PropensityModel:\n    \"\"\"Logistic regression coefficients for propensity scores.\"\"\"\n\n    weights: np.ndarray\n    feature_mean: np.ndarray\n    feature_scale: np.ndarray\n\n    def _scale(self, features: np.ndarray) -&gt; np.ndarray:\n        scaled = features.copy()\n        scaled[:, 1:] = (scaled[:, 1:] - self.feature_mean) / (\n            self.feature_scale + 1e-12\n        )\n        return scaled\n\n    def predict_proba(self, features: np.ndarray) -&gt; np.ndarray:\n        logits = self._scale(features) @ self.weights\n        return 1.0 / (1.0 + np.exp(-logits))\n\n\n@dataclass\nclass DoubleMLResult:\n    \"\"\"Double machine learning effect estimate.\"\"\"\n\n    ate: float\n    nuisance_r2: Tuple[float, float]\n\n\n@dataclass\nclass UpliftResult:\n    \"\"\"Two-model uplift estimates for targeting.\"\"\"\n\n    treatment_response: float\n    control_response: float\n    uplift: float\n\n\ndef generate_synthetic_treatment_data(\n    n: int = 600, random_state: int = 63\n) -&gt; pd.DataFrame:\n    \"\"\"Create observational data with known treatment effect.\"\"\"\n\n    rng = np.random.default_rng(random_state)\n    age = rng.integers(18, 65, size=n)\n    browsing_time = rng.normal(4.0, 1.0, size=n)\n    income = rng.normal(55_000, 8_000, size=n)\n    baseline = 0.1 + 0.002 * (age - 30) + 0.00001 * (income - 50_000)\n    propensity_logits = -0.5 + 0.04 * (browsing_time - 4) + 0.00003 * (income - 55_000)\n    propensity = 1.0 / (1.0 + np.exp(-propensity_logits))\n    treatment = rng.binomial(1, propensity)\n    true_effect = 0.15\n    outcome = np.clip(\n        baseline + true_effect * treatment + 0.005 * (browsing_time - 4), 0, 1\n    )\n    return pd.DataFrame(\n        {\n            \"age\": age,\n            \"browsing_time\": browsing_time,\n            \"income\": income,\n            \"treatment\": treatment,\n            \"outcome\": outcome,\n            \"true_propensity\": propensity,\n            \"true_effect\": np.repeat(true_effect, n),\n        }\n    )\n\n\ndef difference_in_means(data: pd.DataFrame) -&gt; ABTestResult:\n    \"\"\"Compute lift and confidence interval for a randomized test.\"\"\"\n\n    treated = data[data[\"treatment\"] == 1][\"outcome\"].to_numpy(dtype=float)\n    control = data[data[\"treatment\"] == 0][\"outcome\"].to_numpy(dtype=float)\n    lift = treated.mean() - control.mean()\n    stderr = np.sqrt(\n        treated.var(ddof=1) / treated.size + control.var(ddof=1) / control.size\n    )\n    ci_low = lift - 1.96 * stderr\n    ci_high = lift + 1.96 * stderr\n    return ABTestResult(\n        lift=float(lift),\n        stderr=float(stderr),\n        ci_low=float(ci_low),\n        ci_high=float(ci_high),\n    )\n\n\ndef _prepare_design_matrix(\n    data: pd.DataFrame, include_intercept: bool = True\n) -&gt; np.ndarray:\n    features = data[[\"age\", \"browsing_time\", \"income\"]].to_numpy(dtype=float)\n    if include_intercept:\n        return np.column_stack([np.ones(len(data)), features])\n    return features\n\n\ndef fit_propensity_model(\n    data: pd.DataFrame, lr: float = 0.05, epochs: int = 800\n) -&gt; PropensityModel:\n    \"\"\"Fit logistic regression via gradient descent for propensity scores.\"\"\"\n\n    X = _prepare_design_matrix(data)\n    y = data[\"treatment\"].to_numpy(dtype=float)\n    feature_mean = X[:, 1:].mean(axis=0)\n    feature_scale = X[:, 1:].std(axis=0) + 1e-6\n    X_scaled = X.copy()\n    X_scaled[:, 1:] = (X_scaled[:, 1:] - feature_mean) / feature_scale\n    weights = np.zeros(X.shape[1], dtype=float)\n    for _ in range(epochs):\n        logits = X_scaled @ weights\n        preds = 1.0 / (1.0 + np.exp(-logits))\n        gradient = X_scaled.T @ (preds - y) / len(y)\n        weights -= lr * gradient\n    return PropensityModel(\n        weights=weights, feature_mean=feature_mean, feature_scale=feature_scale\n    )\n\n\ndef estimate_ipw_ate(data: pd.DataFrame, propensity_model: PropensityModel) -&gt; float:\n    \"\"\"Inverse propensity weighting estimate of the treatment effect.\"\"\"\n\n    X = _prepare_design_matrix(data)\n    propensities = propensity_model.predict_proba(X)\n    treated = data[\"treatment\"].to_numpy(dtype=float)\n    outcome = data[\"outcome\"].to_numpy(dtype=float)\n    weights_treated = treated / (propensities + 1e-12)\n    weights_control = (1 - treated) / (1 - propensities + 1e-12)\n    ate = (weights_treated @ outcome) / weights_treated.sum() - (\n        weights_control @ outcome\n    ) / weights_control.sum()\n    return float(ate)\n\n\ndef _linear_regression(X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    beta, *_ = np.linalg.lstsq(X, y, rcond=None)\n    return beta\n\n\ndef double_machine_learning(data: pd.DataFrame) -&gt; DoubleMLResult:\n    \"\"\"Compute double ML ATE with two-fold cross fitting.\"\"\"\n\n    n = len(data)\n    fold = n // 2\n    indices = np.arange(n)\n    X = data[[\"age\", \"browsing_time\", \"income\"]].to_numpy(dtype=float)\n    T = data[\"treatment\"].to_numpy(dtype=float)\n    Y = data[\"outcome\"].to_numpy(dtype=float)\n    residuals_y = np.zeros_like(Y)\n    residuals_t = np.zeros_like(T)\n    r2_y: List[float] = []  # type: ignore[name-defined]\n    r2_t: List[float] = []  # type: ignore[name-defined]\n    for train_idx, test_idx in (\n        (indices[:fold], indices[fold:]),\n        (indices[fold:], indices[:fold]),\n    ):\n        X_train = X[train_idx]\n        X_test = X[test_idx]\n        Y_train = Y[train_idx]\n        T_train = T[train_idx]\n        X_design_train = np.column_stack([np.ones(len(train_idx)), X_train])\n        beta_y = _linear_regression(X_design_train, Y_train)\n        beta_t = _linear_regression(X_design_train, T_train)\n        X_design_test = np.column_stack([np.ones(len(test_idx)), X_test])\n        y_hat = X_design_test @ beta_y\n        t_hat = X_design_test @ beta_t\n        residuals_y[test_idx] = Y[test_idx] - y_hat\n        residuals_t[test_idx] = T[test_idx] - t_hat\n        ss_tot_y = np.sum((Y[train_idx] - Y[train_idx].mean()) ** 2)\n        ss_res_y = np.sum((Y_train - X_design_train @ beta_y) ** 2)\n        ss_tot_t = np.sum((T_train - T_train.mean()) ** 2)\n        ss_res_t = np.sum((T_train - X_design_train @ beta_t) ** 2)\n        r2_y.append(1 - ss_res_y / (ss_tot_y + 1e-12))\n        r2_t.append(1 - ss_res_t / (ss_tot_t + 1e-12))\n    ate = float(\n        np.dot(residuals_t, residuals_y) / (np.dot(residuals_t, residuals_t) + 1e-12)\n    )\n    return DoubleMLResult(\n        ate=ate, nuisance_r2=(float(np.mean(r2_y)), float(np.mean(r2_t)))\n    )\n\n\ndef two_model_uplift(data: pd.DataFrame) -&gt; UpliftResult:\n    \"\"\"Estimate uplift using separate treatment and control response models.\"\"\"\n\n    treated = data[data[\"treatment\"] == 1]\n    control = data[data[\"treatment\"] == 0]\n    features_treated = np.column_stack(\n        [\n            np.ones(len(treated)),\n            treated[[\"age\", \"browsing_time\", \"income\"]].to_numpy(dtype=float),\n        ]\n    )\n    features_control = np.column_stack(\n        [\n            np.ones(len(control)),\n            control[[\"age\", \"browsing_time\", \"income\"]].to_numpy(dtype=float),\n        ]\n    )\n    beta_treated = _linear_regression(\n        features_treated, treated[\"outcome\"].to_numpy(dtype=float)\n    )\n    beta_control = _linear_regression(\n        features_control, control[\"outcome\"].to_numpy(dtype=float)\n    )\n    cohort = data[[\"age\", \"browsing_time\", \"income\"]].to_numpy(dtype=float)\n    cohort_design = np.column_stack([np.ones(len(cohort)), cohort])\n    treatment_pred = float(np.mean(cohort_design @ beta_treated))\n    control_pred = float(np.mean(cohort_design @ beta_control))\n    uplift = treatment_pred - control_pred\n    return UpliftResult(\n        treatment_response=treatment_pred,\n        control_response=control_pred,\n        uplift=uplift,\n    )\n\n\ndef run_causal_suite(random_state: int = 63) -&gt; Dict[str, object]:\n    \"\"\"Execute all causal estimators for documentation demos.\"\"\"\n\n    data = generate_synthetic_treatment_data(random_state=random_state)\n    ab_result = difference_in_means(data)\n    propensity_model = fit_propensity_model(data)\n    ipw_ate = estimate_ipw_ate(data, propensity_model)\n    dml_result = double_machine_learning(data)\n    uplift_result = two_model_uplift(data)\n    return {\n        \"data\": data,\n        \"ab_test\": ab_result,\n        \"ipw_ate\": ipw_ate,\n        \"double_ml\": dml_result,\n        \"uplift\": uplift_result,\n    }\n\n\nif __name__ == \"__main__\":\n    results = run_causal_suite()\n    print(\"A/B lift:\", results[\"ab_test\"])\n    print(\"IPW ATE:\", results[\"ipw_ate\"])\n    print(\"Double ML ATE:\", results[\"double_ml\"].ate)\n    print(\"Uplift estimate:\", results[\"uplift\"].uplift)\n</code></pre>"},{"location":"lessons/day-64-modern-nlp-pipelines/","title":"Day 64 \u2013 Modern NLP Pipelines","text":"<p>Connect discrete NLP components into a reproducible workflow. After this lesson you will:</p> <ul> <li>Tokenize text with configurable normalization options.</li> <li>Build deterministic embedding tables for rapid experimentation.</li> <li>Fine-tune a lightweight transformer-style classifier head on sentence labels.</li> <li>Retrieve support passages and perform retrieval-augmented generation (RAG).</li> <li>Evaluate generations with deterministic exact-match and token-overlap metrics.</li> </ul> <p>Run <code>python Day_64_Modern_NLP_Pipelines/solutions.py</code> to explore end-to-end text processing with seeded toy corpora.</p>"},{"location":"lessons/day-64-modern-nlp-pipelines/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Modern NLP pipeline utilities for Day 64.\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Mapping, Sequence, Tuple\n\nimport numpy as np\n\nTokenizedCorpus = List[List[str]]\n\n\ndef tokenize_corpus(\n    corpus: Sequence[str],\n    *,\n    lowercase: bool = True,\n    strip_punctuation: bool = True,\n) -&gt; TokenizedCorpus:\n    \"\"\"Tokenize strings with deterministic options.\"\"\"\n\n    table = str.maketrans({c: \" \" for c in \"!,.?;:\"}) if strip_punctuation else None\n    tokenized: TokenizedCorpus = []\n    for doc in corpus:\n        text = doc.translate(table) if table is not None else doc\n        if lowercase:\n            text = text.lower()\n        tokens = [token for token in text.split() if token]\n        tokenized.append(tokens)\n    return tokenized\n\n\ndef build_embedding_table(\n    tokens: TokenizedCorpus, embedding_dim: int = 8\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Create deterministic embeddings via hashing.\"\"\"\n\n    vocab = sorted({token for doc in tokens for token in doc})\n    table: Dict[str, np.ndarray] = {}\n    for token in vocab:\n        digest = hashlib.sha256(token.encode(\"utf-8\")).digest()\n        seed = int.from_bytes(digest[:4], \"little\")\n        rng = np.random.default_rng(seed)\n        table[token] = rng.normal(0, 1, size=embedding_dim)\n    return table\n\n\ndef document_embeddings(\n    tokens: TokenizedCorpus, embeddings: Mapping[str, np.ndarray]\n) -&gt; np.ndarray:\n    \"\"\"Average token embeddings for each document.\"\"\"\n\n    doc_vectors: List[np.ndarray] = []\n    for doc in tokens:\n        if doc:\n            vecs = [embeddings[token] for token in doc]\n            doc_vectors.append(np.mean(vecs, axis=0))\n        else:\n            doc_vectors.append(\n                np.zeros(next(iter(embeddings.values())).shape, dtype=float)\n            )\n    return np.vstack(doc_vectors)\n\n\n@dataclass\nclass MiniTransformer:\n    \"\"\"Lightweight classifier head operating on document embeddings.\"\"\"\n\n    weights: np.ndarray\n    bias: float\n\n    def predict_proba(self, embeddings: np.ndarray) -&gt; np.ndarray:\n        logits = embeddings @ self.weights + self.bias\n        return 1.0 / (1.0 + np.exp(-logits))\n\n    def predict(self, embeddings: np.ndarray) -&gt; np.ndarray:\n        return (self.predict_proba(embeddings) &gt;= 0.5).astype(int)\n\n\n@dataclass\nclass FineTuneHistory:\n    \"\"\"Record of loss values during fine-tuning.\"\"\"\n\n    losses: List[float]\n\n\ndef fine_tune_transformer(\n    embeddings: np.ndarray,\n    labels: Sequence[int],\n    epochs: int = 200,\n    lr: float = 0.1,\n) -&gt; Tuple[MiniTransformer, FineTuneHistory]:\n    \"\"\"Train a logistic head on top of frozen document embeddings.\"\"\"\n\n    y = np.asarray(labels, dtype=float)\n    weights = np.zeros(embeddings.shape[1], dtype=float)\n    bias = 0.0\n    losses: List[float] = []\n    for _ in range(epochs):\n        logits = embeddings @ weights + bias\n        probs = 1.0 / (1.0 + np.exp(-logits))\n        loss = -np.mean(y * np.log(probs + 1e-12) + (1 - y) * np.log(1 - probs + 1e-12))\n        losses.append(float(loss))\n        gradient_w = embeddings.T @ (probs - y) / len(y)\n        gradient_b = np.mean(probs - y)\n        weights -= lr * gradient_w\n        bias -= lr * gradient_b\n    model = MiniTransformer(weights=weights, bias=bias)\n    return model, FineTuneHistory(losses=losses)\n\n\ndef retrieve_documents(\n    query_embedding: np.ndarray,\n    doc_embeddings: np.ndarray,\n    top_k: int = 1,\n) -&gt; List[int]:\n    \"\"\"Return indices of nearest documents by cosine similarity.\"\"\"\n\n    similarities = (\n        doc_embeddings\n        @ query_embedding\n        / (\n            np.linalg.norm(doc_embeddings, axis=1)\n            * (np.linalg.norm(query_embedding) + 1e-12)\n        )\n    )\n    ranked = np.argsort(similarities)[::-1]\n    return ranked[:top_k].tolist()\n\n\ndef rag_generate(\n    query: str,\n    corpus: Sequence[str],\n    doc_embeddings: np.ndarray,\n    embeddings_table: Mapping[str, np.ndarray],\n    top_k: int = 1,\n) -&gt; str:\n    \"\"\"Perform retrieval-augmented generation by echoing top documents.\"\"\"\n\n    tokenized_query = tokenize_corpus([query])[0]\n    if tokenized_query:\n        query_vec = np.mean(\n            [embeddings_table[token] for token in tokenized_query], axis=0\n        )\n    else:\n        query_vec = np.zeros(next(iter(embeddings_table.values())).shape, dtype=float)\n    doc_indices = retrieve_documents(query_vec, doc_embeddings, top_k=top_k)\n    retrieved = [corpus[idx] for idx in doc_indices]\n    return \" \\n\".join(\n        [f\"Answer: {retrieved[0] if retrieved else ''}\", \"Sources:\"] + retrieved\n    )\n\n\ndef evaluate_generation(reference: str, prediction: str) -&gt; Dict[str, float]:\n    \"\"\"Compute deterministic exact-match and token-overlap metrics.\"\"\"\n\n    ref_tokens = tokenize_corpus([reference])[0]\n    pred_tokens = tokenize_corpus([prediction])[0]\n    exact = float(reference.strip().lower() == prediction.strip().lower())\n    overlap = len(set(ref_tokens) &amp; set(pred_tokens)) / (len(set(ref_tokens)) + 1e-12)\n    recall = len(set(ref_tokens) &amp; set(pred_tokens)) / (len(set(ref_tokens)) + 1e-12)\n    precision = len(set(ref_tokens) &amp; set(pred_tokens)) / (\n        len(set(pred_tokens)) + 1e-12\n    )\n    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n    return {\n        \"exact_match\": exact,\n        \"overlap\": overlap,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }\n\n\ndef build_pipeline(corpus: Sequence[str], labels: Sequence[int]) -&gt; Dict[str, object]:\n    \"\"\"Utility for documentation walkthroughs.\"\"\"\n\n    tokens = tokenize_corpus(corpus)\n    embedding_table = build_embedding_table(tokens)\n    doc_vecs = document_embeddings(tokens, embedding_table)\n    model, history = fine_tune_transformer(doc_vecs, labels)\n    rag_answer = rag_generate(\"summary\", corpus, doc_vecs, embedding_table, top_k=2)\n    metrics = evaluate_generation(corpus[0], rag_answer)\n    return {\n        \"tokens\": tokens,\n        \"embeddings\": embedding_table,\n        \"doc_vectors\": doc_vecs,\n        \"model\": model,\n        \"history\": history,\n        \"rag_answer\": rag_answer,\n        \"metrics\": metrics,\n    }\n\n\nif __name__ == \"__main__\":\n    corpus = [\n        \"Transformers capture long-range dependencies with self-attention.\",\n        \"Retrieval augmented generation grounds answers in documents.\",\n        \"Tokenization and embeddings define the vocabulary space.\",\n    ]\n    labels = [1, 1, 0]\n    pipeline = build_pipeline(corpus, labels)\n    print(\"Loss trajectory (first 5):\", pipeline[\"history\"].losses[:5])\n    print(\"RAG output:\\n\", pipeline[\"rag_answer\"])\n    print(\"Metrics:\", pipeline[\"metrics\"])\n</code></pre>"},{"location":"lessons/day-65-mlops-pipelines-and-ci/","title":"Day 65 \u2013 MLOps Pipelines and CI/CD Automation","text":"<p>Day 50 introduced model persistence. Day 65 expands that foundation into production-grade automation that glues together feature engineering, training, registration, and deployment inside a repeatable delivery pipeline.</p>"},{"location":"lessons/day-65-mlops-pipelines-and-ci/#learning-goals","title":"Learning goals","text":"<ul> <li>Feature stores \u2013 Design entities, feature views, and point-in-time   joins that keep online/offline data consistent across training and   inference.</li> <li>Model registries \u2013 Promote trained artefacts through staging,   production, and archival stages with metadata-rich lineage tracking.</li> <li>Workflow orchestration \u2013 Compare how Apache Airflow DAGs and   Prefect flows coordinate complex ML tasks with retries, schedules, and   parameterised runs.</li> <li>Continuous integration and delivery \u2013 Implement GitHub Actions   workflows that lint, test, train, and roll out models with automated   safety gates and human approvals when necessary.</li> </ul>"},{"location":"lessons/day-65-mlops-pipelines-and-ci/#hands-on-practice","title":"Hands-on practice","text":"<p><code>solutions.py</code> ships a lightweight pipeline simulator that mirrors a feature store refresh, model training job, model registry promotion, and GitHub Actions deployment stage. The tasks are wired together with a miniature DAG executor inspired by Airflow/PyPrefect semantics so you can experiment with dependency resolution locally.</p> <p>Run the module to see the orchestration trace:</p> <pre><code>python Day_65_MLOps_Pipelines_and_CI/solutions.py\n</code></pre> <p>The included tests (<code>tests/test_day_65.py</code>) stub raw feature inputs and assert that the DAG executes in topological order, promoting a versioned model artefact only after automated evaluation passes.</p>"},{"location":"lessons/day-65-mlops-pipelines-and-ci/#extend-the-exercise","title":"Extend the exercise","text":"<ul> <li>Swap the in-memory feature store with Feast or Tecton to practice   managing online/offline materialisation.</li> <li>Replace the registry stub with MLflow\u2019s model registry to integrate   experiment tracking and stage transitions.</li> <li>Export the DAG to YAML/JSON and feed it into Airflow or Prefect for a   production-ready orchestration pattern.</li> <li>Fork the GitHub Actions example into your repository to add matrix   testing (Python versions, CPU vs GPU runners) and continuous delivery   to Kubernetes, SageMaker, or Vertex AI.</li> </ul>"},{"location":"lessons/day-65-mlops-pipelines-and-ci/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Utility helpers for orchestrating an end-to-end MLOps pipeline.\n\nThe module intentionally mirrors the stages that appear in a production\nGitHub Actions workflow: refresh a feature store, train and evaluate a\nmodel, register the resulting artefact, and perform a deployment gate.\n\nInstead of depending on heavy external services, the code uses\nlightweight, deterministic stubs so unit tests can simulate an Apache\nAirflow or Prefect DAG locally. Each task receives a consolidated\ncontext dictionary (similar to Airflow's XCom or Prefect's task result)\nand may add new keys for downstream tasks.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom datetime import UTC, datetime\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n)\n\nFeatureRow = Mapping[str, Any]\n\n\n@dataclass\nclass Task:\n    \"\"\"Represents a node in an orchestration graph.\n\n    Attributes\n    ----------\n    name:\n        Unique identifier for the task. Names are used to resolve\n        dependencies and to expose results in the execution context.\n    run:\n        Callable that receives the merged execution context and returns a\n        value that is stored under ``name`` for downstream tasks.\n    upstream:\n        Optional list of task names that must finish before this task\n        executes. The dependency semantics align with Airflow DAGs and\n        Prefect flows.\n    \"\"\"\n\n    name: str\n    run: Callable[[MutableMapping[str, Any]], Any]\n    upstream: List[str] = field(default_factory=list)\n\n\nclass PipelineDAG:\n    \"\"\"A minimal directed acyclic graph executor for ML pipelines.\"\"\"\n\n    def __init__(self, tasks: Iterable[Task]):\n        self._tasks: Dict[str, Task] = {}\n        for task in tasks:\n            if task.name in self._tasks:\n                raise ValueError(f\"Duplicate task name detected: {task.name}\")\n            self._tasks[task.name] = task\n        for task in self._tasks.values():\n            for dependency in task.upstream:\n                if dependency not in self._tasks:\n                    raise ValueError(\n                        f\"Task '{task.name}' references unknown dependency '{dependency}'\"\n                    )\n\n    @property\n    def tasks(self) -&gt; Dict[str, Task]:\n        return self._tasks\n\n    def topological_order(self) -&gt; List[str]:\n        \"\"\"Return a deterministic topological ordering of the tasks.\"\"\"\n\n        temporary_marks: set[str] = set()\n        permanent_marks: set[str] = set()\n        ordered: List[str] = []\n\n        def visit(node_name: str) -&gt; None:\n            if node_name in permanent_marks:\n                return\n            if node_name in temporary_marks:\n                raise ValueError(\"Cycle detected in DAG definition\")\n            temporary_marks.add(node_name)\n            node = self._tasks[node_name]\n            for dependency in node.upstream:\n                visit(dependency)\n            permanent_marks.add(node_name)\n            temporary_marks.remove(node_name)\n            ordered.append(node_name)\n\n        for name in sorted(self._tasks):\n            if name not in permanent_marks:\n                visit(name)\n        return ordered\n\n    def execute(\n        self, base_context: Optional[MutableMapping[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute tasks respecting dependencies.\n\n        Parameters\n        ----------\n        base_context:\n            Optional dictionary containing static inputs (for example raw\n            features or configuration). Tasks may mutate this dictionary,\n            mimicking orchestration platforms that provide shared context\n            objects.\n        \"\"\"\n\n        context: MutableMapping[str, Any]\n        if base_context is None:\n            context = {}\n        else:\n            context = base_context\n        ordered = self.topological_order()\n        for name in ordered:\n            task = self._tasks[name]\n            context[name] = task.run(context)\n        context[\"execution_order\"] = ordered\n        return context\n\n\ndef upsert_feature_store(rows: Iterable[FeatureRow]) -&gt; Dict[str, FeatureRow]:\n    \"\"\"Materialise feature rows into an in-memory feature store.\n\n    The function keeps the most recent row for each primary key and\n    stamps the ingestion time. Production feature stores (Feast, Tecton,\n    Vertex AI Feature Store) provide similar semantics.\n    \"\"\"\n\n    feature_store: Dict[str, FeatureRow] = {}\n    for row in rows:\n        entity_id = str(row.get(\"entity_id\"))\n        feature_store[entity_id] = {\n            **row,\n            \"ingested_at\": datetime.now(UTC).isoformat(timespec=\"seconds\"),\n        }\n    return feature_store\n\n\ndef train_model_from_store(store: Mapping[str, FeatureRow]) -&gt; Dict[str, Any]:\n    \"\"\"Train and evaluate a trivial model using feature store contents.\"\"\"\n\n    feature_values = [row.get(\"feature_value\", 0.0) for row in store.values()]\n    if not feature_values:\n        raise ValueError(\"Feature store is empty; cannot train model\")\n    avg_feature = sum(feature_values) / len(feature_values)\n    # The \"model\" is encoded as a slope anchored by the mean feature value.\n    model_artifact = {\n        \"parameters\": {\"slope\": avg_feature / (1 + abs(avg_feature))},\n        \"metrics\": {\"validation_accuracy\": 0.8 + (avg_feature % 0.2)},\n    }\n    return model_artifact\n\n\ndef register_model(\n    model: Mapping[str, Any], *, name: str, stage: str\n) -&gt; Dict[str, Any]:\n    \"\"\"Record model metadata as if interacting with an MLflow-style registry.\"\"\"\n\n    if \"metrics\" not in model:\n        raise KeyError(\"Model metadata must include 'metrics'\")\n    version = datetime.now(UTC).strftime(\"%Y%m%d%H%M%S\")\n    registry_entry = {\n        \"name\": name,\n        \"version\": version,\n        \"stage\": stage,\n        \"metrics\": model[\"metrics\"],\n    }\n    return registry_entry\n\n\ndef github_actions_deploy(entry: Mapping[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Simulate a GitHub Actions job that deploys a registered model.\"\"\"\n\n    if entry.get(\"stage\") != \"Staging\":\n        return {\n            \"status\": \"skipped\",\n            \"reason\": \"Only staging models deploy automatically\",\n        }\n    if entry.get(\"metrics\", {}).get(\"validation_accuracy\", 0.0) &lt; 0.85:\n        return {\n            \"status\": \"failed\",\n            \"reason\": \"Quality gate failed\",\n        }\n    return {\n        \"status\": \"success\",\n        \"environment\": \"production\",\n        \"commit_sha\": \"demo-sha\",\n    }\n\n\ndef build_mlops_pipeline(raw_rows: Iterable[FeatureRow]) -&gt; PipelineDAG:\n    \"\"\"Construct the pipeline DAG with deterministic task wiring.\"\"\"\n\n    # Persist raw rows in the base context so the feature-store task can\n    # consume them. The orchestrator will attach results by task name.\n    base_context = {\"raw_rows\": list(raw_rows)}\n\n    def feature_task(context: MutableMapping[str, Any]) -&gt; Dict[str, FeatureRow]:\n        return upsert_feature_store(context[\"raw_rows\"])\n\n    def training_task(context: MutableMapping[str, Any]) -&gt; Dict[str, Any]:\n        return train_model_from_store(context[\"feature_store\"])\n\n    def registry_task(context: MutableMapping[str, Any]) -&gt; Dict[str, Any]:\n        return register_model(\n            context[\"model_training\"], name=\"churn_model\", stage=\"Staging\"\n        )\n\n    def deployment_task(context: MutableMapping[str, Any]) -&gt; Dict[str, Any]:\n        return github_actions_deploy(context[\"model_registry\"])\n\n    tasks = [\n        Task(name=\"feature_store\", run=feature_task),\n        Task(name=\"model_training\", run=training_task, upstream=[\"feature_store\"]),\n        Task(name=\"model_registry\", run=registry_task, upstream=[\"model_training\"]),\n        Task(name=\"deployment\", run=deployment_task, upstream=[\"model_registry\"]),\n    ]\n\n    dag = PipelineDAG(tasks)\n    # Attach the base context so callers can re-use it between runs.\n    dag.base_context = base_context  # type: ignore[attr-defined]\n    return dag\n\n\ndef run_pipeline(raw_rows: Iterable[FeatureRow]) -&gt; Dict[str, Any]:\n    \"\"\"Helper for scripts/tests: build the DAG and execute it.\"\"\"\n\n    dag = build_mlops_pipeline(raw_rows)\n    context = getattr(dag, \"base_context\", {})\n    return dag.execute(context)\n\n\nif __name__ == \"__main__\":\n    rows = [\n        {\"entity_id\": 1, \"feature_value\": 0.42},\n        {\"entity_id\": 2, \"feature_value\": 0.58},\n    ]\n    results = run_pipeline(rows)\n    print(\"Execution order:\", results[\"execution_order\"])  # noqa: T201\n    print(\"Deployment status:\", results[\"deployment\"])  # noqa: T201\n</code></pre>"},{"location":"lessons/day-66-model-deployment-and-serving/","title":"Day 66 \u2013 Model Deployment and Serving Patterns","text":"<p>Production machine learning systems expose predictions through a variety of runtime patterns. Building on the MLOps pipeline from Day 65, this lesson compares the trade-offs between synchronous APIs, high-throughput RPC services, scheduled batch scoring, streaming inference, and resource-constrained edge deployments.</p>"},{"location":"lessons/day-66-model-deployment-and-serving/#learning-goals","title":"Learning goals","text":"<ul> <li>REST serving with FastAPI \u2013 Design JSON contracts, response   schemas, and dependency-injected models that work with serverless   platforms, container orchestrators, or BentoML services.</li> <li>gRPC microservices \u2013 Use protobuf schemas and streaming RPCs for   low-latency online inference with strong typing and bi-directional   streaming.</li> <li>Batch and streaming predictions \u2013 Trigger nightly or hourly   backfills alongside event-driven inference to balance cost and   freshness requirements.</li> <li>Edge deployment \u2013 Package lightweight runtimes that run inside   mobile apps, browsers (WebAssembly), or IoT gateways with offline   caching and resilience to intermittent connectivity.</li> <li>Operational readiness \u2013 Instrument health endpoints, log   structured telemetry, and integrate load testing into CI to prevent   regressions.</li> </ul>"},{"location":"lessons/day-66-model-deployment-and-serving/#hands-on-practice","title":"Hands-on practice","text":"<p><code>solutions.py</code> provides protocol-specific adapters for a mock model and a synthetic load-testing harness. The helpers lean on FastAPI-style input validation semantics and BentoML-inspired service bundling while keeping runtime dependencies lightweight for local experimentation.</p> <p>Run the example service locally:</p> <pre><code>python Day_66_Model_Deployment_and_Serving/solutions.py\n</code></pre> <p>Then execute the tests (<code>tests/test_day_66.py</code>) to verify that the REST, gRPC, and batch adapters share a consistent response schema and survive a stress scenario with concurrent workers.</p>"},{"location":"lessons/day-66-model-deployment-and-serving/#extend-the-exercise","title":"Extend the exercise","text":"<ul> <li>Swap the stubbed adapters with real FastAPI routers and BentoML   services to deploy a containerised API.</li> <li>Generate protobuf definitions for the gRPC helper and implement a   client using <code>grpcio</code> or <code>grpclib</code>.</li> <li>Port the load test harness to <code>locust</code>, <code>k6</code>, or <code>vegeta</code> and capture   latency percentiles across different hardware profiles.</li> <li>Add schema evolution examples demonstrating backwards-compatible API   rollouts.</li> </ul>"},{"location":"lessons/day-66-model-deployment-and-serving/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Adapters that demonstrate multiple model serving patterns.\n\nThe code intentionally avoids heavyweight dependencies so it can run\ninside unit tests, yet the abstractions mirror FastAPI/BentoML service\ninterfaces, gRPC handlers, and streaming/batch processors.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom statistics import mean\nfrom time import perf_counter\nfrom typing import Any, Callable, Dict, Iterable, List, Mapping, Sequence\n\n\n@dataclass\nclass PredictionResponse:\n    \"\"\"Normalised response payload shared across transports.\"\"\"\n\n    predictions: List[float]\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return {\"predictions\": self.predictions, \"metadata\": self.metadata}\n\n\ndef _coerce_predictions(raw: Iterable[Any]) -&gt; List[float]:\n    values: List[float] = []\n    for item in raw:\n        try:\n            values.append(float(item))\n        except (TypeError, ValueError) as exc:  # pragma: no cover - defensive\n            raise ValueError(f\"Prediction values must be numeric: {item!r}\") from exc\n    return values\n\n\ndef fastapi_rest_adapter(\n    model: Callable[[Sequence[float]], Sequence[float]],\n) -&gt; Callable[[Mapping[str, Any]], Dict[str, Any]]:\n    \"\"\"Create a FastAPI-style callable that accepts JSON payloads.\"\"\"\n\n    def predict(payload: Mapping[str, Any]) -&gt; Dict[str, Any]:\n        instances = payload.get(\"instances\")\n        if instances is None:\n            raise KeyError(\"Payload missing 'instances'\")\n        predictions = model(instances)\n        response = PredictionResponse(\n            predictions=_coerce_predictions(predictions),\n            metadata={\"transport\": \"REST\", \"framework\": \"FastAPI\"},\n        )\n        return response.to_dict()\n\n    return predict\n\n\ndef grpc_streaming_adapter(\n    model: Callable[[Sequence[float]], Sequence[float]],\n) -&gt; Callable[[Iterable[Mapping[str, Any]]], Iterable[Dict[str, Any]]]:\n    \"\"\"Return a generator-like gRPC handler that yields streaming responses.\"\"\"\n\n    def handler(\n        request_iterator: Iterable[Mapping[str, Any]],\n    ) -&gt; Iterable[Dict[str, Any]]:\n        for payload in request_iterator:\n            instances = payload.get(\"instances\", [])\n            predictions = model(instances)\n            response = PredictionResponse(\n                predictions=_coerce_predictions(predictions),\n                metadata={\"transport\": \"gRPC\", \"streaming\": True},\n            )\n            yield response.to_dict()\n\n    return handler\n\n\ndef batch_scoring_runner(\n    model: Callable[[Sequence[float]], Sequence[float]],\n    batches: Iterable[Sequence[float]],\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Process offline batches while preserving response structure.\"\"\"\n\n    outputs: List[Dict[str, Any]] = []\n    for batch in batches:\n        predictions = model(batch)\n        response = PredictionResponse(\n            predictions=_coerce_predictions(predictions),\n            metadata={\"transport\": \"batch\", \"batch_size\": len(list(batch))},\n        )\n        outputs.append(response.to_dict())\n    return outputs\n\n\ndef edge_inference_adapter(\n    model: Callable[[Sequence[float]], Sequence[float]], *, quantise: bool = True\n) -&gt; Callable[[Sequence[float]], Dict[str, Any]]:\n    \"\"\"Wrap a model for offline/edge execution with optional quantisation.\"\"\"\n\n    def run(features: Sequence[float]) -&gt; Dict[str, Any]:\n        scaled = [round(float(x), 3) for x in features]\n        predictions = model(scaled)\n        response = PredictionResponse(\n            predictions=_coerce_predictions(predictions),\n            metadata={\"transport\": \"edge\", \"quantised\": quantise},\n        )\n        return response.to_dict()\n\n    return run\n\n\ndef ensure_response_schema(payload: Mapping[str, Any]) -&gt; None:\n    \"\"\"Validate that a payload follows the canonical schema.\"\"\"\n\n    if \"predictions\" not in payload:\n        raise AssertionError(\"Missing predictions field\")\n    if not isinstance(payload[\"predictions\"], list):\n        raise AssertionError(\"Predictions must be a list\")\n    for value in payload[\"predictions\"]:\n        if not isinstance(value, (int, float)):\n            raise AssertionError(\"Predictions must be numeric\")\n    metadata = payload.get(\"metadata\", {})\n    if not isinstance(metadata, dict):\n        raise AssertionError(\"Metadata must be a mapping\")\n\n\n@dataclass\nclass LoadTestResult:\n    avg_latency: float\n    throughput: float\n    success_rate: float\n\n\ndef run_synthetic_load_test(\n    endpoint: Callable[[Mapping[str, Any]], Mapping[str, Any]],\n    payloads: Sequence[Mapping[str, Any]],\n    *,\n    warmups: int = 1,\n) -&gt; LoadTestResult:\n    \"\"\"Execute a deterministic load-test loop against an endpoint.\"\"\"\n\n    total_latency = 0.0\n    successes = 0\n    # Warmup calls (not included in metrics but ensure caches are primed)\n    for i in range(warmups):\n        endpoint(payloads[i % len(payloads)])\n    for payload in payloads:\n        start = perf_counter()\n        response = endpoint(payload)\n        latency = perf_counter() - start\n        total_latency += latency\n        try:\n            ensure_response_schema(response)\n            successes += 1\n        except AssertionError:\n            pass\n    avg_latency = total_latency / len(payloads)\n    throughput = len(payloads) / max(total_latency, 1e-6)\n    success_rate = successes / len(payloads)\n    return LoadTestResult(\n        avg_latency=avg_latency, throughput=throughput, success_rate=success_rate\n    )\n\n\ndef averaged_ensembled_model(instances: Sequence[float]) -&gt; List[float]:\n    \"\"\"Reference model that mimics an ensemble averaged prediction.\"\"\"\n\n    if not instances:\n        return [0.0]\n    centre = mean(float(x) for x in instances)\n    return [round(centre * 0.8 + 0.1, 4)]\n\n\ndef describe_serving_landscape() -&gt; Dict[str, Any]:\n    \"\"\"Summarise the pros/cons of deployment patterns for quick reference.\"\"\"\n\n    return {\n        \"REST\": {\"latency\": \"medium\", \"strength\": \"ubiquitous clients\"},\n        \"gRPC\": {\"latency\": \"low\", \"strength\": \"typed contracts\"},\n        \"batch\": {\"latency\": \"high\", \"strength\": \"cost efficiency\"},\n        \"streaming\": {\"latency\": \"low\", \"strength\": \"event-driven\"},\n        \"edge\": {\"latency\": \"ultra-low\", \"strength\": \"offline ready\"},\n    }\n\n\nif __name__ == \"__main__\":\n    model = averaged_ensembled_model\n    endpoint = fastapi_rest_adapter(model)\n    sample_payloads = [{\"instances\": [0.1, 0.2, 0.4]} for _ in range(10)]\n    result = run_synthetic_load_test(endpoint, sample_payloads)\n    print(\"Synthetic load test\", result)  # noqa: T201\n</code></pre>"},{"location":"lessons/day-67-model-monitoring-and-reliability/","title":"Day 67 \u2013 Model Monitoring and Reliability Engineering","text":"<p>The final instalment of the MLOps arc closes the loop from deployment to operations. After mastering persistence (Day 50), automation (Day 65), and serving (Day 66), this lesson introduces the observability patterns that keep models trustworthy in production.</p>"},{"location":"lessons/day-67-model-monitoring-and-reliability/#learning-goals","title":"Learning goals","text":"<ul> <li>Data and concept drift detection \u2013 Track feature distributions with   population stability index (PSI), Kullback\u2013Leibler divergence, or   threshold-based heuristics that trigger alerts when inputs shift.</li> <li>Automated retraining triggers \u2013 Combine drift signals, performance   metrics, and business guardrails to decide when to schedule a new   training job.</li> <li>Progressive delivery \u2013 Roll out models with canary or shadow   deployments, automatically rolling back if latency or accuracy   regressions appear.</li> <li>Observability tooling \u2013 Instrument models with Prometheus metrics   exporters, OpenTelemetry traces, and structured logging for rapid   incident response.</li> </ul>"},{"location":"lessons/day-67-model-monitoring-and-reliability/#hands-on-practice","title":"Hands-on practice","text":"<p><code>solutions.py</code> provides synthetic drift generators, simple detection algorithms, and a canary analysis helper. These components emit metrics that could be scraped by Prometheus or pushed to OpenTelemetry collectors, illustrating how to connect monitoring to automated decision systems.</p> <p>Run the script to see drift alerts bubble up:</p> <pre><code>python Day_67_Model_Monitoring_and_Reliability/solutions.py\n</code></pre> <p><code>tests/test_day_67.py</code> feeds controlled distribution shifts through the helpers and confirms that alerts fire, retraining queues populate, and canary verdicts respect latency/accuracy thresholds.</p>"},{"location":"lessons/day-67-model-monitoring-and-reliability/#extend-the-exercise","title":"Extend the exercise","text":"<ul> <li>Replace the heuristic drift detector with <code>alibi-detect</code>, <code>evidently</code>,   or scikit-multiflow to monitor complex multivariate shifts.</li> <li>Export the observability payloads to Prometheus using <code>prometheus- client</code> counters, gauges, and histograms.</li> <li>Emit OpenTelemetry traces that attach prediction metadata and user   identifiers for distributed tracing across microservices.</li> <li>Integrate human-in-the-loop acknowledgement by forwarding alerts to an   incident management platform.</li> </ul>"},{"location":"lessons/day-67-model-monitoring-and-reliability/#additional-materials","title":"Additional Materials","text":"<ul> <li>solutions.ipynb</li> </ul> solutions.py <p>View on GitHub</p> solutions.py<pre><code>\"\"\"Monitoring utilities for production ML systems.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom statistics import mean, pstdev\nfrom typing import Any, Dict, Iterable, List, Mapping\n\n\n@dataclass\nclass DriftReport:\n    feature: str\n    baseline_mean: float\n    current_mean: float\n    drift_score: float\n    triggered: bool\n\n\ndef compute_mean_drift(\n    baseline: Iterable[float],\n    current: Iterable[float],\n    *,\n    threshold: float = 0.2,\n) -&gt; DriftReport:\n    \"\"\"Compare distributions using a simple relative mean difference.\"\"\"\n\n    baseline_list = list(baseline)\n    current_list = list(current)\n    if not baseline_list or not current_list:\n        raise ValueError(\"Both baseline and current samples must be provided\")\n    baseline_mean = mean(baseline_list)\n    current_mean = mean(current_list)\n    baseline_std = pstdev(baseline_list) or 1e-6\n    drift_score = abs(current_mean - baseline_mean) / baseline_std\n    triggered = drift_score &gt;= threshold\n    return DriftReport(\n        feature=\"feature_value\",\n        baseline_mean=round(baseline_mean, 4),\n        current_mean=round(current_mean, 4),\n        drift_score=round(drift_score, 4),\n        triggered=triggered,\n    )\n\n\ndef should_trigger_retraining(\n    report: DriftReport, *, accuracy: float, latency: float\n) -&gt; bool:\n    \"\"\"Decide whether to retrain given drift and live metrics.\"\"\"\n\n    if report.triggered:\n        return True\n    if accuracy &lt; 0.78:\n        return True\n    if latency &gt; 0.5:\n        return True\n    return False\n\n\n@dataclass\nclass CanaryVerdict:\n    promote: bool\n    reason: str\n    metrics: Dict[str, float] = field(default_factory=dict)\n\n\ndef evaluate_canary(\n    baseline_metrics: Mapping[str, float],\n    candidate_metrics: Mapping[str, float],\n    *,\n    allowed_latency_delta: float = 0.05,\n    min_accuracy: float = 0.8,\n) -&gt; CanaryVerdict:\n    \"\"\"Compare baseline vs candidate metrics and decide promotion.\"\"\"\n\n    latency_delta = candidate_metrics.get(\"latency\", 0.0) - baseline_metrics.get(\n        \"latency\", 0.0\n    )\n    accuracy = candidate_metrics.get(\"accuracy\", 0.0)\n    error_rate = candidate_metrics.get(\"error_rate\", 0.0)\n    if accuracy &lt; min_accuracy:\n        return CanaryVerdict(False, \"Accuracy below threshold\", {\"accuracy\": accuracy})\n    if latency_delta &gt; allowed_latency_delta:\n        return CanaryVerdict(\n            False, \"Latency regression\", {\"latency_delta\": round(latency_delta, 4)}\n        )\n    if error_rate &gt; baseline_metrics.get(\"error_rate\", 0.0) * 1.2:\n        return CanaryVerdict(False, \"Error rate increase\", {\"error_rate\": error_rate})\n    return CanaryVerdict(\n        True,\n        \"Canary healthy\",\n        {\"accuracy\": accuracy, \"latency_delta\": round(latency_delta, 4)},\n    )\n\n\ndef build_observability_snapshot(\n    report: DriftReport,\n    verdict: CanaryVerdict,\n    *,\n    predictions_served: int,\n) -&gt; Dict[str, Any]:\n    \"\"\"Aggregate metrics for Prometheus/OpenTelemetry exporters.\"\"\"\n\n    return {\n        \"drift\": {\n            \"feature\": report.feature,\n            \"score\": report.drift_score,\n            \"triggered\": report.triggered,\n        },\n        \"canary\": {\n            \"promote\": verdict.promote,\n            \"reason\": verdict.reason,\n            \"metrics\": verdict.metrics,\n        },\n        \"counters\": {\n            \"predictions_served_total\": predictions_served,\n        },\n    }\n\n\ndef detect_drift_across_features(\n    baseline_frame: Mapping[str, Iterable[float]],\n    current_frame: Mapping[str, Iterable[float]],\n    *,\n    threshold: float = 0.2,\n) -&gt; Dict[str, DriftReport]:\n    \"\"\"Apply mean drift detection across multiple features.\"\"\"\n\n    reports: Dict[str, DriftReport] = {}\n    for feature, baseline_values in baseline_frame.items():\n        current_values = current_frame.get(feature)\n        if current_values is None:\n            continue\n        reports[feature] = compute_mean_drift(\n            baseline_values, current_values, threshold=threshold\n        )\n    return reports\n\n\ndef enqueue_retraining_tasks(\n    reports: Mapping[str, DriftReport],\n    *,\n    accuracy: float,\n    latency: float,\n) -&gt; List[str]:\n    \"\"\"Return a queue of features that should trigger retraining.\"\"\"\n\n    queue: List[str] = []\n    for feature, report in reports.items():\n        if should_trigger_retraining(report, accuracy=accuracy, latency=latency):\n            queue.append(feature)\n    return queue\n\n\nif __name__ == \"__main__\":\n    baseline = [0.1, 0.2, 0.15, 0.18]\n    current = [0.35, 0.4, 0.45, 0.38]\n    report = compute_mean_drift(baseline, current)\n    verdict = evaluate_canary(\n        {\"latency\": 0.2, \"accuracy\": 0.83, \"error_rate\": 0.05},\n        {\"latency\": 0.22, \"accuracy\": 0.85, \"error_rate\": 0.04},\n    )\n    snapshot = build_observability_snapshot(report, verdict, predictions_served=1200)\n    print(\"Drift report\", report)  # noqa: T201\n    print(\"Canary verdict\", verdict)  # noqa: T201\n    print(\"Observability snapshot\", snapshot)  # noqa: T201\n</code></pre>"}]}