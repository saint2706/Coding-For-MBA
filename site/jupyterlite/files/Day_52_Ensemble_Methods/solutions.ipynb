{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab44f5e8",
   "metadata": {},
   "source": [
    "# Day 52 â€“ Ensemble Methods\n",
    "\n",
    "Day 52 highlights how bagging, boosting, and stacking unlock better accuracy\n",
    "than single estimators. Use the notebook or `solutions.py` helpers to:\n",
    "\n",
    "- Generate a balanced synthetic dataset for comparing ensemble families.\n",
    "- Train a random forest with out-of-bag (OOB) scoring and export feature\n",
    "  importances for stakeholder-ready summaries.\n",
    "- Fit a gradient boosting model, combine learners through stacking, and calibrate\n",
    "  probabilities so the predictions can power downstream decision rules.\n",
    "\n",
    "Execute `python Day_52_Ensemble_Methods/solutions.py` to print validation scores\n",
    "for each ensemble configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76aac76",
   "metadata": {},
   "source": [
    "Reusable ensemble helpers for Day 52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, brier_score_loss\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnsembleResult:\n",
    "    \"\"\"Summary of an ensemble model and its validation score.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    model: object\n",
    "    score: float\n",
    "\n",
    "\n",
    "def generate_classification_data(\n",
    "    n_samples: int = 400,\n",
    "    n_features: int = 12,\n",
    "    n_informative: int = 6,\n",
    "    class_sep: float = 1.8,\n",
    "    random_state: int = 52,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return a deterministic classification dataset suitable for ensembles.\"\"\"\n",
    "\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_informative,\n",
    "        n_redundant=0,\n",
    "        class_sep=class_sep,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_random_forest(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    n_estimators: int = 200,\n",
    "    max_depth: int | None = None,\n",
    "    random_state: int = 52,\n",
    ") -> RandomForestClassifier:\n",
    "    \"\"\"Fit a random forest classifier with out-of-bag scoring enabled.\"\"\"\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        oob_score=True,\n",
    "        random_state=random_state,\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_gradient_boosting(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.05,\n",
    "    n_estimators: int = 300,\n",
    "    max_depth: int = 2,\n",
    "    random_state: int = 52,\n",
    ") -> GradientBoostingClassifier:\n",
    "    \"\"\"Train a gradient boosting classifier with mild regularisation.\"\"\"\n",
    "\n",
    "    model = GradientBoostingClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_stacking_classifier(\n",
    "    estimators: List[Tuple[str, Pipeline]] | None = None,\n",
    "    random_state: int = 52,\n",
    ") -> StackingClassifier:\n",
    "    \"\"\"Create a stacking classifier with logistic regression as the final estimator.\"\"\"\n",
    "\n",
    "    if estimators is None:\n",
    "        estimators = [\n",
    "            (\n",
    "                \"rf\",\n",
    "                make_pipeline(\n",
    "                    StandardScaler(with_mean=False),\n",
    "                    RandomForestClassifier(\n",
    "                        n_estimators=150,\n",
    "                        max_depth=None,\n",
    "                        random_state=random_state,\n",
    "                        n_jobs=-1,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"gb\",\n",
    "                make_pipeline(\n",
    "                    StandardScaler(),\n",
    "                    GradientBoostingClassifier(\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=200,\n",
    "                        max_depth=2,\n",
    "                        random_state=random_state,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    final_estimator = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "    stacking = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=final_estimator,\n",
    "        passthrough=False,\n",
    "        stack_method=\"predict_proba\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    return stacking\n",
    "\n",
    "\n",
    "def calibrate_classifier(\n",
    "    model,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    method: str = \"isotonic\",\n",
    "    cv: int = 3,\n",
    ") -> CalibratedClassifierCV:\n",
    "    \"\"\"Wrap a fitted classifier with probability calibration.\"\"\"\n",
    "\n",
    "    calibrated = CalibratedClassifierCV(estimator=model, method=method, cv=cv)\n",
    "    calibrated.fit(X_train, y_train)\n",
    "    return calibrated\n",
    "\n",
    "\n",
    "def evaluate_classifier(\n",
    "    model, X_test: np.ndarray, y_test: np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Return accuracy and Brier score for the provided classifier.\"\"\"\n",
    "\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_test, preds)),\n",
    "        \"brier\": float(brier_score_loss(y_test, probs)),\n",
    "    }\n",
    "\n",
    "\n",
    "def export_feature_importance(\n",
    "    model: RandomForestClassifier,\n",
    "    feature_names: Sequence[str],\n",
    "    output_path: str | Path | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Return and optionally persist feature importances as a DataFrame.\"\"\"\n",
    "\n",
    "    importances = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": list(feature_names),\n",
    "            \"importance\": model.feature_importances_,\n",
    "        }\n",
    "    ).sort_values(\"importance\", ascending=False)\n",
    "    if output_path is not None:\n",
    "        output_path = Path(output_path)\n",
    "        importances.to_csv(output_path, index=False)\n",
    "    return importances.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def evaluate_with_cross_validation(\n",
    "    model,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    cv: int = 5,\n",
    "    scoring: str = \"roc_auc\",\n",
    ") -> float:\n",
    "    \"\"\"Return the mean cross-validated score for a classifier.\"\"\"\n",
    "\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "def run_day52_demo() -> Dict[str, EnsembleResult]:\n",
    "    \"\"\"Train and evaluate the featured ensemble models.\"\"\"\n",
    "\n",
    "    X, y = generate_classification_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=52, stratify=y\n",
    "    )\n",
    "\n",
    "    rf = train_random_forest(X_train, y_train)\n",
    "    gb = train_gradient_boosting(X_train, y_train)\n",
    "\n",
    "    stacking = build_stacking_classifier()\n",
    "    stacking.fit(X_train, y_train)\n",
    "    calibrated = calibrate_classifier(stacking, X_train, y_train)\n",
    "\n",
    "    results = {\n",
    "        \"random_forest\": EnsembleResult(\n",
    "            name=\"random_forest\",\n",
    "            model=rf,\n",
    "            score=float(rf.oob_score_),\n",
    "        ),\n",
    "        \"gradient_boosting\": EnsembleResult(\n",
    "            name=\"gradient_boosting\",\n",
    "            model=gb,\n",
    "            score=float(evaluate_with_cross_validation(gb, X, y)),\n",
    "        ),\n",
    "        \"stacking_calibrated\": EnsembleResult(\n",
    "            name=\"stacking_calibrated\",\n",
    "            model=calibrated,\n",
    "            score=evaluate_classifier(calibrated, X_test, y_test)[\"accuracy\"],\n",
    "        ),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_day52_demo()\n",
    "    for name, result in results.items():\n",
    "        print(f\"{name}: validation score = {result.score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
