{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160b478f",
   "metadata": {},
   "source": [
    "# Day 66 – Model Deployment and Serving Patterns\n",
    "\n",
    "Production machine learning systems expose predictions through a variety\n",
    "of runtime patterns. Building on the MLOps pipeline from Day 65, this\n",
    "lesson compares the trade-offs between synchronous APIs, high-throughput\n",
    "RPC services, scheduled batch scoring, streaming inference, and\n",
    "resource-constrained edge deployments.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- **REST serving with FastAPI** – Design JSON contracts, response\n",
    "  schemas, and dependency-injected models that work with serverless\n",
    "  platforms, container orchestrators, or BentoML services.\n",
    "- **gRPC microservices** – Use protobuf schemas and streaming RPCs for\n",
    "  low-latency online inference with strong typing and bi-directional\n",
    "  streaming.\n",
    "- **Batch and streaming predictions** – Trigger nightly or hourly\n",
    "  backfills alongside event-driven inference to balance cost and\n",
    "  freshness requirements.\n",
    "- **Edge deployment** – Package lightweight runtimes that run inside\n",
    "  mobile apps, browsers (WebAssembly), or IoT gateways with offline\n",
    "  caching and resilience to intermittent connectivity.\n",
    "- **Operational readiness** – Instrument health endpoints, log\n",
    "  structured telemetry, and integrate load testing into CI to prevent\n",
    "  regressions.\n",
    "\n",
    "## Hands-on practice\n",
    "\n",
    "`solutions.py` provides protocol-specific adapters for a mock model and a\n",
    "synthetic load-testing harness. The helpers lean on FastAPI-style input\n",
    "validation semantics and BentoML-inspired service bundling while keeping\n",
    "runtime dependencies lightweight for local experimentation.\n",
    "\n",
    "Run the example service locally:\n",
    "\n",
    "```bash\n",
    "python Day_66_Model_Deployment_and_Serving/solutions.py\n",
    "```\n",
    "\n",
    "Then execute the tests (`tests/test_day_66.py`) to verify that the REST,\n",
    "gRPC, and batch adapters share a consistent response schema and survive a\n",
    "stress scenario with concurrent workers.\n",
    "\n",
    "## Extend the exercise\n",
    "\n",
    "- Swap the stubbed adapters with real FastAPI routers and BentoML\n",
    "  services to deploy a containerised API.\n",
    "- Generate protobuf definitions for the gRPC helper and implement a\n",
    "  client using `grpcio` or `grpclib`.\n",
    "- Port the load test harness to `locust`, `k6`, or `vegeta` and capture\n",
    "  latency percentiles across different hardware profiles.\n",
    "- Add schema evolution examples demonstrating backwards-compatible API\n",
    "  rollouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926dc27e",
   "metadata": {},
   "source": [
    "Adapters that demonstrate multiple model serving patterns.\n",
    "\n",
    "The code intentionally avoids heavyweight dependencies so it can run\n",
    "inside unit tests, yet the abstractions mirror FastAPI/BentoML service\n",
    "interfaces, gRPC handlers, and streaming/batch processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from statistics import mean\n",
    "from time import perf_counter\n",
    "from typing import Any, Callable, Dict, Iterable, List, Mapping, Sequence\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PredictionResponse:\n",
    "    \"\"\"Normalised response payload shared across transports.\"\"\"\n",
    "\n",
    "    predictions: List[float]\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\"predictions\": self.predictions, \"metadata\": self.metadata}\n",
    "\n",
    "\n",
    "def _coerce_predictions(raw: Iterable[Any]) -> List[float]:\n",
    "    values: List[float] = []\n",
    "    for item in raw:\n",
    "        try:\n",
    "            values.append(float(item))\n",
    "        except (TypeError, ValueError) as exc:  # pragma: no cover - defensive\n",
    "            raise ValueError(f\"Prediction values must be numeric: {item!r}\") from exc\n",
    "    return values\n",
    "\n",
    "\n",
    "def fastapi_rest_adapter(\n",
    "    model: Callable[[Sequence[float]], Sequence[float]],\n",
    ") -> Callable[[Mapping[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"Create a FastAPI-style callable that accepts JSON payloads.\"\"\"\n",
    "\n",
    "    def predict(payload: Mapping[str, Any]) -> Dict[str, Any]:\n",
    "        instances = payload.get(\"instances\")\n",
    "        if instances is None:\n",
    "            raise KeyError(\"Payload missing 'instances'\")\n",
    "        predictions = model(instances)\n",
    "        response = PredictionResponse(\n",
    "            predictions=_coerce_predictions(predictions),\n",
    "            metadata={\"transport\": \"REST\", \"framework\": \"FastAPI\"},\n",
    "        )\n",
    "        return response.to_dict()\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "def grpc_streaming_adapter(\n",
    "    model: Callable[[Sequence[float]], Sequence[float]],\n",
    ") -> Callable[[Iterable[Mapping[str, Any]]], Iterable[Dict[str, Any]]]:\n",
    "    \"\"\"Return a generator-like gRPC handler that yields streaming responses.\"\"\"\n",
    "\n",
    "    def handler(\n",
    "        request_iterator: Iterable[Mapping[str, Any]],\n",
    "    ) -> Iterable[Dict[str, Any]]:\n",
    "        for payload in request_iterator:\n",
    "            instances = payload.get(\"instances\", [])\n",
    "            predictions = model(instances)\n",
    "            response = PredictionResponse(\n",
    "                predictions=_coerce_predictions(predictions),\n",
    "                metadata={\"transport\": \"gRPC\", \"streaming\": True},\n",
    "            )\n",
    "            yield response.to_dict()\n",
    "\n",
    "    return handler\n",
    "\n",
    "\n",
    "def batch_scoring_runner(\n",
    "    model: Callable[[Sequence[float]], Sequence[float]],\n",
    "    batches: Iterable[Sequence[float]],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process offline batches while preserving response structure.\"\"\"\n",
    "\n",
    "    outputs: List[Dict[str, Any]] = []\n",
    "    for batch in batches:\n",
    "        predictions = model(batch)\n",
    "        response = PredictionResponse(\n",
    "            predictions=_coerce_predictions(predictions),\n",
    "            metadata={\"transport\": \"batch\", \"batch_size\": len(list(batch))},\n",
    "        )\n",
    "        outputs.append(response.to_dict())\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def edge_inference_adapter(\n",
    "    model: Callable[[Sequence[float]], Sequence[float]], *, quantise: bool = True\n",
    ") -> Callable[[Sequence[float]], Dict[str, Any]]:\n",
    "    \"\"\"Wrap a model for offline/edge execution with optional quantisation.\"\"\"\n",
    "\n",
    "    def run(features: Sequence[float]) -> Dict[str, Any]:\n",
    "        scaled = [round(float(x), 3) for x in features]\n",
    "        predictions = model(scaled)\n",
    "        response = PredictionResponse(\n",
    "            predictions=_coerce_predictions(predictions),\n",
    "            metadata={\"transport\": \"edge\", \"quantised\": quantise},\n",
    "        )\n",
    "        return response.to_dict()\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "def ensure_response_schema(payload: Mapping[str, Any]) -> None:\n",
    "    \"\"\"Validate that a payload follows the canonical schema.\"\"\"\n",
    "\n",
    "    if \"predictions\" not in payload:\n",
    "        raise AssertionError(\"Missing predictions field\")\n",
    "    if not isinstance(payload[\"predictions\"], list):\n",
    "        raise AssertionError(\"Predictions must be a list\")\n",
    "    for value in payload[\"predictions\"]:\n",
    "        if not isinstance(value, (int, float)):\n",
    "            raise AssertionError(\"Predictions must be numeric\")\n",
    "    metadata = payload.get(\"metadata\", {})\n",
    "    if not isinstance(metadata, dict):\n",
    "        raise AssertionError(\"Metadata must be a mapping\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LoadTestResult:\n",
    "    avg_latency: float\n",
    "    throughput: float\n",
    "    success_rate: float\n",
    "\n",
    "\n",
    "def run_synthetic_load_test(\n",
    "    endpoint: Callable[[Mapping[str, Any]], Mapping[str, Any]],\n",
    "    payloads: Sequence[Mapping[str, Any]],\n",
    "    *,\n",
    "    warmups: int = 1,\n",
    ") -> LoadTestResult:\n",
    "    \"\"\"Execute a deterministic load-test loop against an endpoint.\"\"\"\n",
    "\n",
    "    total_latency = 0.0\n",
    "    successes = 0\n",
    "    # Warmup calls (not included in metrics but ensure caches are primed)\n",
    "    for i in range(warmups):\n",
    "        endpoint(payloads[i % len(payloads)])\n",
    "    for payload in payloads:\n",
    "        start = perf_counter()\n",
    "        response = endpoint(payload)\n",
    "        latency = perf_counter() - start\n",
    "        total_latency += latency\n",
    "        try:\n",
    "            ensure_response_schema(response)\n",
    "            successes += 1\n",
    "        except AssertionError:\n",
    "            pass\n",
    "    avg_latency = total_latency / len(payloads)\n",
    "    throughput = len(payloads) / max(total_latency, 1e-6)\n",
    "    success_rate = successes / len(payloads)\n",
    "    return LoadTestResult(\n",
    "        avg_latency=avg_latency, throughput=throughput, success_rate=success_rate\n",
    "    )\n",
    "\n",
    "\n",
    "def averaged_ensembled_model(instances: Sequence[float]) -> List[float]:\n",
    "    \"\"\"Reference model that mimics an ensemble averaged prediction.\"\"\"\n",
    "\n",
    "    if not instances:\n",
    "        return [0.0]\n",
    "    centre = mean(float(x) for x in instances)\n",
    "    return [round(centre * 0.8 + 0.1, 4)]\n",
    "\n",
    "\n",
    "def describe_serving_landscape() -> Dict[str, Any]:\n",
    "    \"\"\"Summarise the pros/cons of deployment patterns for quick reference.\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"REST\": {\"latency\": \"medium\", \"strength\": \"ubiquitous clients\"},\n",
    "        \"gRPC\": {\"latency\": \"low\", \"strength\": \"typed contracts\"},\n",
    "        \"batch\": {\"latency\": \"high\", \"strength\": \"cost efficiency\"},\n",
    "        \"streaming\": {\"latency\": \"low\", \"strength\": \"event-driven\"},\n",
    "        \"edge\": {\"latency\": \"ultra-low\", \"strength\": \"offline ready\"},\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = averaged_ensembled_model\n",
    "    endpoint = fastapi_rest_adapter(model)\n",
    "    sample_payloads = [{\"instances\": [0.1, 0.2, 0.4]} for _ in range(10)]\n",
    "    result = run_synthetic_load_test(endpoint, sample_payloads)\n",
    "    print(\"Synthetic load test\", result)  # noqa: T201"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
