{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b09602",
   "metadata": {},
   "source": [
    "# Day 62 â€“ Model Interpretability and Fairness\n",
    "\n",
    "Explainable and responsible AI practices underpin trustworthy analytics. After this lesson you will:\n",
    "\n",
    "- Compute additive SHAP-style attributions for linear models and verify they sum to the predicted score.\n",
    "- Fit lightweight LIME surrogates around individual observations using locally weighted regression.\n",
    "- Produce counterfactual examples that respect feature bounds to meet target outcomes.\n",
    "- Quantify bias with statistical parity, disparate impact, and equal opportunity metrics.\n",
    "- Apply simple reweighing mitigation to close gaps in simulated lending data.\n",
    "\n",
    "Run `python Day_62_Model_Interpretability_and_Fairness/solutions.py` to walk through interpretability utilities, fairness diagnostics, and mitigation experiments on deterministic toy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd778c5",
   "metadata": {},
   "source": [
    "Interpretability and fairness helpers for Day 62."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8498817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Mapping, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LinearModel:\n",
    "    \"\"\"Simple linear model container.\"\"\"\n",
    "\n",
    "    coefficients: np.ndarray\n",
    "    intercept: float\n",
    "    feature_names: Sequence[str]\n",
    "\n",
    "    def predict(self, features: Sequence[float]) -> float:\n",
    "        vector = np.asarray(features, dtype=float)\n",
    "        return float(self.intercept + np.dot(self.coefficients, vector))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ShapExplanation:\n",
    "    \"\"\"Container for SHAP-style additive explanations.\"\"\"\n",
    "\n",
    "    base_value: float\n",
    "    contributions: np.ndarray\n",
    "\n",
    "    def reconstructed_prediction(self) -> float:\n",
    "        return float(self.base_value + self.contributions.sum())\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LimeExplanation:\n",
    "    \"\"\"Container for LIME-style local linear approximations.\"\"\"\n",
    "\n",
    "    intercept: float\n",
    "    weights: np.ndarray\n",
    "    prediction: float\n",
    "\n",
    "    def local_prediction(self, instance: Sequence[float]) -> float:\n",
    "        vector = np.asarray(instance, dtype=float)\n",
    "        return float(self.intercept + np.dot(self.weights, vector))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CounterfactualResult:\n",
    "    \"\"\"Result of a counterfactual search.\"\"\"\n",
    "\n",
    "    original_prediction: float\n",
    "    target: float\n",
    "    counterfactual_features: np.ndarray\n",
    "    counterfactual_prediction: float\n",
    "    delta: np.ndarray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FairnessReport:\n",
    "    \"\"\"Bias metrics calculated on binary outcomes.\"\"\"\n",
    "\n",
    "    statistical_parity: float\n",
    "    disparate_impact: float\n",
    "    equal_opportunity: float\n",
    "\n",
    "\n",
    "def load_credit_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Return a deterministic lending dataset for fairness experiments.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(62)\n",
    "    records: List[Dict[str, float]] = []\n",
    "    for credit_score in (580, 620, 660, 700, 740):\n",
    "        for income in (42_000, 58_000, 74_000):\n",
    "            for gender in (\"F\", \"M\"):\n",
    "                base_prob = (\n",
    "                    0.15 + 0.0006 * (credit_score - 600) + 0.000002 * (income - 50_000)\n",
    "                )\n",
    "                shift = -0.04 if gender == \"F\" else 0.0\n",
    "                approval = rng.random() < (base_prob + shift)\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"credit_score\": credit_score,\n",
    "                        \"income\": income,\n",
    "                        \"gender\": gender,\n",
    "                        \"approved\": float(approval),\n",
    "                        \"default_risk\": 0.35\n",
    "                        - 0.0004 * credit_score\n",
    "                        - 0.0000015 * income\n",
    "                        + (0.02 if gender == \"F\" else 0.0),\n",
    "                    }\n",
    "                )\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "\n",
    "def train_default_risk_model() -> LinearModel:\n",
    "    \"\"\"Fit a closed-form linear regression on the credit dataset.\"\"\"\n",
    "\n",
    "    df = load_credit_dataset()\n",
    "    feature_names = [\"credit_score\", \"income\", \"is_female\"]\n",
    "    X = np.column_stack(\n",
    "        [\n",
    "            df[\"credit_score\"].to_numpy(dtype=float),\n",
    "            df[\"income\"].to_numpy(dtype=float),\n",
    "            (df[\"gender\"] == \"F\").to_numpy(dtype=float),\n",
    "        ]\n",
    "    )\n",
    "    y = df[\"default_risk\"].to_numpy(dtype=float)\n",
    "    X_design = np.column_stack([np.ones(len(df)), X])\n",
    "    coefficients, *_ = np.linalg.lstsq(X_design, y, rcond=None)\n",
    "    intercept = float(coefficients[0])\n",
    "    weights = np.asarray(coefficients[1:], dtype=float)\n",
    "    return LinearModel(\n",
    "        coefficients=weights, intercept=intercept, feature_names=feature_names\n",
    "    )\n",
    "\n",
    "\n",
    "def _baseline_from_dataset(\n",
    "    model: LinearModel, dataset: pd.DataFrame | None = None\n",
    ") -> Tuple[float, np.ndarray]:\n",
    "    if dataset is None:\n",
    "        dataset = load_credit_dataset()\n",
    "    baseline_features = np.column_stack(\n",
    "        [\n",
    "            dataset[\"credit_score\"].to_numpy(dtype=float),\n",
    "            dataset[\"income\"].to_numpy(dtype=float),\n",
    "            (dataset[\"gender\"] == \"F\").to_numpy(dtype=float),\n",
    "        ]\n",
    "    ).mean(axis=0)\n",
    "    base_value = model.predict(baseline_features)\n",
    "    return base_value, baseline_features\n",
    "\n",
    "\n",
    "def compute_shap_values(\n",
    "    model: LinearModel,\n",
    "    instance: Sequence[float],\n",
    "    dataset: pd.DataFrame | None = None,\n",
    ") -> ShapExplanation:\n",
    "    \"\"\"Return additive SHAP-style contributions for a linear model.\"\"\"\n",
    "\n",
    "    base_value, baseline_features = _baseline_from_dataset(model, dataset)\n",
    "    instance_arr = np.asarray(instance, dtype=float)\n",
    "    contributions = model.coefficients * (instance_arr - baseline_features)\n",
    "    return ShapExplanation(base_value=base_value, contributions=contributions)\n",
    "\n",
    "\n",
    "def _kernel_weights(distances: np.ndarray, kernel_width: float) -> np.ndarray:\n",
    "    weights = np.exp(-(distances**2) / (kernel_width**2))\n",
    "    return weights / (weights.sum() + 1e-12)\n",
    "\n",
    "\n",
    "def lime_explanation(\n",
    "    model: LinearModel,\n",
    "    instance: Sequence[float],\n",
    "    num_samples: int = 200,\n",
    "    kernel_width: float = 0.75,\n",
    "    random_state: int = 62,\n",
    ") -> LimeExplanation:\n",
    "    \"\"\"Fit a locally weighted surrogate model around an instance.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    instance_arr = np.asarray(instance, dtype=float)\n",
    "    noise = rng.normal(\n",
    "        scale=[20.0, 5_000.0, 0.2], size=(num_samples, instance_arr.size)\n",
    "    )\n",
    "    samples = instance_arr + noise\n",
    "    predictions = np.apply_along_axis(model.predict, 1, samples)\n",
    "    distances = np.linalg.norm(samples - instance_arr, axis=1)\n",
    "    weights = _kernel_weights(distances, kernel_width)\n",
    "    X_design = np.column_stack([np.ones(num_samples), samples])\n",
    "    W = np.diag(weights)\n",
    "    beta = np.linalg.pinv(X_design.T @ W @ X_design) @ (X_design.T @ W @ predictions)\n",
    "    coefs = np.asarray(beta[1:], dtype=float)\n",
    "    prediction = model.predict(instance_arr)\n",
    "    intercept = float(prediction - np.dot(coefs, instance_arr))\n",
    "    return LimeExplanation(intercept=intercept, weights=coefs, prediction=prediction)\n",
    "\n",
    "\n",
    "def generate_counterfactual(\n",
    "    model: LinearModel,\n",
    "    instance: Sequence[float],\n",
    "    target: float,\n",
    "    bounds: Mapping[str, Tuple[float, float]],\n",
    ") -> CounterfactualResult:\n",
    "    \"\"\"Compute a counterfactual by moving along the coefficient direction.\"\"\"\n",
    "\n",
    "    features = np.asarray(instance, dtype=float)\n",
    "    original_pred = model.predict(features)\n",
    "    direction = model.coefficients\n",
    "    scale = (target - original_pred) / (np.dot(direction, direction) + 1e-12)\n",
    "    raw_cf = features + scale * direction\n",
    "    ordered_bounds = np.array(\n",
    "        [bounds[name] for name in model.feature_names], dtype=float\n",
    "    )\n",
    "    clipped_cf = np.clip(raw_cf, ordered_bounds[:, 0], ordered_bounds[:, 1])\n",
    "    cf_prediction = model.predict(clipped_cf)\n",
    "    return CounterfactualResult(\n",
    "        original_prediction=original_pred,\n",
    "        target=target,\n",
    "        counterfactual_features=clipped_cf,\n",
    "        counterfactual_prediction=cf_prediction,\n",
    "        delta=clipped_cf - features,\n",
    "    )\n",
    "\n",
    "\n",
    "def fairness_metrics(dataset: pd.DataFrame) -> FairnessReport:\n",
    "    \"\"\"Compute key bias metrics for the approval outcome.\"\"\"\n",
    "\n",
    "    grouped = dataset.groupby(\"gender\")\n",
    "    approval_rate = grouped[\"approved\"].mean()\n",
    "    female_rate = float(approval_rate.get(\"F\", np.nan))\n",
    "    male_rate = float(approval_rate.get(\"M\", np.nan))\n",
    "    statistical_parity = female_rate - male_rate\n",
    "    disparate_impact = female_rate / male_rate if male_rate > 0 else np.nan\n",
    "\n",
    "    # Equal opportunity: P(approval=1 | default risk below threshold)\n",
    "    low_risk = dataset[dataset[\"default_risk\"] < dataset[\"default_risk\"].median()]\n",
    "    eq_grouped = low_risk.groupby(\"gender\")[\"approved\"].mean()\n",
    "    female_eq = float(eq_grouped.get(\"F\", np.nan))\n",
    "    male_eq = float(eq_grouped.get(\"M\", np.nan))\n",
    "    equal_opportunity = female_eq - male_eq\n",
    "\n",
    "    return FairnessReport(\n",
    "        statistical_parity=statistical_parity,\n",
    "        disparate_impact=disparate_impact,\n",
    "        equal_opportunity=equal_opportunity,\n",
    "    )\n",
    "\n",
    "\n",
    "def apply_reweighing(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a dataframe with sample weights that mitigate statistical parity gaps.\"\"\"\n",
    "\n",
    "    df = dataset.copy()\n",
    "    grouped = df.groupby(\"gender\")\n",
    "    approval_rate = grouped[\"approved\"].mean()\n",
    "    target_rate = float(df[\"approved\"].mean())\n",
    "    weights = []\n",
    "    for _, row in df.iterrows():\n",
    "        group_rate = float(approval_rate[row[\"gender\"]])\n",
    "        weights.append(target_rate / (group_rate + 1e-12))\n",
    "    df[\"sample_weight\"] = weights\n",
    "    return df\n",
    "\n",
    "\n",
    "def mitigation_effect(dataset: pd.DataFrame) -> FairnessReport:\n",
    "    \"\"\"Recompute fairness metrics after reweighing.\"\"\"\n",
    "\n",
    "    reweighted = apply_reweighing(dataset)\n",
    "    weighted = {\n",
    "        gender: float(np.average(grp[\"approved\"], weights=grp[\"sample_weight\"]))\n",
    "        for gender, grp in reweighted.groupby(\"gender\")\n",
    "    }\n",
    "    female_rate = float(weighted.get(\"F\", np.nan))\n",
    "    male_rate = float(weighted.get(\"M\", np.nan))\n",
    "    statistical_parity = female_rate - male_rate\n",
    "    disparate_impact = female_rate / male_rate if male_rate > 0 else np.nan\n",
    "\n",
    "    low_risk = reweighted[\n",
    "        reweighted[\"default_risk\"] < reweighted[\"default_risk\"].median()\n",
    "    ]\n",
    "    eq_weighted = {\n",
    "        gender: float(np.average(grp[\"approved\"], weights=grp[\"sample_weight\"]))\n",
    "        for gender, grp in low_risk.groupby(\"gender\")\n",
    "    }\n",
    "    female_eq = float(eq_weighted.get(\"F\", np.nan))\n",
    "    male_eq = float(eq_weighted.get(\"M\", np.nan))\n",
    "    equal_opportunity = female_eq - male_eq\n",
    "\n",
    "    return FairnessReport(\n",
    "        statistical_parity=statistical_parity,\n",
    "        disparate_impact=disparate_impact,\n",
    "        equal_opportunity=equal_opportunity,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_interpretability_suite() -> Dict[str, object]:\n",
    "    \"\"\"Execute interpretability and fairness utilities for documentation demos.\"\"\"\n",
    "\n",
    "    model = train_default_risk_model()\n",
    "    dataset = load_credit_dataset()\n",
    "    instance = dataset.loc[0, [\"credit_score\", \"income\", \"gender\"]]\n",
    "    encoded_instance = np.array(\n",
    "        [instance[\"credit_score\"], instance[\"income\"], float(instance[\"gender\"] == \"F\")]\n",
    "    )\n",
    "    shap = compute_shap_values(model, encoded_instance, dataset)\n",
    "    lime = lime_explanation(model, encoded_instance)\n",
    "    bounds = {\n",
    "        \"credit_score\": (500, 850),\n",
    "        \"income\": (30_000, 120_000),\n",
    "        \"is_female\": (0.0, 1.0),\n",
    "    }\n",
    "    counterfactual = generate_counterfactual(\n",
    "        model, encoded_instance, target=0.05, bounds=bounds\n",
    "    )\n",
    "    fairness = fairness_metrics(dataset)\n",
    "    mitigated = mitigation_effect(dataset)\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"shap\": shap,\n",
    "        \"lime\": lime,\n",
    "        \"counterfactual\": counterfactual,\n",
    "        \"fairness\": fairness,\n",
    "        \"mitigated\": mitigated,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    report = run_interpretability_suite()\n",
    "    print(\"Base prediction:\", report[\"shap\"].reconstructed_prediction())\n",
    "    print(\n",
    "        \"LIME local prediction:\",\n",
    "        report[\"lime\"].local_prediction(\n",
    "            report[\"counterfactual\"].counterfactual_features\n",
    "        ),\n",
    "    )\n",
    "    print(\"Counterfactual delta:\", report[\"counterfactual\"].delta)\n",
    "    print(\"Fairness metrics:\", report[\"fairness\"])\n",
    "    print(\"After mitigation:\", report[\"mitigated\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
