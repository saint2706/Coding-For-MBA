{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05731795",
   "metadata": {},
   "source": [
    "# üìò Day 30: Web Scraping - Extracting Data from the Web\n",
    "\n",
    "Sometimes, the data you need isn't available in a clean CSV file or through an API. It's simply displayed on a website. **Web scraping** is the process of automatically downloading the HTML code of a web page and extracting useful information from it.\n",
    "\n",
    "This is an incredibly powerful tool for a business analyst, allowing you to gather competitive intelligence, track news sentiment, collect product prices, and much more.\n",
    "\n",
    "## üì¶ Working Offline\n",
    "\n",
    "If you do not have internet access, you can still explore the examples in this lesson. The folder includes a curated `presidents.csv` containing a snapshot of key columns‚Äînumber, name, party, term dates, and vice presidents‚Äîfor every U.S. president through Joe Biden. The exercise scripts will look for this local file first, so you can experiment with parsing and analysis even when the Wikipedia page is unavailable. When a connection is available you can still re-run the scraper to refresh the dataset, which will regenerate `presidents.json`. Git ignores these generated JSON files so your repository stays clean.\n",
    "\n",
    "**A VERY IMPORTANT NOTE ON ETHICS AND LEGALITY:**\n",
    "\n",
    "- **Check `robots.txt`:** Always check a website's `robots.txt` file (e.g., `https://example.com/robots.txt`) to see which parts of the site you are allowed to scrape. Respect the rules.\n",
    "- **Be Gentle:** Don't send too many requests in a short period. You could overwhelm the website's server, which is inconsiderate and may get your IP address blocked. Introduce delays between your requests.\n",
    "- **Identify Yourself:** Set a user-agent in your request headers that identifies your script or bot.\n",
    "- **Public Data Only:** Only scrape data that is publicly visible. Do not attempt to scrape information that is behind a login or a paywall.\n",
    "\n",
    "## The Web Scraping Toolkit\n",
    "\n",
    "We will use two main libraries for web scraping:\n",
    "\n",
    "1. **`requests`**: A simple and elegant library for making HTTP requests to download web pages.\n",
    "1. **`BeautifulSoup`**: A library for parsing HTML and XML documents. It creates a parse tree from the page's source code that you can use to extract data.\n",
    "\n",
    "## The Scraping Process\n",
    "\n",
    "1. **Inspect the Page:** Use your web browser's \"Inspect\" or \"View Source\" tool to understand the HTML structure of the page you want to scrape. Find the HTML tags (e.g., `<h1>`, `<p>`, `<table>`, `<div>`) that contain the data you need. Look for unique `id` or `class` attributes on those tags.\n",
    "1. **Download the HTML:** Use the `requests.get(url)` function to download the page's HTML content.\n",
    "1. **Create a \"Soup\":** Pass the downloaded HTML to the `BeautifulSoup` constructor to create a parsable object.\n",
    "1. **Find Your Data:** Use BeautifulSoup's methods, like `find()` and `find_all()`, to locate the specific HTML tags containing your data.\n",
    "1. **Extract the Text:** Once you have the tags, use the `.get_text()` method to extract the clean text from them.\n",
    "1. **Structure the Data:** Organize your extracted data into a list or, even better, a Pandas DataFrame.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://example.com' # A simple example page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the first <h1> tag\n",
    "header = soup.find('h1').get_text()\n",
    "\n",
    "# Find all <p> (paragraph) tags\n",
    "paragraphs = soup.find_all('p')\n",
    "first_paragraph_text = paragraphs[0].get_text()\n",
    "```\n",
    "\n",
    "## üî¨ Profiling the Scraper\n",
    "\n",
    "Profiling helps you spot whether networking or HTML parsing is the bottleneck. Two helper commands wire into the shared profiler:\n",
    "\n",
    "```bash\n",
    "python Day_30_Web_Scraping/profile_web_scraping.py --mode cprofile\n",
    "python Day_30_Web_Scraping/profile_web_scraping.py --mode timeit --local-html Day_30_Web_Scraping/books_sample.html --repeat 5 --number 3\n",
    "```\n",
    "\n",
    "The `cProfile` output shows that almost all time is spent inside `requests.Session.get`‚Äînetwork I/O dominates the runtime, so batching requests or caching responses offers the biggest win.„Äêad83b3‚Ä†L1-L29„Äë For deterministic timing, use the saved `books_sample.html` page (refresh it with `curl http://books.toscrape.com/ -o Day_30_Web_Scraping/books_sample.html`). Parsing that local file takes ~0.03 seconds per iteration across five repeats, letting you focus on BeautifulSoup performance without hitting the network.„Äêde293a‚Ä†L1-L7„Äë Reusing a single `requests.Session` and avoiding repeated downloads can dramatically cut the cost when scraping multiple pages.\n",
    "\n",
    "## üíª Exercises: Day 30\n",
    "\n",
    "For these exercises, we will scrape the website `http://books.toscrape.com/`, a site specifically designed for scraping practice.\n",
    "\n",
    "1. **Scrape Book Titles:**\n",
    "\n",
    "   - Visit `http://books.toscrape.com/`.\n",
    "   - Write a script that downloads the page content.\n",
    "   - Create a BeautifulSoup object from the content.\n",
    "   - Find all the book titles on the first page. (Hint: Inspect the page to see what tag the titles are in. They are inside `<h3>` tags, within an `<a>` tag).\n",
    "   - Create a list of all the book titles and print it.\n",
    "\n",
    "1. **Scrape Book Prices:**\n",
    "\n",
    "   - On the same page, find all the book prices. (Hint: They are in `p` tags with the class `price_color`).\n",
    "   - Extract the text of the prices (e.g., \"¬£51.77\").\n",
    "   - Create a list of all the prices and print it.\n",
    "\n",
    "1. **Create a DataFrame:**\n",
    "\n",
    "   - Combine your work from the previous two exercises.\n",
    "   - Create a script that scrapes both the titles and the prices.\n",
    "   - Store the results in a Pandas DataFrame with two columns: \"Title\" and \"Price\".\n",
    "   - Print the first 5 rows of your new DataFrame using `.head()`.\n",
    "\n",
    "üéâ **Great job!** Web scraping is a powerful skill that opens up a vast new source of data for your analyses. While it can be complex, mastering the basics of `requests` and `BeautifulSoup` is a huge step forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce80b5",
   "metadata": {},
   "source": [
    "Day 30: Web Scraping in Practice\n",
    "\n",
    "This script demonstrates the fundamentals of web scraping by\n",
    "extracting book titles and prices from a practice website.\n",
    "\n",
    "This educational example shows how to:\n",
    "- Make HTTP requests with proper headers\n",
    "- Parse HTML content with BeautifulSoup\n",
    "- Handle errors gracefully\n",
    "- Extract and clean data\n",
    "- Structure data in pandas DataFrame\n",
    "\n",
    "Author: 50 Days of Python Course\n",
    "Purpose: Educational example for MBA students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The URL of the website we want to scrape\n",
    "# This site is specifically designed for scraping practice.\n",
    "URL = \"http://books.toscrape.com/\"\n",
    "\n",
    "\n",
    "class ScrapingError(Exception):\n",
    "    \"\"\"Custom exception for scraping errors.\"\"\"\n",
    "\n",
    "\n",
    "def scrape_books(\n",
    "    url: str, session: Optional[requests.Session] = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scrape book data from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to scrape\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the raw scraped DataFrame, the cleaned DataFrame, and\n",
    "        a dictionary of summary statistics.\n",
    "    \"\"\"\n",
    "    # --- 1. Download the HTML Content ---\n",
    "    # Use requests.get() to download the page.\n",
    "    # It's good practice to include a 'User-Agent' header to identify your script.\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.Timeout:\n",
    "        raise\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError:\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as exc:\n",
    "        raise ScrapingError(\"Error downloading the page\") from exc\n",
    "\n",
    "    # If we get here, the request was successful\n",
    "    return process_book_data(response.content)\n",
    "\n",
    "\n",
    "def process_book_data(\n",
    "    html_content: bytes,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process the HTML response and extract book data.\n",
    "\n",
    "    Args:\n",
    "        response: The HTTP response object\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the raw scraped DataFrame, the cleaned DataFrame, and\n",
    "        a dictionary of summary statistics.\n",
    "    \"\"\"\n",
    "    # --- 2. Create a BeautifulSoup Object ---\n",
    "    # This object parses the HTML content and makes it searchable.\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # --- 3. Find and Extract Data ---\n",
    "    # We inspected the page and found that book information is within <article> tags with the class 'product_pod'\n",
    "    books = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "    if not books:\n",
    "        raise ValueError(\"No books found in the provided HTML content\")\n",
    "\n",
    "    titles = []\n",
    "    prices = []\n",
    "\n",
    "    # Loop through each book found on the page\n",
    "    for book in books:\n",
    "        # Type check to ensure book is a Tag\n",
    "        if not isinstance(book, bs4.element.Tag):\n",
    "            continue\n",
    "\n",
    "        # The title is in an 'a' tag within an 'h3' tag.\n",
    "        # We access the 'title' attribute of the 'a' tag.\n",
    "        h3_tag = book.find(\"h3\")\n",
    "        if isinstance(h3_tag, bs4.element.Tag):\n",
    "            a_tag = h3_tag.find(\"a\")\n",
    "            if isinstance(a_tag, bs4.element.Tag):\n",
    "                title = a_tag.get(\"title\")\n",
    "                titles.append(str(title) if title else \"N/A\")\n",
    "            else:\n",
    "                titles.append(\"N/A\")\n",
    "        else:\n",
    "            titles.append(\"N/A\")\n",
    "\n",
    "        # The price is in a 'p' tag with the class 'price_color'\n",
    "        price_tag = book.find(\"p\", attrs={\"class\": \"price_color\"})\n",
    "        if isinstance(price_tag, bs4.element.Tag):\n",
    "            price_text = price_tag.get_text(strip=True)\n",
    "            prices.append(price_text)\n",
    "        else:\n",
    "            prices.append(\"N/A\")\n",
    "\n",
    "    # --- 4. Structure the Data in a DataFrame ---\n",
    "    if not titles or not prices or len(titles) != len(prices):\n",
    "        raise ValueError(\"Mismatch between titles and prices in the HTML content\")\n",
    "\n",
    "    book_data = pd.DataFrame({\"Title\": titles, \"Price\": prices})\n",
    "\n",
    "    # --- 5. Data Cleaning (Bonus) ---\n",
    "    clean_data = book_data.copy()\n",
    "    clean_data[\"Price_Float\"] = pd.to_numeric(\n",
    "        clean_data[\"Price\"].str.replace(\"¬£\", \"\", regex=False), errors=\"coerce\"\n",
    "    )\n",
    "    clean_data = clean_data.dropna(subset=[\"Price_Float\"]).copy()\n",
    "\n",
    "    if clean_data.empty:\n",
    "        return book_data, clean_data, {}\n",
    "\n",
    "    # --- 6. Basic Analysis ---\n",
    "    price_series = clean_data[\"Price_Float\"]\n",
    "    analysis: Dict[str, Any] = {\n",
    "        \"average_price\": float(price_series.mean()),\n",
    "        \"min_price\": float(price_series.min()),\n",
    "        \"max_price\": float(price_series.max()),\n",
    "        \"count\": int(len(clean_data)),\n",
    "    }\n",
    "\n",
    "    most_expensive = clean_data.loc[price_series.idxmax()]\n",
    "    cheapest = clean_data.loc[price_series.idxmin()]\n",
    "\n",
    "    analysis[\"most_expensive_title\"] = most_expensive[\"Title\"]\n",
    "    analysis[\"most_expensive_price\"] = most_expensive[\"Price\"]\n",
    "    analysis[\"cheapest_title\"] = cheapest[\"Title\"]\n",
    "    analysis[\"cheapest_price\"] = cheapest[\"Price\"]\n",
    "\n",
    "    return book_data, clean_data, analysis\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate web scraping workflow.\n",
    "    \"\"\"\n",
    "    print(\"üï∏Ô∏è  Day 30: Web Scraping Demonstration\")\n",
    "    print(\"üìö Scraping book data from books.toscrape.com\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Add a small delay to be respectful to the server\n",
    "    print(\"‚è≥ Starting scraping process...\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Execute the scraping\n",
    "    try:\n",
    "        print(f\"üåê Connecting to {URL}...\")\n",
    "        raw_df, clean_df, analysis = scrape_books(URL)\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"‚ùå Request timed out. The server might be slow or unresponsive.\")\n",
    "        return\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Connection error. Please check your internet connection.\")\n",
    "        return\n",
    "    except requests.exceptions.HTTPError as exc:\n",
    "        print(f\"‚ùå HTTP error occurred: {exc}\")\n",
    "        return\n",
    "    except ScrapingError as exc:\n",
    "        print(f\"‚ùå {exc}\")\n",
    "        print(\"üí° This could be due to:\")\n",
    "        print(\"   ‚Ä¢ Network connectivity issues\")\n",
    "        print(\"   ‚Ä¢ Website being temporarily unavailable\")\n",
    "        print(\"   ‚Ä¢ Blocked by website's anti-bot protection\")\n",
    "        print(\"   ‚Ä¢ URL has changed or is incorrect\")\n",
    "        return\n",
    "    except ValueError as exc:\n",
    "        print(f\"‚ùå {exc}\")\n",
    "        print(\"üí° The website structure may have changed. Try updating the parser.\")\n",
    "        return\n",
    "\n",
    "    print(\"‚úÖ Successfully downloaded the content!\")\n",
    "    print(f\"üìä Total books scraped: {len(raw_df)}\")\n",
    "\n",
    "    if clean_df.empty:\n",
    "        print(\"‚ö†Ô∏è  No valid price data found for analysis.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Sample of Scraped Book Data ---\")\n",
    "    print(raw_df.head(10))\n",
    "\n",
    "    print(\"\\n--- Cleaned Price Data ---\")\n",
    "    print(clean_df.head(10))\n",
    "\n",
    "    print(\"\\nüìà Basic Price Analysis:\")\n",
    "    print(f\"   Average price: ¬£{analysis['average_price']:.2f}\")\n",
    "    print(f\"   Minimum price: ¬£{analysis['min_price']:.2f}\")\n",
    "    print(f\"   Maximum price: ¬£{analysis['max_price']:.2f}\")\n",
    "    print(f\"   Number of books: {analysis['count']}\")\n",
    "    print(\n",
    "        f\"üí∞ Most expensive: '{analysis['most_expensive_title']}' - {analysis['most_expensive_price']}\"\n",
    "    )\n",
    "    print(f\"üí∏ Cheapest: '{analysis['cheapest_title']}' - {analysis['cheapest_price']}\")\n",
    "\n",
    "    print(\"\\nüí° Next steps you could take:\")\n",
    "    print(\"   ‚Ä¢ Save data to CSV: clean_df.to_csv('books.csv', index=False)\")\n",
    "    print(\"   ‚Ä¢ Filter books by price range\")\n",
    "    print(\"   ‚Ä¢ Scrape additional pages for more data\")\n",
    "    print(\"   ‚Ä¢ Add more data fields (ratings, availability, etc.)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
