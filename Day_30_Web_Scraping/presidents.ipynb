{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f6c23d",
   "metadata": {},
   "source": [
    "# üìò Day 30: Web Scraping - Extracting Data from the Web\n",
    "\n",
    "Sometimes, the data you need isn't available in a clean CSV file or through an API. It's simply displayed on a website. **Web scraping** is the process of automatically downloading the HTML code of a web page and extracting useful information from it.\n",
    "\n",
    "This is an incredibly powerful tool for a business analyst, allowing you to gather competitive intelligence, track news sentiment, collect product prices, and much more.\n",
    "\n",
    "## üì¶ Working Offline\n",
    "\n",
    "If you do not have internet access, you can still explore the examples in this lesson. The folder includes a curated `presidents.csv` containing a snapshot of key columns‚Äînumber, name, party, term dates, and vice presidents‚Äîfor every U.S. president through Joe Biden. The exercise scripts will look for this local file first, so you can experiment with parsing and analysis even when the Wikipedia page is unavailable. When a connection is available you can still re-run the scraper to refresh the dataset, which will regenerate `presidents.json`. Git ignores these generated JSON files so your repository stays clean.\n",
    "\n",
    "**A VERY IMPORTANT NOTE ON ETHICS AND LEGALITY:**\n",
    "\n",
    "- **Check `robots.txt`:** Always check a website's `robots.txt` file (e.g., `https://example.com/robots.txt`) to see which parts of the site you are allowed to scrape. Respect the rules.\n",
    "- **Be Gentle:** Don't send too many requests in a short period. You could overwhelm the website's server, which is inconsiderate and may get your IP address blocked. Introduce delays between your requests.\n",
    "- **Identify Yourself:** Set a user-agent in your request headers that identifies your script or bot.\n",
    "- **Public Data Only:** Only scrape data that is publicly visible. Do not attempt to scrape information that is behind a login or a paywall.\n",
    "\n",
    "## The Web Scraping Toolkit\n",
    "\n",
    "We will use two main libraries for web scraping:\n",
    "\n",
    "1. **`requests`**: A simple and elegant library for making HTTP requests to download web pages.\n",
    "1. **`BeautifulSoup`**: A library for parsing HTML and XML documents. It creates a parse tree from the page's source code that you can use to extract data.\n",
    "\n",
    "## The Scraping Process\n",
    "\n",
    "1. **Inspect the Page:** Use your web browser's \"Inspect\" or \"View Source\" tool to understand the HTML structure of the page you want to scrape. Find the HTML tags (e.g., `<h1>`, `<p>`, `<table>`, `<div>`) that contain the data you need. Look for unique `id` or `class` attributes on those tags.\n",
    "1. **Download the HTML:** Use the `requests.get(url)` function to download the page's HTML content.\n",
    "1. **Create a \"Soup\":** Pass the downloaded HTML to the `BeautifulSoup` constructor to create a parsable object.\n",
    "1. **Find Your Data:** Use BeautifulSoup's methods, like `find()` and `find_all()`, to locate the specific HTML tags containing your data.\n",
    "1. **Extract the Text:** Once you have the tags, use the `.get_text()` method to extract the clean text from them.\n",
    "1. **Structure the Data:** Organize your extracted data into a list or, even better, a Pandas DataFrame.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://example.com' # A simple example page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the first <h1> tag\n",
    "header = soup.find('h1').get_text()\n",
    "\n",
    "# Find all <p> (paragraph) tags\n",
    "paragraphs = soup.find_all('p')\n",
    "first_paragraph_text = paragraphs[0].get_text()\n",
    "```\n",
    "\n",
    "## üî¨ Profiling the Scraper\n",
    "\n",
    "Profiling helps you spot whether networking or HTML parsing is the bottleneck. Two helper commands wire into the shared profiler:\n",
    "\n",
    "```bash\n",
    "python Day_30_Web_Scraping/profile_web_scraping.py --mode cprofile\n",
    "python Day_30_Web_Scraping/profile_web_scraping.py --mode timeit --local-html Day_30_Web_Scraping/books_sample.html --repeat 5 --number 3\n",
    "```\n",
    "\n",
    "The `cProfile` output shows that almost all time is spent inside `requests.Session.get`‚Äînetwork I/O dominates the runtime, so batching requests or caching responses offers the biggest win.„Äêad83b3‚Ä†L1-L29„Äë For deterministic timing, use the saved `books_sample.html` page (refresh it with `curl http://books.toscrape.com/ -o Day_30_Web_Scraping/books_sample.html`). Parsing that local file takes ~0.03 seconds per iteration across five repeats, letting you focus on BeautifulSoup performance without hitting the network.„Äêde293a‚Ä†L1-L7„Äë Reusing a single `requests.Session` and avoiding repeated downloads can dramatically cut the cost when scraping multiple pages.\n",
    "\n",
    "## üíª Exercises: Day 30\n",
    "\n",
    "For these exercises, we will scrape the website `http://books.toscrape.com/`, a site specifically designed for scraping practice.\n",
    "\n",
    "1. **Scrape Book Titles:**\n",
    "\n",
    "   - Visit `http://books.toscrape.com/`.\n",
    "   - Write a script that downloads the page content.\n",
    "   - Create a BeautifulSoup object from the content.\n",
    "   - Find all the book titles on the first page. (Hint: Inspect the page to see what tag the titles are in. They are inside `<h3>` tags, within an `<a>` tag).\n",
    "   - Create a list of all the book titles and print it.\n",
    "\n",
    "1. **Scrape Book Prices:**\n",
    "\n",
    "   - On the same page, find all the book prices. (Hint: They are in `p` tags with the class `price_color`).\n",
    "   - Extract the text of the prices (e.g., \"¬£51.77\").\n",
    "   - Create a list of all the prices and print it.\n",
    "\n",
    "1. **Create a DataFrame:**\n",
    "\n",
    "   - Combine your work from the previous two exercises.\n",
    "   - Create a script that scrapes both the titles and the prices.\n",
    "   - Store the results in a Pandas DataFrame with two columns: \"Title\" and \"Price\".\n",
    "   - Print the first 5 rows of your new DataFrame using `.head()`.\n",
    "\n",
    "üéâ **Great job!** Web scraping is a powerful skill that opens up a vast new source of data for your analyses. While it can be complex, mastering the basics of `requests` and `BeautifulSoup` is a huge step forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd8a34d",
   "metadata": {},
   "source": [
    "Day 30: Web Scraping Presidents Data.\n",
    "\n",
    "This script prefers locally curated data so learners can work offline, falls back\n",
    "to a lightweight mock dataset, and only reaches out to Wikipedia when it needs to\n",
    "refresh the snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "MOCK_PRESIDENTS_DATA = [\n",
    "    {\n",
    "        \"number\": \"1\",\n",
    "        \"president\": \"George Washington\",\n",
    "        \"term_start\": \"1789-04-30\",\n",
    "        \"term_end\": \"1797-03-04\",\n",
    "        \"party\": \"Independent\",\n",
    "        \"vice_president\": \"John Adams\",\n",
    "    },\n",
    "    {\n",
    "        \"number\": \"2\",\n",
    "        \"president\": \"John Adams\",\n",
    "        \"term_start\": \"1797-03-04\",\n",
    "        \"term_end\": \"1801-03-04\",\n",
    "        \"party\": \"Federalist\",\n",
    "        \"vice_president\": \"Thomas Jefferson\",\n",
    "    },\n",
    "    {\n",
    "        \"number\": \"3\",\n",
    "        \"president\": \"Thomas Jefferson\",\n",
    "        \"term_start\": \"1801-03-04\",\n",
    "        \"term_end\": \"1809-03-04\",\n",
    "        \"party\": \"Democratic-Republican\",\n",
    "        \"vice_president\": \"Aaron Burr; George Clinton\",\n",
    "    },\n",
    "    {\n",
    "        \"number\": \"4\",\n",
    "        \"president\": \"James Madison\",\n",
    "        \"term_start\": \"1809-03-04\",\n",
    "        \"term_end\": \"1817-03-04\",\n",
    "        \"party\": \"Democratic-Republican\",\n",
    "        \"vice_president\": \"George Clinton; Elbridge Gerry\",\n",
    "    },\n",
    "    {\n",
    "        \"number\": \"5\",\n",
    "        \"president\": \"James Monroe\",\n",
    "        \"term_start\": \"1817-03-04\",\n",
    "        \"term_end\": \"1825-03-04\",\n",
    "        \"party\": \"Democratic-Republican\",\n",
    "        \"vice_president\": \"Daniel D. Tompkins\",\n",
    "    },\n",
    "    {\n",
    "        \"number\": \"6\",\n",
    "        \"president\": \"John Quincy Adams\",\n",
    "        \"term_start\": \"1825-03-04\",\n",
    "        \"term_end\": \"1829-03-04\",\n",
    "        \"party\": \"Democratic-Republican\",\n",
    "        \"vice_president\": \"John C. Calhoun\",\n",
    "    },\n",
    "    {\n",
    "        \"number\": \"7\",\n",
    "        \"president\": \"Andrew Jackson\",\n",
    "        \"term_start\": \"1829-03-04\",\n",
    "        \"term_end\": \"1837-03-04\",\n",
    "        \"party\": \"Democratic\",\n",
    "        \"vice_president\": \"John C. Calhoun; Martin Van Buren\",\n",
    "    },\n",
    "    {\n",
    "        \"number\": \"8\",\n",
    "        \"president\": \"Martin Van Buren\",\n",
    "        \"term_start\": \"1837-03-04\",\n",
    "        \"term_end\": \"1841-03-04\",\n",
    "        \"party\": \"Democratic\",\n",
    "        \"vice_president\": \"Richard Mentor Johnson\",\n",
    "    },\n",
    "    {\n",
    "        \"number\": \"9\",\n",
    "        \"president\": \"William Henry Harrison\",\n",
    "        \"term_start\": \"1841-03-04\",\n",
    "        \"term_end\": \"1841-04-04\",\n",
    "        \"party\": \"Whig\",\n",
    "        \"vice_president\": \"John Tyler\",\n",
    "    },\n",
    "    {\n",
    "        \"number\": \"10\",\n",
    "        \"president\": \"John Tyler\",\n",
    "        \"term_start\": \"1841-04-04\",\n",
    "        \"term_end\": \"1845-03-04\",\n",
    "        \"party\": \"Whig (expelled)\",\n",
    "        \"vice_president\": \"None\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def scrape_presidents_data(url: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Scrapes the list of US presidents from a Wikipedia page.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the Wikipedia page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the presidents' data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üåê Connecting to {url}...\")\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        print(\"üìä Parsing HTML tables...\")\n",
    "        # Use response.text instead of response.content for pd.read_html\n",
    "        tables = pd.read_html(response.text)\n",
    "\n",
    "        if not tables:\n",
    "            print(\"‚ùå No tables found on the page\")\n",
    "            return None\n",
    "\n",
    "        print(f\"‚úÖ Found {len(tables)} tables on the page\")\n",
    "\n",
    "        # The first table is typically the one we want\n",
    "        presidents_df = tables[0].copy()\n",
    "\n",
    "        print(f\"üìã Original data shape: {presidents_df.shape}\")\n",
    "\n",
    "        # Clean up the data\n",
    "        # Drop the last row if it appears to be a footnote (common in Wikipedia tables)\n",
    "        if len(presidents_df) > 1:\n",
    "            presidents_df = presidents_df.iloc[:-1].copy()\n",
    "\n",
    "        # Replace empty strings with NaN\n",
    "        presidents_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "        presidents_df.replace(\n",
    "            \"‚Äî\", float(\"NaN\"), inplace=True\n",
    "        )  # Common dash used in Wikipedia\n",
    "\n",
    "        # Drop columns that are all NaN\n",
    "        presidents_df.dropna(how=\"all\", axis=1, inplace=True)\n",
    "\n",
    "        print(f\"üìã Cleaned data shape: {presidents_df.shape}\")\n",
    "        print(f\"üìã Columns: {list(presidents_df.columns)}\")\n",
    "\n",
    "        return presidents_df\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"‚ùå Request timed out. The server might be slow.\")\n",
    "        return None\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Connection error. Please check your internet connection.\")\n",
    "        return None\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"‚ùå HTTP error occurred: {e}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error downloading the page: {e}\")\n",
    "        return None\n",
    "    except (IndexError, KeyError) as e:\n",
    "        print(f\"‚ùå Error parsing the table: {e}\")\n",
    "        print(\"üí° The website structure may have changed.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_csv_to_json(csv_file_path: str, json_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Converts a CSV file to a JSON file.\n",
    "\n",
    "    The JSON file will have the first column of the CSV as keys.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): The path to the input CSV file.\n",
    "        json_file_path (str): The path to the output JSON file.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    try:\n",
    "        print(f\"üìÑ Converting {csv_file_path} to {json_file_path}...\")\n",
    "        with open(csv_file_path, encoding=\"utf-8\") as csv_file:\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Check if fieldnames exist and get the first column\n",
    "            if not csv_reader.fieldnames:\n",
    "                print(\"‚ùå No column headers found in CSV file\")\n",
    "                return\n",
    "\n",
    "            key_column = csv_reader.fieldnames[0]\n",
    "            print(f\"üìä Using '{key_column}' as the key column\")\n",
    "\n",
    "            row_count = 0\n",
    "            for row in csv_reader:\n",
    "                key = row[key_column]\n",
    "                if key:  # Only add rows with non-empty keys\n",
    "                    data[key] = row\n",
    "                    row_count += 1\n",
    "\n",
    "            print(f\"‚úÖ Processed {row_count} records\")\n",
    "\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"‚úÖ Successfully converted to JSON: {json_file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: The file {csv_file_path} was not found.\")\n",
    "    except PermissionError:\n",
    "        print(\"‚ùå Error: Permission denied when accessing files.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred during conversion: {e}\")\n",
    "\n",
    "\n",
    "def save_mock_json(json_file_path: Path) -> None:\n",
    "    \"\"\"Persist the lightweight mock dataset to a JSON file.\"\"\"\n",
    "\n",
    "    print(\"üìÑ Local CSV not found. Using built-in mock dataset.\")\n",
    "    data = {item[\"number\"]: item for item in MOCK_PRESIDENTS_DATA}\n",
    "    with json_file_path.open(\"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Mock JSON file created: {json_file_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to scrape, process, and save the presidents' data.\n",
    "    \"\"\"\n",
    "    print(\"üï∏Ô∏è  Day 30: Web Scraping Presidents Data\")\n",
    "    print(\"üèõÔ∏è  Scraping US Presidents data from Wikipedia\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # URL of the Wikipedia page with the list of US presidents\n",
    "    presidents_url = (\n",
    "        \"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\"\n",
    "    )\n",
    "\n",
    "    # Define file paths relative to the script's location\n",
    "    # This makes the script more portable\n",
    "    base_dir = Path(__file__).resolve().parent\n",
    "    csv_path = base_dir / \"presidents.csv\"\n",
    "    json_path = base_dir / \"presidents.json\"\n",
    "    # The JSON outputs are ignored by git so learners can regenerate them\n",
    "    # locally without creating untracked files.\n",
    "\n",
    "    print(f\"üìÅ Output files will be saved to: {base_dir}\")\n",
    "\n",
    "    # Scrape the data\n",
    "    if csv_path.exists():\n",
    "        print(\"üìÑ Found curated CSV. Converting to JSON without scraping.\")\n",
    "        convert_csv_to_json(str(csv_path), str(json_path))\n",
    "        return\n",
    "\n",
    "    # If the curated CSV is missing, provide learners with the mock dataset first.\n",
    "    save_mock_json(json_path)\n",
    "\n",
    "    presidents_df = scrape_presidents_data(presidents_url)\n",
    "\n",
    "    if presidents_df is not None:\n",
    "        try:\n",
    "            # Save the DataFrame to a temporary CSV file, without the index\n",
    "            temp_csv_path = base_dir / \"presidents_download.csv\"\n",
    "            print(\"üíæ Saving scraped data to a temporary CSV...\")\n",
    "            presidents_df.to_csv(temp_csv_path, index=False, encoding=\"utf-8\")\n",
    "            print(f\"‚úÖ CSV file created: {temp_csv_path}\")\n",
    "\n",
    "            # Convert the CSV to JSON\n",
    "            convert_csv_to_json(str(temp_csv_path), str(json_path))\n",
    "\n",
    "            # Remove the temporary CSV file\n",
    "            try:\n",
    "                temp_csv_path.unlink()\n",
    "                print(\"üßπ Removed temporary CSV file\")\n",
    "            except OSError as e:\n",
    "                print(\n",
    "                    f\"‚ö†Ô∏è  Warning: Could not remove temporary file {temp_csv_path}: {e}\"\n",
    "                )\n",
    "\n",
    "            # Verify the JSON file was created and show some info\n",
    "            if json_path.exists():\n",
    "                file_size = json_path.stat().st_size\n",
    "                print(f\"\\nüéâ Success! Created '{json_path}' ({file_size:,} bytes)\")\n",
    "\n",
    "                # Show a preview of the data\n",
    "                try:\n",
    "                    with json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                        print(f\"üìä Total presidents in dataset: {len(data)}\")\n",
    "                        if data:\n",
    "                            first_key = next(iter(data))\n",
    "                            print(\n",
    "                                f\"üìã Sample entry keys: {list(data[first_key].keys())[:5]}...\"\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Could not preview JSON data: {e}\")\n",
    "            else:\n",
    "                print(\"‚ùå JSON file was not created successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in main processing: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to scrape presidents data.\")\n",
    "        print(\"üí° This could be due to:\")\n",
    "        print(\"   ‚Ä¢ Network connectivity issues\")\n",
    "        print(\"   ‚Ä¢ Wikipedia page structure changes\")\n",
    "        print(\"   ‚Ä¢ Rate limiting on Wikipedia\")\n",
    "        print(\"   ‚Ä¢ Server blocking the request\")\n",
    "        print(\"   ‚Ä¢ Temporary website unavailability\")\n",
    "        print(\"üì¶ Continuing with the mock dataset so you can keep practicing offline.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
