{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b63b88",
   "metadata": {},
   "source": [
    "Day 30: Web Scraping in Practice\n",
    "\n",
    "This script demonstrates the fundamentals of web scraping by\n",
    "extracting book titles and prices from a practice website.\n",
    "\n",
    "This educational example shows how to:\n",
    "- Make HTTP requests with proper headers\n",
    "- Parse HTML content with BeautifulSoup\n",
    "- Handle errors gracefully\n",
    "- Extract and clean data\n",
    "- Structure data in pandas DataFrame\n",
    "\n",
    "Author: 50 Days of Python Course\n",
    "Purpose: Educational example for MBA students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4cd825",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The URL of the website we want to scrape\n",
    "# This site is specifically designed for scraping practice.\n",
    "URL = \"http://books.toscrape.com/\"\n",
    "\n",
    "\n",
    "class ScrapingError(Exception):\n",
    "    \"\"\"Custom exception for scraping errors.\"\"\"\n",
    "\n",
    "\n",
    "def scrape_books(\n",
    "    url: str, session: Optional[requests.Session] = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scrape book data from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to scrape\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the raw scraped DataFrame, the cleaned DataFrame, and\n",
    "        a dictionary of summary statistics.\n",
    "    \"\"\"\n",
    "    # --- 1. Download the HTML Content ---\n",
    "    # Use requests.get() to download the page.\n",
    "    # It's good practice to include a 'User-Agent' header to identify your script.\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.Timeout:\n",
    "        raise\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError:\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as exc:\n",
    "        raise ScrapingError(\"Error downloading the page\") from exc\n",
    "\n",
    "    # If we get here, the request was successful\n",
    "    return process_book_data(response.content)\n",
    "\n",
    "\n",
    "def process_book_data(\n",
    "    html_content: bytes,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process the HTML response and extract book data.\n",
    "\n",
    "    Args:\n",
    "        response: The HTTP response object\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the raw scraped DataFrame, the cleaned DataFrame, and\n",
    "        a dictionary of summary statistics.\n",
    "    \"\"\"\n",
    "    # --- 2. Create a BeautifulSoup Object ---\n",
    "    # This object parses the HTML content and makes it searchable.\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # --- 3. Find and Extract Data ---\n",
    "    # We inspected the page and found that book information is within <article> tags with the class 'product_pod'\n",
    "    books = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "    if not books:\n",
    "        raise ValueError(\"No books found in the provided HTML content\")\n",
    "\n",
    "    titles = []\n",
    "    prices = []\n",
    "\n",
    "    # Loop through each book found on the page\n",
    "    for book in books:\n",
    "        # Type check to ensure book is a Tag\n",
    "        if not isinstance(book, bs4.element.Tag):\n",
    "            continue\n",
    "\n",
    "        # The title is in an 'a' tag within an 'h3' tag.\n",
    "        # We access the 'title' attribute of the 'a' tag.\n",
    "        h3_tag = book.find(\"h3\")\n",
    "        if isinstance(h3_tag, bs4.element.Tag):\n",
    "            a_tag = h3_tag.find(\"a\")\n",
    "            if isinstance(a_tag, bs4.element.Tag):\n",
    "                title = a_tag.get(\"title\")\n",
    "                titles.append(str(title) if title else \"N/A\")\n",
    "            else:\n",
    "                titles.append(\"N/A\")\n",
    "        else:\n",
    "            titles.append(\"N/A\")\n",
    "\n",
    "        # The price is in a 'p' tag with the class 'price_color'\n",
    "        price_tag = book.find(\"p\", attrs={\"class\": \"price_color\"})\n",
    "        if isinstance(price_tag, bs4.element.Tag):\n",
    "            price_text = price_tag.get_text(strip=True)\n",
    "            prices.append(price_text)\n",
    "        else:\n",
    "            prices.append(\"N/A\")\n",
    "\n",
    "    # --- 4. Structure the Data in a DataFrame ---\n",
    "    if not titles or not prices or len(titles) != len(prices):\n",
    "        raise ValueError(\"Mismatch between titles and prices in the HTML content\")\n",
    "\n",
    "    book_data = pd.DataFrame({\"Title\": titles, \"Price\": prices})\n",
    "\n",
    "    # --- 5. Data Cleaning (Bonus) ---\n",
    "    clean_data = book_data.copy()\n",
    "    clean_data[\"Price_Float\"] = pd.to_numeric(\n",
    "        clean_data[\"Price\"].str.replace(\"¬£\", \"\", regex=False), errors=\"coerce\"\n",
    "    )\n",
    "    clean_data = clean_data.dropna(subset=[\"Price_Float\"]).copy()\n",
    "\n",
    "    if clean_data.empty:\n",
    "        return book_data, clean_data, {}\n",
    "\n",
    "    # --- 6. Basic Analysis ---\n",
    "    price_series = clean_data[\"Price_Float\"]\n",
    "    analysis: Dict[str, Any] = {\n",
    "        \"average_price\": float(price_series.mean()),\n",
    "        \"min_price\": float(price_series.min()),\n",
    "        \"max_price\": float(price_series.max()),\n",
    "        \"count\": int(len(clean_data)),\n",
    "    }\n",
    "\n",
    "    most_expensive = clean_data.loc[price_series.idxmax()]\n",
    "    cheapest = clean_data.loc[price_series.idxmin()]\n",
    "\n",
    "    analysis[\"most_expensive_title\"] = most_expensive[\"Title\"]\n",
    "    analysis[\"most_expensive_price\"] = most_expensive[\"Price\"]\n",
    "    analysis[\"cheapest_title\"] = cheapest[\"Title\"]\n",
    "    analysis[\"cheapest_price\"] = cheapest[\"Price\"]\n",
    "\n",
    "    return book_data, clean_data, analysis\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate web scraping workflow.\n",
    "    \"\"\"\n",
    "    print(\"üï∏Ô∏è  Day 30: Web Scraping Demonstration\")\n",
    "    print(\"üìö Scraping book data from books.toscrape.com\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Add a small delay to be respectful to the server\n",
    "    print(\"‚è≥ Starting scraping process...\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Execute the scraping\n",
    "    try:\n",
    "        print(f\"üåê Connecting to {URL}...\")\n",
    "        raw_df, clean_df, analysis = scrape_books(URL)\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"‚ùå Request timed out. The server might be slow or unresponsive.\")\n",
    "        return\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Connection error. Please check your internet connection.\")\n",
    "        return\n",
    "    except requests.exceptions.HTTPError as exc:\n",
    "        print(f\"‚ùå HTTP error occurred: {exc}\")\n",
    "        return\n",
    "    except ScrapingError as exc:\n",
    "        print(f\"‚ùå {exc}\")\n",
    "        print(\"üí° This could be due to:\")\n",
    "        print(\"   ‚Ä¢ Network connectivity issues\")\n",
    "        print(\"   ‚Ä¢ Website being temporarily unavailable\")\n",
    "        print(\"   ‚Ä¢ Blocked by website's anti-bot protection\")\n",
    "        print(\"   ‚Ä¢ URL has changed or is incorrect\")\n",
    "        return\n",
    "    except ValueError as exc:\n",
    "        print(f\"‚ùå {exc}\")\n",
    "        print(\"üí° The website structure may have changed. Try updating the parser.\")\n",
    "        return\n",
    "\n",
    "    print(\"‚úÖ Successfully downloaded the content!\")\n",
    "    print(f\"üìä Total books scraped: {len(raw_df)}\")\n",
    "\n",
    "    if clean_df.empty:\n",
    "        print(\"‚ö†Ô∏è  No valid price data found for analysis.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Sample of Scraped Book Data ---\")\n",
    "    print(raw_df.head(10))\n",
    "\n",
    "    print(\"\\n--- Cleaned Price Data ---\")\n",
    "    print(clean_df.head(10))\n",
    "\n",
    "    print(\"\\nüìà Basic Price Analysis:\")\n",
    "    print(f\"   Average price: ¬£{analysis['average_price']:.2f}\")\n",
    "    print(f\"   Minimum price: ¬£{analysis['min_price']:.2f}\")\n",
    "    print(f\"   Maximum price: ¬£{analysis['max_price']:.2f}\")\n",
    "    print(f\"   Number of books: {analysis['count']}\")\n",
    "    print(\n",
    "        f\"üí∞ Most expensive: '{analysis['most_expensive_title']}' - {analysis['most_expensive_price']}\"\n",
    "    )\n",
    "    print(f\"üí∏ Cheapest: '{analysis['cheapest_title']}' - {analysis['cheapest_price']}\")\n",
    "\n",
    "    print(\"\\nüí° Next steps you could take:\")\n",
    "    print(\"   ‚Ä¢ Save data to CSV: clean_df.to_csv('books.csv', index=False)\")\n",
    "    print(\"   ‚Ä¢ Filter books by price range\")\n",
    "    print(\"   ‚Ä¢ Scrape additional pages for more data\")\n",
    "    print(\"   ‚Ä¢ Add more data fields (ratings, availability, etc.)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
