{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1bb7489",
   "metadata": {},
   "source": [
    "Profile the book scraping workflow used in the web scraping lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "try:\n",
    "    from mypackage.profiling import print_report, profile_callable\n",
    "except ImportError:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.append(str(PROJECT_ROOT))\n",
    "    from mypackage.profiling import print_report, profile_callable\n",
    "\n",
    "try:  # pragma: no cover - runtime guard for direct execution\n",
    "    from .web_scraping import URL, process_book_data, scrape_books\n",
    "except ImportError:  # pragma: no cover - allows ``python profile_web_scraping.py``\n",
    "    CURRENT_DIR = Path(__file__).resolve().parent\n",
    "    if str(CURRENT_DIR) not in sys.path:\n",
    "        sys.path.append(str(CURRENT_DIR))\n",
    "    from web_scraping import URL, process_book_data, scrape_books  # type: ignore\n",
    "\n",
    "\n",
    "def build_pipeline(url: str, html_path: Path | None) -> Callable[[], None]:\n",
    "    \"\"\"Return a callable that performs the scraping workflow.\"\"\"\n",
    "\n",
    "    if html_path is not None:\n",
    "        html_bytes = html_path.read_bytes()\n",
    "\n",
    "        def pipeline() -> None:\n",
    "            process_book_data(html_bytes)\n",
    "\n",
    "    else:\n",
    "\n",
    "        def pipeline() -> None:\n",
    "            scrape_books(url)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser(description=__doc__)\n",
    "    parser.add_argument(\n",
    "        \"--mode\",\n",
    "        choices=(\"cprofile\", \"timeit\"),\n",
    "        default=\"cprofile\",\n",
    "        help=\"Profiling backend to use (default: cprofile)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--url\",\n",
    "        default=URL,\n",
    "        help=\"Target URL to scrape when not using --local-html\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--local-html\",\n",
    "        type=Path,\n",
    "        help=\"Optional path to a saved HTML page for offline profiling\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--repeat\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        help=\"Number of timing repeats when --mode=timeit\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--number\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of calls per repeat when --mode=timeit\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == \"timeit\" and args.local_html is None:\n",
    "        raise SystemExit(\n",
    "            \"--mode=timeit requires --local-html to avoid repeated network calls\"\n",
    "        )\n",
    "\n",
    "    pipeline = build_pipeline(url=args.url, html_path=args.local_html)\n",
    "    profile_report, timing_report = profile_callable(\n",
    "        pipeline,\n",
    "        mode=args.mode,\n",
    "        repeat=args.repeat,\n",
    "        number=args.number,\n",
    "    )\n",
    "    print_report(profile_report=profile_report, timing_report=timing_report)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
