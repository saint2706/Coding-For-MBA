{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a53009b",
   "metadata": {},
   "source": [
    "# Day 47: Convolutional Neural Networks (CNNs) for Computer Vision\n",
    "\n",
    "Welcome to Day 47! Today, we dive into **Convolutional Neural Networks (CNNs)**, a specialized type of neural network that has revolutionized the field of **Computer Vision**.\n",
    "\n",
    "> **Prerequisites:** Ensure TensorFlow is installed with `pip install tensorflow` (CPU build by default; use TensorFlow's GPU installation guide if you have compatible hardware). Need to review package installation basics? Revisit the Day 20 Python Package Manager lesson.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What are CNNs?\n",
    "\n",
    "CNNs are designed to automatically and adaptively learn spatial hierarchies of features from images. Unlike standard neural networks, which treat inputs as flat vectors, CNNs preserve the spatial relationship between pixels.\n",
    "\n",
    "### Core Components of a CNN\n",
    "\n",
    "1. **Convolutional Layer (`Conv2D`)**\n",
    "\n",
    "   - This is the main building block of a CNN. It uses **filters** (or kernels) to slide over the input image and perform a convolution operation.\n",
    "   - This process creates **feature maps** that highlight specific patterns like edges, corners, or textures in the image. The network learns the optimal values for these filters during training.\n",
    "\n",
    "1. **Pooling Layer (`MaxPooling2D`)**\n",
    "\n",
    "   - The pooling layer is used to downsample the feature maps, reducing their spatial dimensions.\n",
    "   - This reduces the number of parameters and computation in the network, helping to control overfitting.\n",
    "   - **Max Pooling** is the most common type, where a filter slides over the feature map and takes the maximum value from each region.\n",
    "\n",
    "1. **Flatten Layer**\n",
    "\n",
    "   - After the convolutional and pooling layers have extracted features, the resulting multi-dimensional feature maps are flattened into a single one-dimensional vector.\n",
    "   - This vector is then fed into a standard fully connected neural network (like the one from Day 46) for classification.\n",
    "\n",
    "### A Typical CNN Architecture\n",
    "\n",
    "A common CNN architecture consists of a stack of `Conv2D` and `MaxPooling2D` layers, followed by one or more `Dense` layers for classification.\n",
    "\n",
    "1. **Input Image**\n",
    "1. **[Conv2D -> ReLU Activation -> MaxPooling2D]** (This block can be repeated multiple times)\n",
    "1. **Flatten Layer**\n",
    "1. **Dense Layer (with ReLU)**\n",
    "1. **Output Dense Layer (with Softmax for classification)**\n",
    "\n",
    "______________________________________________________________________\n",
    "\n",
    "## Practice Exercise\n",
    "\n",
    "- The `solutions.py` module now exports granular helpers (`prepare_mnist_data`, `build_cnn_model`, `train_cnn_model`, etc.) so you can mix and match pieces in notebooks or tests without kicking off a full five-epoch training run on import.\n",
    "- The code covers:\n",
    "  1. Loading and preprocessing the MNIST image data. (Images are normalized to be between 0 and 1).\n",
    "  1. Building a sequential CNN model with `Conv2D`, `MaxPooling2D`, `Flatten`, and `Dense` layers.\n",
    "  1. Compiling and training the CNN.\n",
    "  1. Evaluating its performance on the test set. A well-trained CNN can achieve very high accuracy on this task.\n",
    "\n",
    "### Running the example and tests\n",
    "\n",
    "- For the full demonstration run `python Day_47_Convolutional_Neural_Networks/solutions.py`. Expect a few epochs of training output plus the final metrics.\n",
    "- To verify the pipeline quickly (and without downloading the entire MNIST dataset), use the short smoke test: `pytest tests/test_day_47.py`. The test swaps in a tiny synthetic dataset and trains for a single epoch.\n",
    "- CNN training benefits from GPU acceleration. TensorFlow will automatically use your GPU if the drivers and CUDA/cuDNN stack are configured; otherwise the CPU-only run will simply take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85037428",
   "metadata": {},
   "source": [
    "Reusable helpers for building and training a small CNN on MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac634eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int = DEFAULT_SEED) -> None:\n",
    "    \"\"\"Synchronise NumPy and TensorFlow RNGs for deterministic runs.\"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "\n",
    "def prepare_mnist_data(\n",
    "    normalize: bool = True,\n",
    ") -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Load MNIST images, optionally normalise pixels, and add a channel axis.\"\"\"\n",
    "\n",
    "    (train_images, train_labels), (test_images, test_labels) = (\n",
    "        datasets.mnist.load_data()\n",
    "    )\n",
    "\n",
    "    train_images = train_images.astype(\"float32\")\n",
    "    test_images = test_images.astype(\"float32\")\n",
    "\n",
    "    if normalize:\n",
    "        train_images /= 255.0\n",
    "        test_images /= 255.0\n",
    "\n",
    "    train_images = train_images[..., tf.newaxis]\n",
    "    test_images = test_images[..., tf.newaxis]\n",
    "\n",
    "    return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "\n",
    "def build_cnn_model(\n",
    "    input_shape: Tuple[int, int, int] = (28, 28, 1),\n",
    "    num_classes: int = 10,\n",
    "    conv_filters: Tuple[int, int, int] = (32, 64, 64),\n",
    "    dense_units: int = 64,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"Create an MNIST classifier mirroring the tutorial architecture.\"\"\"\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    for index, filters in enumerate(conv_filters):\n",
    "        model.add(layers.Conv2D(filters, (3, 3), activation=\"relu\"))\n",
    "        if index < len(conv_filters) - 1:\n",
    "            model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_units, activation=\"relu\"))\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_cnn_model(\n",
    "    model: tf.keras.Model,\n",
    "    optimizer: str = \"adam\",\n",
    "    loss: str = \"sparse_categorical_crossentropy\",\n",
    "    metrics: Tuple[str, ...] = (\"accuracy\",),\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"Compile the CNN with sensible defaults for classification.\"\"\"\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=list(metrics))\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_cnn_model(\n",
    "    model: tf.keras.Model,\n",
    "    train_images: np.ndarray,\n",
    "    train_labels: np.ndarray,\n",
    "    *,\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 64,\n",
    "    validation_data: Tuple[np.ndarray, np.ndarray] | None = None,\n",
    "    validation_split: float = 0.0,\n",
    "    verbose: int = 1,\n",
    "    shuffle: bool = True,\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \"\"\"Fit the CNN and return the training history.\"\"\"\n",
    "\n",
    "    history = model.fit(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=validation_data,\n",
    "        validation_split=validation_split,\n",
    "        verbose=verbose,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_cnn_model(\n",
    "    model: tf.keras.Model,\n",
    "    test_images: np.ndarray,\n",
    "    test_labels: np.ndarray,\n",
    "    *,\n",
    "    verbose: int = 2,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate the trained CNN on the test split.\"\"\"\n",
    "\n",
    "    return model.evaluate(test_images, test_labels, verbose=verbose, return_dict=True)\n",
    "\n",
    "\n",
    "def run_full_workflow(\n",
    "    *,\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 64,\n",
    "    verbose: int = 1,\n",
    "    seed: int = DEFAULT_SEED,\n",
    ") -> Tuple[tf.keras.callbacks.History, Dict[str, float], tf.keras.Model]:\n",
    "    \"\"\"Train and evaluate the CNN end-to-end, returning the artifacts.\"\"\"\n",
    "\n",
    "    set_global_seed(seed)\n",
    "    (train_images, train_labels), (test_images, test_labels) = prepare_mnist_data()\n",
    "    model = build_cnn_model(input_shape=train_images.shape[1:])\n",
    "    compile_cnn_model(model)\n",
    "    history = train_cnn_model(\n",
    "        model,\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test_images, test_labels),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    metrics = evaluate_cnn_model(model, test_images, test_labels, verbose=verbose)\n",
    "    return history, metrics, model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    history, metrics, model = run_full_workflow()\n",
    "\n",
    "    print(\"--- CNN for MNIST Classification ---\")\n",
    "    model.summary()\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Final training accuracy:\", history.history[\"accuracy\"][-1])\n",
    "    print(\"Test metrics:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
