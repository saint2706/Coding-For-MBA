{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a78370",
   "metadata": {},
   "source": [
    "Utility functions for training a simple neural network on the Iris dataset.\n",
    "\n",
    "The original tutorial for Day 46 walked through the full workflow of preparing\n",
    "data, building a model, fitting it, and evaluating the results using\n",
    "print statements.  This refactor exposes each major step as a reusable\n",
    "function so that the workflow can be unit tested and reused from other\n",
    "scripts without executing expensive training at import time.\n",
    "\n",
    "The helper functions are intentionally lightweight: callers control the\n",
    "number of training epochs, verbosity, and whether validation data is used.\n",
    "Each function returns rich objects (e.g. `History` instances or metric\n",
    "dictionaries) so tests can assert on their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IrisData:\n",
    "    \"\"\"Container for the Iris dataset splits and fitted preprocessors.\"\"\"\n",
    "\n",
    "    X_train: np.ndarray\n",
    "    X_test: np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test: np.ndarray\n",
    "    scaler: StandardScaler\n",
    "    encoder: OneHotEncoder\n",
    "    target_names: Iterable[str]\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int = DEFAULT_SEED) -> None:\n",
    "    \"\"\"Ensure reproducible results across NumPy and TensorFlow.\"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "\n",
    "def prepare_iris_data(\n",
    "    test_size: float = 0.2, random_state: int = DEFAULT_SEED\n",
    ") -> IrisData:\n",
    "    \"\"\"Load, scale, and encode the Iris dataset.\n",
    "\n",
    "    Args:\n",
    "        test_size: Fraction of samples to allocate to the test split.\n",
    "        random_state: Deterministic seed used for the train/test split.\n",
    "\n",
    "    Returns:\n",
    "        An :class:`IrisData` instance containing the dataset splits along with\n",
    "        the fitted preprocessing objects.\n",
    "    \"\"\"\n",
    "\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # ``sparse_output`` is available in newer scikit-learn releases; fall back\n",
    "    # to the legacy ``sparse`` flag when running on older versions.\n",
    "    try:\n",
    "        encoder = OneHotEncoder(sparse_output=False)  # type: ignore[arg-type]\n",
    "    except TypeError:  # pragma: no cover - executed on older scikit-learn\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "    y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_onehot, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return IrisData(\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        scaler=scaler,\n",
    "        encoder=encoder,\n",
    "        target_names=iris.target_names,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_iris_model(\n",
    "    input_shape: int, num_classes: int, hidden_units: Tuple[int, ...] = (10, 10)\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"Create and compile the neural network used for Iris classification.\"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(input_shape,)))\n",
    "    for units in hidden_units:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_iris_model(\n",
    "    model: tf.keras.Model,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    *,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 8,\n",
    "    validation_split: float = 0.2,\n",
    "    verbose: int = 0,\n",
    "    shuffle: bool = True,\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \"\"\"Fit the neural network and return the resulting history object.\"\"\"\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=verbose,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_iris_model(\n",
    "    model: tf.keras.Model, X_test: np.ndarray, y_test: np.ndarray, *, verbose: int = 0\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate the trained model and return the metrics as a dictionary.\"\"\"\n",
    "\n",
    "    return model.evaluate(X_test, y_test, verbose=verbose, return_dict=True)\n",
    "\n",
    "\n",
    "def run_full_workflow(\n",
    "    *,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 8,\n",
    "    validation_split: float = 0.2,\n",
    "    verbose: int = 0,\n",
    "    seed: int = DEFAULT_SEED,\n",
    ") -> Tuple[tf.keras.callbacks.History, Dict[str, float], tf.keras.Model, IrisData]:\n",
    "    \"\"\"Execute the end-to-end Iris training workflow.\n",
    "\n",
    "    This helper is convenient for interactive exploration while keeping the\n",
    "    individual steps separately testable.\n",
    "    \"\"\"\n",
    "\n",
    "    set_global_seed(seed)\n",
    "    data = prepare_iris_data(random_state=seed)\n",
    "    model = build_iris_model(data.X_train.shape[1], data.y_train.shape[1])\n",
    "    history = train_iris_model(\n",
    "        model,\n",
    "        data.X_train,\n",
    "        data.y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=verbose,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    metrics = evaluate_iris_model(model, data.X_test, data.y_test, verbose=verbose)\n",
    "    return history, metrics, model, data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    history, metrics, model, data = run_full_workflow(verbose=1)\n",
    "\n",
    "    print(\"--- Neural Network for Iris Classification ---\")\n",
    "    print(f\"Training set shape: {data.X_train.shape}\")\n",
    "    print(f\"One-hot encoded target shape: {data.y_train.shape}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"Model Summary:\")\n",
    "    model.summary()\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\n",
    "        \"Training complete. Final validation accuracy:\"\n",
    "        f\" {history.history['val_accuracy'][-1]:.3f}\"\n",
    "    )\n",
    "    print(\"Test metrics:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Demonstrate an example prediction using the fitted scaler.\n",
    "    sample = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
    "    sample_scaled = data.scaler.transform(sample)\n",
    "    prediction = model.predict(sample_scaled)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "    print(\"Prediction for a new sample:\")\n",
    "    print(f\"Probabilities: {prediction[0]}\")\n",
    "    print(f\"Predicted class index: {predicted_class}\")\n",
    "    print(f\"Predicted class name: {data.target_names[predicted_class]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
