{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535d28d6",
   "metadata": {},
   "source": [
    "# Day 46: Introduction to Neural Networks & Frameworks\n",
    "\n",
    "Welcome to Day 46! Today, we begin our exploration of **Deep Learning** by introducing the fundamental building block: the **Artificial Neural Network (ANN)**. We'll also discuss the major frameworks used to build these powerful models.\n",
    "\n",
    "> **Prerequisites:** Install TensorFlow with `pip install tensorflow` to run today's neural-network examples (this installs the CPU build; GPU acceleration requires following TensorFlow's CUDA setup). You'll also need scikit-learn for data prep with `pip install scikit-learn`. Need a refresher on using `pip`? Revisit the Day 20 Python Package Manager lesson.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What is a Neural Network?\n",
    "\n",
    "An Artificial Neural Network is a computational model inspired by the structure and function of the human brain. It consists of interconnected nodes, called **neurons**, organized in layers.\n",
    "\n",
    "### Structure of a Neural Network\n",
    "\n",
    "1. **Input Layer:** Receives the initial data or features. The number of neurons in this layer corresponds to the number of features in the dataset.\n",
    "\n",
    "1. **Hidden Layers:** These are the intermediate layers between the input and output layers. A neural network can have zero or more hidden layers. A \"deep\" neural network has multiple hidden layers.\n",
    "\n",
    "   - Each neuron in a hidden layer receives inputs from the previous layer, applies a mathematical operation (a weighted sum followed by an **activation function**), and passes the result to the next layer.\n",
    "\n",
    "1. **Output Layer:** Produces the final result.\n",
    "\n",
    "   - For a **regression** task, it might have a single neuron with a linear activation.\n",
    "   - For a **classification** task, it might have multiple neurons (one for each class) with a softmax activation function to output probabilities.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. Without them, a neural network would just be a linear regression model.\n",
    "\n",
    "- **Common Examples:**\n",
    "  - **ReLU (Rectified Linear Unit):** `f(x) = max(0, x)`. The most widely used activation function.\n",
    "  - **Sigmoid:** `f(x) = 1 / (1 + e^(-x))`. Used in the output layer for binary classification.\n",
    "  - **Softmax:** Generalizes the sigmoid function for multi-class classification.\n",
    "\n",
    "### How Neural Networks Learn\n",
    "\n",
    "They learn through a process called **backpropagation** and **gradient descent**.\n",
    "\n",
    "1. The network makes a prediction (forward pass).\n",
    "1. The prediction error is calculated using a **loss function**.\n",
    "1. The backpropagation algorithm calculates the gradient of the loss function with respect to the network's weights.\n",
    "1. The weights are updated using gradient descent to minimize the error.\n",
    "\n",
    "## Deep Learning Frameworks\n",
    "\n",
    "Building neural networks from scratch is complex. We use specialized libraries to handle the heavy lifting.\n",
    "\n",
    "- **TensorFlow:** Developed by Google, it's a powerful and flexible ecosystem for machine learning. It's known for its production-readiness and scalability.\n",
    "- **PyTorch:** Developed by Facebook's AI Research lab, it's known for its simplicity, ease of use, and Pythonic nature, making it a favorite in the research community.\n",
    "\n",
    "______________________________________________________________________\n",
    "\n",
    "## Practice Exercise\n",
    "\n",
    "- The `solutions.py` module now exposes dedicated helper functions for each step of the workflow (data preparation, model construction, training, and evaluation). Import the pieces you need instead of running everything on import.\n",
    "- The code covers:\n",
    "  1. Loading and preparing the Iris dataset.\n",
    "  1. Building a sequential model with dense (fully connected) layers.\n",
    "  1. Compiling the model (specifying the optimizer, loss function, and metrics).\n",
    "  1. Training the model.\n",
    "  1. Evaluating its performance and making predictions.\n",
    "\n",
    "### Running the example and tests\n",
    "\n",
    "- Execute the full walkthrough from the command line with `python Day_46_Intro_to_Neural_Networks/solutions.py`. The script prints the data shapes, model summary, and evaluation metrics.\n",
    "- Need a quick confidence check without the full 50-epoch run? The automated test performs a single, deterministic epoch: `pytest tests/test_day_46.py`.\n",
    "- Training on GPU hardware is optional for this small dataset, but TensorFlow will automatically leverage your GPU if it's available and correctly configured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e95b0",
   "metadata": {},
   "source": [
    "Utility functions for training a simple neural network on the Iris dataset.\n",
    "\n",
    "The original tutorial for Day 46 walked through the full workflow of preparing\n",
    "data, building a model, fitting it, and evaluating the results using\n",
    "print statements.  This refactor exposes each major step as a reusable\n",
    "function so that the workflow can be unit tested and reused from other\n",
    "scripts without executing expensive training at import time.\n",
    "\n",
    "The helper functions are intentionally lightweight: callers control the\n",
    "number of training epochs, verbosity, and whether validation data is used.\n",
    "Each function returns rich objects (e.g. `History` instances or metric\n",
    "dictionaries) so tests can assert on their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e27dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IrisData:\n",
    "    \"\"\"Container for the Iris dataset splits and fitted preprocessors.\"\"\"\n",
    "\n",
    "    X_train: np.ndarray\n",
    "    X_test: np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test: np.ndarray\n",
    "    scaler: StandardScaler\n",
    "    encoder: OneHotEncoder\n",
    "    target_names: Iterable[str]\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int = DEFAULT_SEED) -> None:\n",
    "    \"\"\"Ensure reproducible results across NumPy and TensorFlow.\"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "\n",
    "def prepare_iris_data(\n",
    "    test_size: float = 0.2, random_state: int = DEFAULT_SEED\n",
    ") -> IrisData:\n",
    "    \"\"\"Load, scale, and encode the Iris dataset.\n",
    "\n",
    "    Args:\n",
    "        test_size: Fraction of samples to allocate to the test split.\n",
    "        random_state: Deterministic seed used for the train/test split.\n",
    "\n",
    "    Returns:\n",
    "        An :class:`IrisData` instance containing the dataset splits along with\n",
    "        the fitted preprocessing objects.\n",
    "    \"\"\"\n",
    "\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # ``sparse_output`` is available in newer scikit-learn releases; fall back\n",
    "    # to the legacy ``sparse`` flag when running on older versions.\n",
    "    try:\n",
    "        encoder = OneHotEncoder(sparse_output=False)  # type: ignore[arg-type]\n",
    "    except TypeError:  # pragma: no cover - executed on older scikit-learn\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "    y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_onehot, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return IrisData(\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        scaler=scaler,\n",
    "        encoder=encoder,\n",
    "        target_names=iris.target_names,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_iris_model(\n",
    "    input_shape: int, num_classes: int, hidden_units: Tuple[int, ...] = (10, 10)\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"Create and compile the neural network used for Iris classification.\"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(input_shape,)))\n",
    "    for units in hidden_units:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_iris_model(\n",
    "    model: tf.keras.Model,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    *,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 8,\n",
    "    validation_split: float = 0.2,\n",
    "    verbose: int = 0,\n",
    "    shuffle: bool = True,\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \"\"\"Fit the neural network and return the resulting history object.\"\"\"\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=verbose,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_iris_model(\n",
    "    model: tf.keras.Model, X_test: np.ndarray, y_test: np.ndarray, *, verbose: int = 0\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate the trained model and return the metrics as a dictionary.\"\"\"\n",
    "\n",
    "    return model.evaluate(X_test, y_test, verbose=verbose, return_dict=True)\n",
    "\n",
    "\n",
    "def run_full_workflow(\n",
    "    *,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 8,\n",
    "    validation_split: float = 0.2,\n",
    "    verbose: int = 0,\n",
    "    seed: int = DEFAULT_SEED,\n",
    ") -> Tuple[tf.keras.callbacks.History, Dict[str, float], tf.keras.Model, IrisData]:\n",
    "    \"\"\"Execute the end-to-end Iris training workflow.\n",
    "\n",
    "    This helper is convenient for interactive exploration while keeping the\n",
    "    individual steps separately testable.\n",
    "    \"\"\"\n",
    "\n",
    "    set_global_seed(seed)\n",
    "    data = prepare_iris_data(random_state=seed)\n",
    "    model = build_iris_model(data.X_train.shape[1], data.y_train.shape[1])\n",
    "    history = train_iris_model(\n",
    "        model,\n",
    "        data.X_train,\n",
    "        data.y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=verbose,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    metrics = evaluate_iris_model(model, data.X_test, data.y_test, verbose=verbose)\n",
    "    return history, metrics, model, data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    history, metrics, model, data = run_full_workflow(verbose=1)\n",
    "\n",
    "    print(\"--- Neural Network for Iris Classification ---\")\n",
    "    print(f\"Training set shape: {data.X_train.shape}\")\n",
    "    print(f\"One-hot encoded target shape: {data.y_train.shape}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"Model Summary:\")\n",
    "    model.summary()\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\n",
    "        \"Training complete. Final validation accuracy:\"\n",
    "        f\" {history.history['val_accuracy'][-1]:.3f}\"\n",
    "    )\n",
    "    print(\"Test metrics:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Demonstrate an example prediction using the fitted scaler.\n",
    "    sample = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
    "    sample_scaled = data.scaler.transform(sample)\n",
    "    prediction = model.predict(sample_scaled)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "    print(\"Prediction for a new sample:\")\n",
    "    print(f\"Probabilities: {prediction[0]}\")\n",
    "    print(f\"Predicted class index: {predicted_class}\")\n",
    "    print(f\"Predicted class name: {data.target_names[predicted_class]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
