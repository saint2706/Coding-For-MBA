# Day 54 â€“ Probabilistic Modeling

Gaussian mixtures, Bayesian classifiers, expectation-maximisation, and hidden Markov models power
probabilistic reasoning for ambiguous business signals. Use the notebook or `solutions.py` helpers to:

- Simulate multi-modal customer cohorts and recover their structure with Gaussian mixtures.
- Train Gaussian Naive Bayes classifiers that expose posterior log-probabilities for fast decision rules.
- Run expectation-maximisation loops to maximise mixture log-likelihoods on noisy, partially labelled data.
- Implement a numerically stable hidden Markov forward pass to evaluate sequence likelihoods under state
  transitions and Gaussian emissions.

Execute `python Day_54_Probabilistic_Modeling/solutions.py` to print representative log-likelihood outputs
for the reproducible toy datasets.
