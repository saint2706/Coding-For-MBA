{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec7208c",
   "metadata": {},
   "source": [
    "# Day 54 â€“ Probabilistic Modeling\n",
    "\n",
    "Gaussian mixtures, Bayesian classifiers, expectation-maximisation, and hidden Markov models power\n",
    "probabilistic reasoning for ambiguous business signals. Use the notebook or `solutions.py` helpers to:\n",
    "\n",
    "- Simulate multi-modal customer cohorts and recover their structure with Gaussian mixtures.\n",
    "- Train Gaussian Naive Bayes classifiers that expose posterior log-probabilities for fast decision rules.\n",
    "- Run expectation-maximisation loops to maximise mixture log-likelihoods on noisy, partially labelled data.\n",
    "- Implement a numerically stable hidden Markov forward pass to evaluate sequence likelihoods under state\n",
    "  transitions and Gaussian emissions.\n",
    "\n",
    "Execute `python Day_54_Probabilistic_Modeling/solutions.py` to print representative log-likelihood outputs\n",
    "for the reproducible toy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba65d7a5",
   "metadata": {},
   "source": [
    "Reusable probabilistic modelling utilities for Day 54."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f552393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HiddenMarkovModel:\n",
    "    \"\"\"Container for Gaussian-emission HMM parameters.\"\"\"\n",
    "\n",
    "    transition: NDArray[np.float64]\n",
    "    startprob: NDArray[np.float64]\n",
    "    means: NDArray[np.float64]\n",
    "    covariances: NDArray[np.float64]\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.transition.shape[0] != self.transition.shape[1]:\n",
    "            msg = \"Transition matrix must be square.\"\n",
    "            raise ValueError(msg)\n",
    "        if not np.allclose(self.transition.sum(axis=1), 1.0):\n",
    "            msg = \"Rows of the transition matrix must sum to 1.\"\n",
    "            raise ValueError(msg)\n",
    "        if not np.isclose(self.startprob.sum(), 1.0):\n",
    "            msg = \"Start probabilities must sum to 1.\"\n",
    "            raise ValueError(msg)\n",
    "        if len(self.means) != self.transition.shape[0]:\n",
    "            msg = \"Means must match number of hidden states.\"\n",
    "            raise ValueError(msg)\n",
    "        if self.covariances.shape[0] != self.transition.shape[0]:\n",
    "            msg = \"Covariances must match number of hidden states.\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "\n",
    "def generate_probabilistic_dataset(\n",
    "    n_samples: int = 400,\n",
    "    random_state: int = 54,\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.int_]]:\n",
    "    \"\"\"Create a reproducible Gaussian mixture dataset with component labels.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    weights = np.array([0.55, 0.45])\n",
    "    means = np.array([[0.0, 0.0], [3.5, 2.8]])\n",
    "    covariances = np.array(\n",
    "        [\n",
    "            [[0.8, 0.2], [0.2, 0.6]],\n",
    "            [[0.5, -0.15], [-0.15, 0.7]],\n",
    "        ]\n",
    "    )\n",
    "    assignments = rng.choice(len(weights), size=n_samples, p=weights)\n",
    "    observations = np.vstack(\n",
    "        [\n",
    "            rng.multivariate_normal(\n",
    "                means[idx], covariances[idx], size=(assignments == idx).sum()\n",
    "            )\n",
    "            for idx in range(len(weights))\n",
    "        ]\n",
    "    )\n",
    "    labels = np.concatenate(\n",
    "        [\n",
    "            np.full((assignments == idx).sum(), idx, dtype=int)\n",
    "            for idx in range(len(weights))\n",
    "        ]\n",
    "    )\n",
    "    ordering = rng.permutation(n_samples)\n",
    "    return observations[ordering], labels[ordering]\n",
    "\n",
    "\n",
    "def fit_gaussian_mixture(\n",
    "    X: ArrayLike,\n",
    "    n_components: int = 2,\n",
    "    covariance_type: str = \"full\",\n",
    "    random_state: int = 54,\n",
    "    max_iter: int = 300,\n",
    ") -> GaussianMixture:\n",
    "    \"\"\"Fit a Gaussian mixture model using expectation-maximisation.\"\"\"\n",
    "\n",
    "    model = GaussianMixture(\n",
    "        n_components=n_components,\n",
    "        covariance_type=covariance_type,\n",
    "        random_state=random_state,\n",
    "        max_iter=max_iter,\n",
    "        tol=1e-4,\n",
    "        reg_covar=1e-6,\n",
    "    )\n",
    "    model.fit(np.asarray(X))\n",
    "    return model\n",
    "\n",
    "\n",
    "def mixture_log_likelihood(model: GaussianMixture, X: ArrayLike) -> float:\n",
    "    \"\"\"Return the total data log-likelihood under the fitted mixture.\"\"\"\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    return float(np.sum(model.score_samples(X)))\n",
    "\n",
    "\n",
    "def run_expectation_maximisation(\n",
    "    X: ArrayLike,\n",
    "    n_components: int = 2,\n",
    "    random_state: int = 54,\n",
    "    max_iter: int = 300,\n",
    ") -> Tuple[GaussianMixture, float]:\n",
    "    \"\"\"Fit a Gaussian mixture and return the average log-likelihood bound.\"\"\"\n",
    "\n",
    "    model = fit_gaussian_mixture(\n",
    "        X,\n",
    "        n_components=n_components,\n",
    "        random_state=random_state,\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "    return model, float(model.lower_bound_)\n",
    "\n",
    "\n",
    "def train_bayesian_classifier(X: ArrayLike, y: ArrayLike) -> GaussianNB:\n",
    "    \"\"\"Train a Gaussian Naive Bayes classifier.\"\"\"\n",
    "\n",
    "    model = GaussianNB()\n",
    "    model.fit(np.asarray(X), np.asarray(y))\n",
    "    return model\n",
    "\n",
    "\n",
    "def bayesian_log_posterior(model: GaussianNB, X: ArrayLike) -> NDArray[np.float64]:\n",
    "    \"\"\"Return class log-posterior probabilities for the given observations.\"\"\"\n",
    "\n",
    "    return np.asarray(model.predict_log_proba(np.asarray(X)))\n",
    "\n",
    "\n",
    "def _gaussian_log_pdf(\n",
    "    x: NDArray[np.float64], mean: NDArray[np.float64], cov: NDArray[np.float64]\n",
    ") -> float:\n",
    "    \"\"\"Log probability density of a multivariate normal distribution.\"\"\"\n",
    "\n",
    "    x = np.atleast_1d(x)\n",
    "    mean = np.atleast_1d(mean)\n",
    "    cov = np.asarray(cov)\n",
    "    diff = x - mean\n",
    "    sign, logdet = np.linalg.slogdet(cov)\n",
    "    if sign <= 0:\n",
    "        msg = \"Covariance matrix must be positive definite.\"\n",
    "        raise ValueError(msg)\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    dim = mean.size\n",
    "    exponent = float(diff.T @ inv_cov @ diff)\n",
    "    return -0.5 * (dim * np.log(2.0 * np.pi) + logdet + exponent)\n",
    "\n",
    "\n",
    "def _logsumexp(\n",
    "    arr: NDArray[np.float64], axis: int | None = None\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"Compute log-sum-exp in a numerically stable fashion.\"\"\"\n",
    "\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    max_val = np.max(arr, axis=axis, keepdims=True)\n",
    "    stabilized = np.exp(arr - max_val)\n",
    "    summed = np.sum(stabilized, axis=axis, keepdims=True)\n",
    "    result = max_val + np.log(summed)\n",
    "    if axis is None:\n",
    "        return np.squeeze(result)\n",
    "    return np.squeeze(result, axis=axis)\n",
    "\n",
    "\n",
    "def hmm_log_likelihood(model: HiddenMarkovModel, observations: ArrayLike) -> float:\n",
    "    \"\"\"Compute the log-likelihood of observations under a Gaussian HMM.\"\"\"\n",
    "\n",
    "    obs = np.asarray(observations, dtype=float)\n",
    "    n_states = model.transition.shape[0]\n",
    "    n_obs = obs.shape[0]\n",
    "    emission_log_probs = np.empty((n_obs, n_states))\n",
    "    for state in range(n_states):\n",
    "        emission_log_probs[:, state] = [\n",
    "            _gaussian_log_pdf(point, model.means[state], model.covariances[state])\n",
    "            for point in obs\n",
    "        ]\n",
    "\n",
    "    log_alpha = np.log(model.startprob) + emission_log_probs[0]\n",
    "    for t in range(1, n_obs):\n",
    "        log_alpha = (\n",
    "            _logsumexp(log_alpha[:, np.newaxis] + np.log(model.transition), axis=0)\n",
    "            + emission_log_probs[t]\n",
    "        )\n",
    "    return float(_logsumexp(log_alpha))\n",
    "\n",
    "\n",
    "def build_demo_hmm(random_state: int = 54) -> HiddenMarkovModel:\n",
    "    \"\"\"Return a small two-state Gaussian HMM for demonstrations.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    transition = np.array([[0.85, 0.15], [0.2, 0.8]])\n",
    "    startprob = np.array([0.6, 0.4])\n",
    "    means = np.array([[0.0], [3.0]])\n",
    "    covariances = np.array([[[0.5]], [[0.7]]])\n",
    "    _ = rng  # Reserved for future extensions; keeps signature consistent.\n",
    "    return HiddenMarkovModel(\n",
    "        transition=transition, startprob=startprob, means=means, covariances=covariances\n",
    "    )\n",
    "\n",
    "\n",
    "def demo_log_likelihoods() -> dict[str, float]:\n",
    "    \"\"\"Train baseline models and return key log-likelihood metrics.\"\"\"\n",
    "\n",
    "    X, labels = generate_probabilistic_dataset()\n",
    "    gmm = fit_gaussian_mixture(X)\n",
    "    total_log_like = mixture_log_likelihood(gmm, X)\n",
    "\n",
    "    bayes = train_bayesian_classifier(X, labels)\n",
    "    avg_bayes_log_like = float(\n",
    "        np.mean(bayesian_log_posterior(bayes, X)[np.arange(len(labels)), labels])\n",
    "    )\n",
    "\n",
    "    hmm = build_demo_hmm()\n",
    "    demo_sequence = np.array([[0.2], [-0.1], [2.8], [3.4], [2.5]])\n",
    "    hmm_log_like = hmm_log_likelihood(hmm, demo_sequence)\n",
    "\n",
    "    return {\n",
    "        \"gmm_log_likelihood\": total_log_like,\n",
    "        \"bayes_average_log_posterior\": avg_bayes_log_like,\n",
    "        \"hmm_log_likelihood\": hmm_log_like,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    metrics = demo_log_likelihoods()\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name}: {value:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
