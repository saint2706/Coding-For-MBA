{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23cddd5d",
   "metadata": {},
   "source": [
    "# Day 64 â€“ Modern NLP Pipelines\n",
    "\n",
    "Connect discrete NLP components into a reproducible workflow. After this lesson you will:\n",
    "\n",
    "- Tokenize text with configurable normalization options.\n",
    "- Build deterministic embedding tables for rapid experimentation.\n",
    "- Fine-tune a lightweight transformer-style classifier head on sentence labels.\n",
    "- Retrieve support passages and perform retrieval-augmented generation (RAG).\n",
    "- Evaluate generations with deterministic exact-match and token-overlap metrics.\n",
    "\n",
    "Run `python Day_64_Modern_NLP_Pipelines/solutions.py` to explore end-to-end text processing with seeded toy corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf06a25",
   "metadata": {},
   "source": [
    "Modern NLP pipeline utilities for Day 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Mapping, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "TokenizedCorpus = List[List[str]]\n",
    "\n",
    "\n",
    "def tokenize_corpus(\n",
    "    corpus: Sequence[str],\n",
    "    *,\n",
    "    lowercase: bool = True,\n",
    "    strip_punctuation: bool = True,\n",
    ") -> TokenizedCorpus:\n",
    "    \"\"\"Tokenize strings with deterministic options.\"\"\"\n",
    "\n",
    "    table = str.maketrans({c: \" \" for c in \"!,.?;:\"}) if strip_punctuation else None\n",
    "    tokenized: TokenizedCorpus = []\n",
    "    for doc in corpus:\n",
    "        text = doc.translate(table) if table is not None else doc\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        tokens = [token for token in text.split() if token]\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def build_embedding_table(\n",
    "    tokens: TokenizedCorpus, embedding_dim: int = 8\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Create deterministic embeddings via hashing.\"\"\"\n",
    "\n",
    "    vocab = sorted({token for doc in tokens for token in doc})\n",
    "    table: Dict[str, np.ndarray] = {}\n",
    "    for token in vocab:\n",
    "        digest = hashlib.sha256(token.encode(\"utf-8\")).digest()\n",
    "        seed = int.from_bytes(digest[:4], \"little\")\n",
    "        rng = np.random.default_rng(seed)\n",
    "        table[token] = rng.normal(0, 1, size=embedding_dim)\n",
    "    return table\n",
    "\n",
    "\n",
    "def document_embeddings(\n",
    "    tokens: TokenizedCorpus, embeddings: Mapping[str, np.ndarray]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Average token embeddings for each document.\"\"\"\n",
    "\n",
    "    doc_vectors: List[np.ndarray] = []\n",
    "    for doc in tokens:\n",
    "        if doc:\n",
    "            vecs = [embeddings[token] for token in doc]\n",
    "            doc_vectors.append(np.mean(vecs, axis=0))\n",
    "        else:\n",
    "            doc_vectors.append(\n",
    "                np.zeros(next(iter(embeddings.values())).shape, dtype=float)\n",
    "            )\n",
    "    return np.vstack(doc_vectors)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MiniTransformer:\n",
    "    \"\"\"Lightweight classifier head operating on document embeddings.\"\"\"\n",
    "\n",
    "    weights: np.ndarray\n",
    "    bias: float\n",
    "\n",
    "    def predict_proba(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        logits = embeddings @ self.weights + self.bias\n",
    "        return 1.0 / (1.0 + np.exp(-logits))\n",
    "\n",
    "    def predict(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        return (self.predict_proba(embeddings) >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FineTuneHistory:\n",
    "    \"\"\"Record of loss values during fine-tuning.\"\"\"\n",
    "\n",
    "    losses: List[float]\n",
    "\n",
    "\n",
    "def fine_tune_transformer(\n",
    "    embeddings: np.ndarray,\n",
    "    labels: Sequence[int],\n",
    "    epochs: int = 200,\n",
    "    lr: float = 0.1,\n",
    ") -> Tuple[MiniTransformer, FineTuneHistory]:\n",
    "    \"\"\"Train a logistic head on top of frozen document embeddings.\"\"\"\n",
    "\n",
    "    y = np.asarray(labels, dtype=float)\n",
    "    weights = np.zeros(embeddings.shape[1], dtype=float)\n",
    "    bias = 0.0\n",
    "    losses: List[float] = []\n",
    "    for _ in range(epochs):\n",
    "        logits = embeddings @ weights + bias\n",
    "        probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "        loss = -np.mean(y * np.log(probs + 1e-12) + (1 - y) * np.log(1 - probs + 1e-12))\n",
    "        losses.append(float(loss))\n",
    "        gradient_w = embeddings.T @ (probs - y) / len(y)\n",
    "        gradient_b = np.mean(probs - y)\n",
    "        weights -= lr * gradient_w\n",
    "        bias -= lr * gradient_b\n",
    "    model = MiniTransformer(weights=weights, bias=bias)\n",
    "    return model, FineTuneHistory(losses=losses)\n",
    "\n",
    "\n",
    "def retrieve_documents(\n",
    "    query_embedding: np.ndarray,\n",
    "    doc_embeddings: np.ndarray,\n",
    "    top_k: int = 1,\n",
    ") -> List[int]:\n",
    "    \"\"\"Return indices of nearest documents by cosine similarity.\"\"\"\n",
    "\n",
    "    similarities = (\n",
    "        doc_embeddings\n",
    "        @ query_embedding\n",
    "        / (\n",
    "            np.linalg.norm(doc_embeddings, axis=1)\n",
    "            * (np.linalg.norm(query_embedding) + 1e-12)\n",
    "        )\n",
    "    )\n",
    "    ranked = np.argsort(similarities)[::-1]\n",
    "    return ranked[:top_k].tolist()\n",
    "\n",
    "\n",
    "def rag_generate(\n",
    "    query: str,\n",
    "    corpus: Sequence[str],\n",
    "    doc_embeddings: np.ndarray,\n",
    "    embeddings_table: Mapping[str, np.ndarray],\n",
    "    top_k: int = 1,\n",
    ") -> str:\n",
    "    \"\"\"Perform retrieval-augmented generation by echoing top documents.\"\"\"\n",
    "\n",
    "    tokenized_query = tokenize_corpus([query])[0]\n",
    "    if tokenized_query:\n",
    "        query_vec = np.mean(\n",
    "            [embeddings_table[token] for token in tokenized_query], axis=0\n",
    "        )\n",
    "    else:\n",
    "        query_vec = np.zeros(next(iter(embeddings_table.values())).shape, dtype=float)\n",
    "    doc_indices = retrieve_documents(query_vec, doc_embeddings, top_k=top_k)\n",
    "    retrieved = [corpus[idx] for idx in doc_indices]\n",
    "    return \" \\n\".join(\n",
    "        [f\"Answer: {retrieved[0] if retrieved else ''}\", \"Sources:\"] + retrieved\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_generation(reference: str, prediction: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute deterministic exact-match and token-overlap metrics.\"\"\"\n",
    "\n",
    "    ref_tokens = tokenize_corpus([reference])[0]\n",
    "    pred_tokens = tokenize_corpus([prediction])[0]\n",
    "    exact = float(reference.strip().lower() == prediction.strip().lower())\n",
    "    overlap = len(set(ref_tokens) & set(pred_tokens)) / (len(set(ref_tokens)) + 1e-12)\n",
    "    recall = len(set(ref_tokens) & set(pred_tokens)) / (len(set(ref_tokens)) + 1e-12)\n",
    "    precision = len(set(ref_tokens) & set(pred_tokens)) / (\n",
    "        len(set(pred_tokens)) + 1e-12\n",
    "    )\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    return {\n",
    "        \"exact_match\": exact,\n",
    "        \"overlap\": overlap,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_pipeline(corpus: Sequence[str], labels: Sequence[int]) -> Dict[str, object]:\n",
    "    \"\"\"Utility for documentation walkthroughs.\"\"\"\n",
    "\n",
    "    tokens = tokenize_corpus(corpus)\n",
    "    embedding_table = build_embedding_table(tokens)\n",
    "    doc_vecs = document_embeddings(tokens, embedding_table)\n",
    "    model, history = fine_tune_transformer(doc_vecs, labels)\n",
    "    rag_answer = rag_generate(\"summary\", corpus, doc_vecs, embedding_table, top_k=2)\n",
    "    metrics = evaluate_generation(corpus[0], rag_answer)\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"embeddings\": embedding_table,\n",
    "        \"doc_vectors\": doc_vecs,\n",
    "        \"model\": model,\n",
    "        \"history\": history,\n",
    "        \"rag_answer\": rag_answer,\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = [\n",
    "        \"Transformers capture long-range dependencies with self-attention.\",\n",
    "        \"Retrieval augmented generation grounds answers in documents.\",\n",
    "        \"Tokenization and embeddings define the vocabulary space.\",\n",
    "    ]\n",
    "    labels = [1, 1, 0]\n",
    "    pipeline = build_pipeline(corpus, labels)\n",
    "    print(\"Loss trajectory (first 5):\", pipeline[\"history\"].losses[:5])\n",
    "    print(\"RAG output:\\n\", pipeline[\"rag_answer\"])\n",
    "    print(\"Metrics:\", pipeline[\"metrics\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
