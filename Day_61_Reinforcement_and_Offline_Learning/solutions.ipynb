{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df7ef2c",
   "metadata": {},
   "source": [
    "# Day 61 – Reinforcement and Offline Learning\n",
    "\n",
    "Reinforcement learning (RL) balances exploration and exploitation while offline evaluation keeps policies safe. After this lesson you can:\n",
    "\n",
    "- Compare value-based, policy-based, and actor–critic methods across episodic control problems.\n",
    "- Simulate contextual bandits and conservative offline policy evaluation with replay buffers and importance sampling.\n",
    "- Analyse stability tricks: entropy bonuses, target networks, batch-constrained Q-learning, and doubly robust estimators.\n",
    "- Reproduce seeded experiments that converge to expected reward thresholds for regression tests.\n",
    "\n",
    "Execute `python Day_61_Reinforcement_and_Offline_Learning/solutions.py` to walk through deterministic policy optimisation, offline evaluation diagnostics, and bandit baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de81548",
   "metadata": {},
   "source": [
    "Reinforcement learning utilities for Day 61."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd7e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EpisodeLog:\n",
    "    \"\"\"Track rewards and moving averages for RL experiments.\"\"\"\n",
    "\n",
    "    rewards: List[float]\n",
    "    moving_average: List[float]\n",
    "    policy_parameter: float\n",
    "\n",
    "\n",
    "def _sigmoid(x: float) -> float:\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def run_policy_gradient_bandit(\n",
    "    episodes: int = 200,\n",
    "    lr: float = 0.2,\n",
    "    random_state: int = 61,\n",
    ") -> EpisodeLog:\n",
    "    \"\"\"Train a REINFORCE-style policy on a two-armed bandit.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    theta = 0.0\n",
    "    baseline = 0.0\n",
    "    rewards: List[float] = []\n",
    "    moving_avg: List[float] = []\n",
    "    for episode in range(episodes):\n",
    "        prob_action_one = _sigmoid(theta)\n",
    "        action = 1 if rng.random() < prob_action_one else 0\n",
    "        reward = float(rng.normal(1.2, 0.05) if action == 1 else rng.normal(0.2, 0.05))\n",
    "        rewards.append(reward)\n",
    "        baseline = 0.9 * baseline + 0.1 * reward\n",
    "        grad = (reward - baseline) * (action - prob_action_one)\n",
    "        theta += lr * grad\n",
    "        moving_avg.append(float(np.mean(rewards[max(0, episode - 19) : episode + 1])))\n",
    "    return EpisodeLog(\n",
    "        rewards=rewards, moving_average=moving_avg, policy_parameter=float(theta)\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QLearningResult:\n",
    "    \"\"\"Container for Q-learning progress on a deterministic MDP.\"\"\"\n",
    "\n",
    "    q_values: np.ndarray\n",
    "    rewards: List[float]\n",
    "\n",
    "\n",
    "def run_q_learning(\n",
    "    episodes: int = 200,\n",
    "    gamma: float = 0.9,\n",
    "    lr: float = 0.3,\n",
    "    epsilon: float = 0.2,\n",
    "    random_state: int = 61,\n",
    ") -> QLearningResult:\n",
    "    \"\"\"Run tabular Q-learning on a two-state MDP.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    q_values = np.zeros((2, 2))\n",
    "    rewards: List[float] = []\n",
    "    transition = {\n",
    "        (0, 0): (0, 0.5),\n",
    "        (0, 1): (1, 1.0),\n",
    "        (1, 0): (0, 0.4),\n",
    "        (1, 1): (1, 1.2),\n",
    "    }\n",
    "    state = 0\n",
    "    for _ in range(episodes):\n",
    "        if rng.random() < epsilon:\n",
    "            action = rng.integers(0, 2)\n",
    "        else:\n",
    "            action = int(np.argmax(q_values[state]))\n",
    "        next_state, reward = transition[(state, action)]\n",
    "        rewards.append(reward)\n",
    "        best_next = np.max(q_values[next_state])\n",
    "        td_target = reward + gamma * best_next\n",
    "        td_error = td_target - q_values[state, action]\n",
    "        q_values[state, action] += lr * td_error\n",
    "        state = next_state\n",
    "    return QLearningResult(q_values=q_values, rewards=rewards)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BanditSummary:\n",
    "    \"\"\"Summary statistics for epsilon-greedy contextual bandit.\"\"\"\n",
    "\n",
    "    action_counts: np.ndarray\n",
    "    cumulative_reward: float\n",
    "    average_reward: float\n",
    "\n",
    "\n",
    "def run_contextual_bandit(\n",
    "    steps: int = 300,\n",
    "    epsilon: float = 0.1,\n",
    "    random_state: int = 61,\n",
    ") -> BanditSummary:\n",
    "    \"\"\"Execute epsilon-greedy strategy on a contextual bandit.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    action_values = np.zeros(3)\n",
    "    action_counts = np.zeros(3, dtype=int)\n",
    "    reward_means = np.array([0.3, 0.8, 1.1])\n",
    "    total_reward = 0.0\n",
    "    for step in range(steps):\n",
    "        if rng.random() < epsilon:\n",
    "            action = rng.integers(0, 3)\n",
    "        else:\n",
    "            action = int(np.argmax(action_values))\n",
    "        reward = float(rng.normal(reward_means[action], 0.05))\n",
    "        action_counts[action] += 1\n",
    "        total_reward += reward\n",
    "        step_size = 1.0 / action_counts[action]\n",
    "        action_values[action] += step_size * (reward - action_values[action])\n",
    "    return BanditSummary(\n",
    "        action_counts=action_counts,\n",
    "        cumulative_reward=total_reward,\n",
    "        average_reward=total_reward / steps,\n",
    "    )\n",
    "\n",
    "\n",
    "def offline_evaluation(\n",
    "    num_samples: int = 500,\n",
    "    random_state: int = 61,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Estimate evaluation policy performance with weighted importance sampling.\"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    reward_means = np.array([0.2, 0.5, 1.0])\n",
    "    behaviour_policy = np.array([0.5, 0.4, 0.1])\n",
    "    evaluation_policy = np.array([0.1, 0.2, 0.7])\n",
    "    weights: List[float] = []\n",
    "    weighted_rewards: List[float] = []\n",
    "    for _ in range(num_samples):\n",
    "        action = rng.choice(3, p=behaviour_policy)\n",
    "        reward = float(rng.normal(reward_means[action], 0.1))\n",
    "        importance = evaluation_policy[action] / behaviour_policy[action]\n",
    "        weights.append(importance)\n",
    "        weighted_rewards.append(importance * reward)\n",
    "    weights_arr = np.array(weights)\n",
    "    weighted_rewards_arr = np.array(weighted_rewards)\n",
    "    estimate = float(weighted_rewards_arr.sum() / (weights_arr.sum() + 1e-9))\n",
    "    ess = float((weights_arr.sum() ** 2) / (np.sum(weights_arr**2) + 1e-9))\n",
    "    return {\"estimate\": estimate, \"effective_sample_size\": ess}\n",
    "\n",
    "\n",
    "def run_rl_suite(random_state: int = 61) -> Dict[str, object]:\n",
    "    \"\"\"Run policy/value/bandit/offline learning experiments and aggregate metrics.\"\"\"\n",
    "\n",
    "    pg = run_policy_gradient_bandit(random_state=random_state)\n",
    "    ql = run_q_learning(random_state=random_state)\n",
    "    bandit = run_contextual_bandit(random_state=random_state)\n",
    "    offline = offline_evaluation(random_state=random_state)\n",
    "    return {\n",
    "        \"policy_gradient\": pg,\n",
    "        \"q_learning\": ql,\n",
    "        \"bandit\": bandit,\n",
    "        \"offline\": offline,\n",
    "    }\n",
    "\n",
    "\n",
    "def _demo() -> None:\n",
    "    results = run_rl_suite()\n",
    "    print(\n",
    "        f\"Policy gradient final avg reward: {results['policy_gradient'].moving_average[-1]:.3f}\"\n",
    "    )\n",
    "    print(f\"Q-learning Q-values: {results['q_learning'].q_values}\")\n",
    "    print(f\"Bandit average reward: {results['bandit'].average_reward:.3f}\")\n",
    "    print(\n",
    "        f\"Offline estimate: {results['offline']['estimate']:.3f} (ESS={results['offline']['effective_sample_size']:.1f})\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
