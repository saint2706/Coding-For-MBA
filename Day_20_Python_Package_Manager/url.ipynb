{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baf384f",
   "metadata": {},
   "source": [
    "# üìò Day 20: Python Package Manager (pip) & Third-Party Libraries\n",
    "\n",
    "The real power of Python for data analysis comes from its vast ecosystem of third-party libraries. These are pre-written modules, created by the community, that provide powerful tools for specific tasks.\n",
    "\n",
    "- **`requests`**: For making HTTP requests to APIs and websites.\n",
    "- **`numpy`**: For numerical computing and advanced math.\n",
    "- **`pandas`**: For data manipulation and analysis (we'll dive deep into this soon).\n",
    "- **`beautifulsoup4`**: For web scraping.\n",
    "\n",
    "## `pip` and `requirements.txt`\n",
    "\n",
    "- **`pip`** is the standard tool for installing these packages. You use it in your terminal (e.g., `pip install requests`).\n",
    "- A **`requirements.txt`** file is the standard way to list all the third-party packages your project depends on. This allows anyone (including you, on a different computer) to install all the necessary libraries at once using a single command: `pip install -r requirements.txt`.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "This project now has a `requirements.txt` file at its root. Before you begin, ensure you have followed the setup instructions in the main [README.md](../../README.md) to create a virtual environment and run `pip install -r requirements.txt`.\n",
    "\n",
    "## Exploring the Refactored Code\n",
    "\n",
    "The script for this lesson, `url.py`, demonstrates how to use `requests` to fetch data from live APIs and `numpy` to perform calculations. The code has been refactored to separate the data fetching logic from the data analysis logic, which is a crucial best practice.\n",
    "\n",
    "1. **Review the Code:** Open `Day_20_Python_Package_Manager/url.py`. Notice the `fetch_api_data()` function, which is responsible for the network request, and the `analyze_...` functions, which take data as input and perform calculations.\n",
    "1. **Run the Script:** From the root directory of the project (`Coding-For-MBA`), run the script. It will make live calls to external APIs and print the analysis.\n",
    "   ```bash\n",
    "   python Day_20_Python_Package_Manager/url.py\n",
    "   ```\n",
    "1. **Run the Tests:** The tests for this lesson demonstrate a key testing technique: **mocking**. We replace the live network call with a \"mock\" object that returns sample data. This makes our tests fast, reliable, and independent of the network.\n",
    "   ```bash\n",
    "   pytest tests/test_day_20.py\n",
    "   ```\n",
    "\n",
    "## üíª Exercises: Day 20\n",
    "\n",
    "1. **Explore the `requests` Response:**\n",
    "\n",
    "   - In a new script (`my_solutions_20.py`), import the `requests` library.\n",
    "   - Make a `get` request to `\"https://api.thecatapi.com/v1/breeds\"`.\n",
    "   - The object returned by `requests.get()` is a `Response` object. Print its `status_code` attribute (e.g., `response.status_code`) and its `headers` attribute.\n",
    "\n",
    "1. **Analyze Different Metrics:**\n",
    "\n",
    "   - Import the `fetch_api_data` and `analyze_breed_metrics` functions from the lesson script.\n",
    "   - Fetch the cat breed data.\n",
    "   - The `analyze_breed_metrics` function can analyze both `'weight'` and `'life_span'`. Call it for `'life_span'` and print the result.\n",
    "\n",
    "1. **Find the Top 3 Origins:**\n",
    "\n",
    "   - Import and use the `analyze_breed_origins` function, but modify the call so it returns the top 3 most common origins instead of the default 5.\n",
    "\n",
    "üéâ **Congratulations!** You now understand how to leverage the vast Python ecosystem using `pip`. This skill unlocks a world of powerful tools for data analysis, machine learning, web development, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f020b4",
   "metadata": {},
   "source": [
    "Day 20: Python Package Manager - Working with Third-Party Libraries (Refactored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Optional import for web scraping\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    BS4_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BS4_AVAILABLE = False\n",
    "\n",
    "# --- Data Fetching Functions ---\n",
    "\n",
    "\n",
    "def fetch_api_data(url: str) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"Fetches and parses JSON data from a given API endpoint.\"\"\"\n",
    "    try:\n",
    "        print(f\"üìö Fetching data from {url}...\")\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "        print(\"‚úÖ Data downloaded successfully!\")\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching data: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ùå Error: Failed to decode JSON response.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Data Analysis Functions ---\n",
    "\n",
    "\n",
    "def analyze_text_frequency(text: str, top_n: int = 5) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Analyzes text to find the most common words.\"\"\"\n",
    "    words = re.findall(r\"\\b[a-z]+\\b\", text.lower())\n",
    "    return Counter(words).most_common(top_n)\n",
    "\n",
    "\n",
    "def parse_metric_range(metric_str: str) -> float:\n",
    "    \"\"\"Parses a string like '3 - 7' and returns the average.\"\"\"\n",
    "    try:\n",
    "        parts = [float(p.strip()) for p in metric_str.split(\"-\")]\n",
    "        return sum(parts) / len(parts)\n",
    "    except (ValueError, IndexError):\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def analyze_breed_metrics(\n",
    "    breeds_data: List[Dict[str, Any]], metric: str, unit: str\n",
    ") -> Optional[Dict[str, float]]:\n",
    "    \"\"\"Analyzes a specific metric (e.g., 'weight', 'life_span') from breed data.\"\"\"\n",
    "    values = []\n",
    "    for breed in breeds_data:\n",
    "        if metric == \"weight\" and \"weight\" in breed and \"metric\" in breed[\"weight\"]:\n",
    "            avg_value = parse_metric_range(breed[\"weight\"][\"metric\"])\n",
    "        elif metric == \"life_span\" and \"life_span\" in breed:\n",
    "            avg_value = parse_metric_range(breed[\"life_span\"])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if avg_value > 0:\n",
    "            values.append(avg_value)\n",
    "\n",
    "    if not values:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"unit\": unit,\n",
    "        \"mean\": np.mean(values),\n",
    "        \"median\": np.median(values),\n",
    "        \"std_dev\": np.std(values),\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_breed_origins(\n",
    "    breeds_data: List[Dict[str, Any]], top_n: int = 5\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"Analyzes the geographic distribution of cat breed origins.\"\"\"\n",
    "    origins = [\n",
    "        breed[\"origin\"]\n",
    "        for breed in breeds_data\n",
    "        if \"origin\" in breed and breed[\"origin\"]\n",
    "    ]\n",
    "    return Counter(origins).most_common(top_n)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate package manager capabilities.\"\"\"\n",
    "    print(\"üöÄ Day 20: Python Package Manager Demo\\n\")\n",
    "\n",
    "    # 1. Analyze Cat Breed Data from TheCatAPI\n",
    "    print(\"--- Analyzing Cat Breed Data ---\")\n",
    "    cat_breeds = fetch_api_data(\"https://api.thecatapi.com/v1/breeds\")\n",
    "    if cat_breeds:\n",
    "        weight_stats = analyze_breed_metrics(cat_breeds, \"weight\", \"kg\")\n",
    "        if weight_stats:\n",
    "            print(\n",
    "                f\"Average Cat Weight: {weight_stats['mean']:.2f} {weight_stats['unit']}\"\n",
    "            )\n",
    "\n",
    "        lifespan_stats = analyze_breed_metrics(cat_breeds, \"life_span\", \"years\")\n",
    "        if lifespan_stats:\n",
    "            print(\n",
    "                f\"Average Cat Lifespan: {lifespan_stats['mean']:.2f} {lifespan_stats['unit']}\"\n",
    "            )\n",
    "\n",
    "        top_origins = analyze_breed_origins(cat_breeds)\n",
    "        print(\"Top 5 Breed Origins:\", top_origins)\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 2. Web Scraping Demonstration (Optional)\n",
    "    if BS4_AVAILABLE:\n",
    "        print(\"\\n--- Web Scraping Demonstration ---\")\n",
    "        try:\n",
    "            response = requests.get(\"https://httpbin.org/html\", timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            title = soup.find(\"h1\")\n",
    "            print(f\"Scraped Page Title: {title.get_text() if title else 'Not Found'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Web scraping demo failed: {e}\")\n",
    "    else:\n",
    "        print(\"\\n--- Web Scraping Demonstration ---\")\n",
    "        print(\"BeautifulSoup4 not installed. Skipping.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Demo complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
