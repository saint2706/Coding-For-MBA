{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e80327",
   "metadata": {},
   "source": [
    "# Day 58 – Transformers and Attention\n",
    "\n",
    "Transformers dominate modern sequence modelling. This lesson demonstrates how to:\n",
    "\n",
    "- Assemble encoder–decoder stacks with multi-head self-attention, cross-attention, and position-wise feed-forward layers.\n",
    "- Fine-tune pretrained checkpoints (Hugging Face style) with layer-freezing schedules, discriminative learning rates, and LoRA adapters.\n",
    "- Visualise token-to-token attention patterns to interpret model focus during inference.\n",
    "- Deploy a deterministic tiny transformer classifier for reproducible experiments on compact datasets.\n",
    "\n",
    "Run `python Day_58_Transformers_and_Attention/solutions.py` to simulate encoder–decoder passes, generate fine-tuning playbooks, and score demo texts with attention heatmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6f0d4",
   "metadata": {},
   "source": [
    "Transformer helpers and deterministic classifier for Day 58."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d54e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    \"\"\"Configuration for the tiny encoder–decoder demonstration.\"\"\"\n",
    "\n",
    "    vocab_size: int = 16\n",
    "    d_model: int = 8\n",
    "    num_heads: int = 2\n",
    "    ff_dim: int = 16\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EncoderDecoderStates:\n",
    "    \"\"\"Container capturing intermediate encoder/decoder representations.\"\"\"\n",
    "\n",
    "    encoder_hidden: np.ndarray\n",
    "    decoder_hidden: np.ndarray\n",
    "    cross_attention: np.ndarray\n",
    "\n",
    "\n",
    "DEFAULT_VOCAB: Tuple[str, ...] = (\n",
    "    \"<pad>\",\n",
    "    \"<unk>\",\n",
    "    \"great\",\n",
    "    \"bad\",\n",
    "    \"product\",\n",
    "    \"service\",\n",
    "    \"love\",\n",
    "    \"hate\",\n",
    "    \"fast\",\n",
    "    \"slow\",\n",
    "    \"support\",\n",
    "    \"terrible\",\n",
    "    \"amazing\",\n",
    "    \"not\",\n",
    "    \"boring\",\n",
    "    \"exciting\",\n",
    ")\n",
    "DEFAULT_LABELS: Tuple[str, ...] = (\"negative\", \"positive\")\n",
    "\n",
    "\n",
    "class TinyTransformerClassifier:\n",
    "    \"\"\"Deterministic self-attention classifier for miniature datasets.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Sequence[str] | None = None,\n",
    "        labels: Sequence[str] | None = None,\n",
    "        config: TransformerConfig | None = None,\n",
    "        random_state: int = 58,\n",
    "    ) -> None:\n",
    "        self.config = config or TransformerConfig(vocab_size=len(DEFAULT_VOCAB))\n",
    "        self.vocab_tokens = tuple(vocab) if vocab is not None else DEFAULT_VOCAB\n",
    "        self.labels = tuple(labels) if labels is not None else DEFAULT_LABELS\n",
    "        self.token_to_id: Dict[str, int] = {\n",
    "            token: idx for idx, token in enumerate(self.vocab_tokens)\n",
    "        }\n",
    "        if \"<unk>\" not in self.token_to_id:\n",
    "            self.token_to_id[\"<unk>\"] = len(self.token_to_id)\n",
    "        rng = np.random.default_rng(random_state)\n",
    "\n",
    "        vocab_size = len(self.token_to_id)\n",
    "        d_model = self.config.d_model\n",
    "        ff_dim = self.config.ff_dim\n",
    "        self.embed = rng.normal(0.0, 0.2, size=(vocab_size, d_model))\n",
    "        self.W_q = rng.normal(0.0, 0.3, size=(d_model, d_model))\n",
    "        self.W_k = rng.normal(0.0, 0.3, size=(d_model, d_model))\n",
    "        self.W_v = rng.normal(0.0, 0.3, size=(d_model, d_model))\n",
    "        self.W_o = rng.normal(0.0, 0.2, size=(d_model, d_model))\n",
    "        self.ff_w1 = rng.normal(0.0, 0.2, size=(d_model, ff_dim))\n",
    "        self.ff_b1 = rng.normal(0.0, 0.1, size=(ff_dim,))\n",
    "        self.ff_w2 = rng.normal(0.0, 0.2, size=(ff_dim, d_model))\n",
    "        self.ff_b2 = rng.normal(0.0, 0.05, size=(d_model,))\n",
    "        self.classifier_w = rng.normal(0.0, 0.4, size=(d_model, len(self.labels)))\n",
    "        self.classifier_b = rng.normal(0.0, 0.1, size=(len(self.labels),))\n",
    "        sentiment = {\n",
    "            \"great\": 1.1,\n",
    "            \"amazing\": 1.0,\n",
    "            \"love\": 1.2,\n",
    "            \"fast\": 0.6,\n",
    "            \"support\": 0.5,\n",
    "            \"bad\": -1.0,\n",
    "            \"terrible\": -1.3,\n",
    "            \"hate\": -1.2,\n",
    "            \"slow\": -0.8,\n",
    "            \"boring\": -0.7,\n",
    "            \"not\": -0.4,\n",
    "            \"service\": -0.1,\n",
    "        }\n",
    "        self.sentiment_vector = np.zeros(vocab_size)\n",
    "        for token, weight in sentiment.items():\n",
    "            idx = self.token_to_id.get(token)\n",
    "            if idx is not None:\n",
    "                self.sentiment_vector[idx] = weight\n",
    "        self.lexicon_scale = 0.6\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Tokenisation utilities\n",
    "    # ------------------------------------------------------------------\n",
    "    def tokenize(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert raw text into token ids.\"\"\"\n",
    "\n",
    "        tokens = text.lower().replace(\"!\", \" \").replace(\"?\", \" \").split()\n",
    "        unk_id = self.token_to_id[\"<unk>\"]\n",
    "        return [self.token_to_id.get(token, unk_id) for token in tokens]\n",
    "\n",
    "    def pad(self, token_ids: Sequence[int], length: int) -> np.ndarray:\n",
    "        \"\"\"Pad or truncate token ids to the provided length.\"\"\"\n",
    "\n",
    "        pad_id = self.token_to_id.get(\"<pad>\", 0)\n",
    "        output = np.full(length, pad_id, dtype=int)\n",
    "        seq = np.asarray(token_ids[:length], dtype=int)\n",
    "        output[: seq.size] = seq\n",
    "        return output\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Transformer block\n",
    "    # ------------------------------------------------------------------\n",
    "    def _reshape_for_heads(self, array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Reshape (seq, d_model) into (num_heads, seq, head_dim).\"\"\"\n",
    "\n",
    "        seq_len, d_model = array.shape\n",
    "        head_dim = d_model // self.config.num_heads\n",
    "        reshaped = array.reshape(seq_len, self.config.num_heads, head_dim)\n",
    "        return np.transpose(reshaped, (1, 0, 2))\n",
    "\n",
    "    def _combine_heads(self, array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Combine (num_heads, seq, head_dim) into (seq, d_model).\"\"\"\n",
    "\n",
    "        num_heads, seq_len, head_dim = array.shape\n",
    "        combined = np.transpose(array, (1, 0, 2)).reshape(seq_len, num_heads * head_dim)\n",
    "        return combined\n",
    "\n",
    "    def _scaled_dot_product(\n",
    "        self, q: np.ndarray, k: np.ndarray, v: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute scaled dot-product attention for a single head.\"\"\"\n",
    "\n",
    "        scale = np.sqrt(q.shape[-1]).astype(float)\n",
    "        scores = (q @ k.T) / scale\n",
    "        scores -= scores.max(axis=-1, keepdims=True)\n",
    "        weights = np.exp(scores)\n",
    "        weights /= weights.sum(axis=-1, keepdims=True)\n",
    "        attended = weights @ v\n",
    "        return attended, weights\n",
    "\n",
    "    def _self_attention(self, embeddings: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply multi-head self-attention to the token embeddings.\"\"\"\n",
    "\n",
    "        query = embeddings @ self.W_q\n",
    "        key = embeddings @ self.W_k\n",
    "        value = embeddings @ self.W_v\n",
    "\n",
    "        q_heads = self._reshape_for_heads(query)\n",
    "        k_heads = self._reshape_for_heads(key)\n",
    "        v_heads = self._reshape_for_heads(value)\n",
    "\n",
    "        outputs = []\n",
    "        attn_scores = []\n",
    "        for head in range(self.config.num_heads):\n",
    "            attended, weights = self._scaled_dot_product(\n",
    "                q_heads[head], k_heads[head], v_heads[head]\n",
    "            )\n",
    "            outputs.append(attended)\n",
    "            attn_scores.append(weights)\n",
    "        concat = self._combine_heads(np.stack(outputs, axis=0))\n",
    "        attn_matrix = np.stack(attn_scores, axis=0)\n",
    "        transformed = concat @ self.W_o\n",
    "        return transformed, attn_matrix\n",
    "\n",
    "    def _feed_forward(self, tensor: np.ndarray) -> np.ndarray:\n",
    "        hidden = np.maximum(0.0, tensor @ self.ff_w1 + self.ff_b1)\n",
    "        return hidden @ self.ff_w2 + self.ff_b2\n",
    "\n",
    "    def forward(self, token_ids: Sequence[int]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Run the transformer block and return logits and attention.\"\"\"\n",
    "\n",
    "        if not token_ids:\n",
    "            token_ids = [self.token_to_id.get(\"<pad>\", 0)]\n",
    "        ids = np.asarray(token_ids, dtype=int)\n",
    "        embeddings = self.embed[ids]\n",
    "        attn_output, attn_weights = self._self_attention(embeddings)\n",
    "        transformed = self._feed_forward(attn_output)\n",
    "        pooled = transformed.mean(axis=0)\n",
    "        logits = pooled @ self.classifier_w + self.classifier_b\n",
    "        lexicon_boost = float(np.sum(self.sentiment_vector[ids]))\n",
    "        if logits.shape[0] >= 2:\n",
    "            logits = logits.copy()\n",
    "            logits[0] -= self.lexicon_scale * lexicon_boost\n",
    "            logits[1] += self.lexicon_scale * lexicon_boost\n",
    "        return logits, attn_weights\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Public API\n",
    "    # ------------------------------------------------------------------\n",
    "    def predict_proba(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Return class probabilities for the given text.\"\"\"\n",
    "\n",
    "        token_ids = self.tokenize(text)\n",
    "        logits, _ = self.forward(token_ids)\n",
    "        logits = logits - logits.max()\n",
    "        probs = np.exp(logits)\n",
    "        probs /= probs.sum()\n",
    "        return {label: float(prob) for label, prob in zip(self.labels, probs)}\n",
    "\n",
    "    def classify(self, text: str) -> str:\n",
    "        \"\"\"Return the most likely label for a text sequence.\"\"\"\n",
    "\n",
    "        probs = self.predict_proba(text)\n",
    "        return max(probs, key=probs.get)\n",
    "\n",
    "    def attention_heatmap(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Return average attention weights across heads for inspection.\"\"\"\n",
    "\n",
    "        token_ids = self.tokenize(text)\n",
    "        _, attn = self.forward(token_ids)\n",
    "        return attn.mean(axis=0)\n",
    "\n",
    "\n",
    "def build_encoder_decoder_stack(\n",
    "    source_tokens: Sequence[int],\n",
    "    target_tokens: Sequence[int],\n",
    "    config: TransformerConfig | None = None,\n",
    "    random_state: int = 58,\n",
    ") -> EncoderDecoderStates:\n",
    "    \"\"\"Run a compact encoder–decoder simulation and return states.\"\"\"\n",
    "\n",
    "    cfg = config or TransformerConfig()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    d_model = cfg.d_model\n",
    "    num_heads = cfg.num_heads\n",
    "\n",
    "    def multi_head(\n",
    "        x: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        seq_len = x.shape[0]\n",
    "        q = x @ W_q\n",
    "        k = x @ W_k\n",
    "        v = x @ W_v\n",
    "        head_dim = d_model // num_heads\n",
    "        q = q.reshape(seq_len, num_heads, head_dim).transpose(1, 0, 2)\n",
    "        k = k.reshape(seq_len, num_heads, head_dim).transpose(1, 0, 2)\n",
    "        v = v.reshape(seq_len, num_heads, head_dim).transpose(1, 0, 2)\n",
    "        outputs = []\n",
    "        for head in range(num_heads):\n",
    "            scale = np.sqrt(head_dim)\n",
    "            weights = (q[head] @ k[head].T) / scale\n",
    "            weights -= weights.max(axis=-1, keepdims=True)\n",
    "            prob = np.exp(weights)\n",
    "            prob /= prob.sum(axis=-1, keepdims=True)\n",
    "            outputs.append(prob @ v[head])\n",
    "        concat = np.stack(outputs, axis=1).reshape(seq_len, d_model)\n",
    "        return concat\n",
    "\n",
    "    vocab_size = cfg.vocab_size\n",
    "    encoder_embed = rng.normal(0.0, 0.4, size=(vocab_size, d_model))\n",
    "    decoder_embed = rng.normal(0.0, 0.4, size=(vocab_size, d_model))\n",
    "\n",
    "    encoder_inp = encoder_embed[np.asarray(source_tokens, dtype=int)]\n",
    "    decoder_inp = decoder_embed[np.asarray(target_tokens, dtype=int)]\n",
    "\n",
    "    W_q = rng.normal(0.0, 0.3, size=(d_model, d_model))\n",
    "    W_k = rng.normal(0.0, 0.3, size=(d_model, d_model))\n",
    "    W_v = rng.normal(0.0, 0.3, size=(d_model, d_model))\n",
    "    encoder_hidden = multi_head(encoder_inp, W_q, W_k, W_v)\n",
    "\n",
    "    cross_W_q = rng.normal(0.0, 0.3, size=(d_model, d_model))\n",
    "    cross_W_k = rng.normal(0.0, 0.3, size=(d_model, d_model))\n",
    "    cross_W_v = rng.normal(0.0, 0.3, size=(d_model, d_model))\n",
    "\n",
    "    decoder_self = multi_head(decoder_inp, W_q, W_k, W_v)\n",
    "    seq_len_t = decoder_inp.shape[0]\n",
    "    q = (\n",
    "        (decoder_self @ cross_W_q)\n",
    "        .reshape(seq_len_t, num_heads, d_model // num_heads)\n",
    "        .transpose(1, 0, 2)\n",
    "    )\n",
    "    k = (\n",
    "        (encoder_hidden @ cross_W_k)\n",
    "        .reshape(encoder_hidden.shape[0], num_heads, d_model // num_heads)\n",
    "        .transpose(1, 0, 2)\n",
    "    )\n",
    "    v = (\n",
    "        (encoder_hidden @ cross_W_v)\n",
    "        .reshape(encoder_hidden.shape[0], num_heads, d_model // num_heads)\n",
    "        .transpose(1, 0, 2)\n",
    "    )\n",
    "\n",
    "    cross_outputs = []\n",
    "    for head in range(num_heads):\n",
    "        scale = np.sqrt(d_model // num_heads)\n",
    "        weights = (q[head] @ k[head].T) / scale\n",
    "        weights -= weights.max(axis=-1, keepdims=True)\n",
    "        prob = np.exp(weights)\n",
    "        prob /= prob.sum(axis=-1, keepdims=True)\n",
    "        cross_outputs.append(prob @ v[head])\n",
    "    cross_attention = np.stack(cross_outputs, axis=0)\n",
    "    decoder_hidden = cross_attention.transpose(1, 0, 2).reshape(seq_len_t, d_model)\n",
    "\n",
    "    return EncoderDecoderStates(\n",
    "        encoder_hidden=encoder_hidden,\n",
    "        decoder_hidden=decoder_hidden,\n",
    "        cross_attention=cross_attention,\n",
    "    )\n",
    "\n",
    "\n",
    "def fine_tuning_playbook(\n",
    "    base_model: str = \"distilbert-base-uncased\",\n",
    "    lr: float = 2e-5,\n",
    "    weight_decay: float = 0.01,\n",
    "    epochs: int = 3,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"Return a Hugging Face style fine-tuning recipe for documentation.\"\"\"\n",
    "\n",
    "    schedule = [\n",
    "        {\n",
    "            \"phase\": 1,\n",
    "            \"frozen_layers\": \"embeddings+encoder[:2]\",\n",
    "            \"learning_rate\": lr / 10,\n",
    "        },\n",
    "        {\"phase\": 2, \"frozen_layers\": \"encoder[:1]\", \"learning_rate\": lr},\n",
    "        {\"phase\": 3, \"adapter\": \"LoRA rank=4\", \"learning_rate\": lr * 1.5},\n",
    "    ]\n",
    "    return {\n",
    "        \"model\": base_model,\n",
    "        \"epochs\": epochs,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"discriminative_lrs\": schedule,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def demo_attention_visualisation(\n",
    "    text: str, classifier: TinyTransformerClassifier | None = None\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"Return attention weights and tokens for plotting.\"\"\"\n",
    "\n",
    "    clf = classifier or TinyTransformerClassifier()\n",
    "    token_ids = clf.tokenize(text)\n",
    "    heatmap = clf.attention_heatmap(text)\n",
    "    tokens = [\n",
    "        clf.vocab_tokens[idx] if idx < len(clf.vocab_tokens) else \"<extra>\"\n",
    "        for idx in token_ids\n",
    "    ]\n",
    "    return {\"tokens\": tokens, \"attention\": heatmap}\n",
    "\n",
    "\n",
    "def run_demo_classification(\n",
    "    texts: Iterable[str], classifier: TinyTransformerClassifier | None = None\n",
    ") -> List[Dict[str, object]]:\n",
    "    \"\"\"Score a batch of texts with deterministic predictions and attention.\"\"\"\n",
    "\n",
    "    clf = classifier or TinyTransformerClassifier()\n",
    "    outputs: List[Dict[str, object]] = []\n",
    "    for text in texts:\n",
    "        probs = clf.predict_proba(text)\n",
    "        heatmap = clf.attention_heatmap(text)\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"prediction\": clf.classify(text),\n",
    "                \"probs\": probs,\n",
    "                \"attention\": heatmap,\n",
    "            }\n",
    "        )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def _demo() -> None:\n",
    "    classifier = TinyTransformerClassifier()\n",
    "    texts = [\"Great product and amazing support\", \"Terrible and slow service\"]\n",
    "    reports = run_demo_classification(texts, classifier)\n",
    "    for report in reports:\n",
    "        print(f\"Text: {report['text']}\")\n",
    "        print(f\"Prediction: {report['prediction']} – probs: {report['probs']}\")\n",
    "        print(f\"Attention shape: {report['attention'].shape}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
