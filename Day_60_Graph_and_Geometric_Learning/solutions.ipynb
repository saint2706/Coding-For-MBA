{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6815708d",
   "metadata": {},
   "source": [
    "# Day 60 â€“ Graph and Geometric Learning\n",
    "\n",
    "Graph neural networks capture relational structure beyond Euclidean grids. This lesson focuses on:\n",
    "\n",
    "- Building GraphSAGE neighbourhood aggregators and graph attention networks (GAT) from first principles.\n",
    "- Preparing toy graphs and feature matrices compatible with PyTorch Geometric or DGL workflows.\n",
    "- Training node classifiers with message passing, skip connections, and softmax heads on miniature datasets.\n",
    "- Evaluating accuracy, attention weights, and representation quality for stakeholder-ready reporting.\n",
    "\n",
    "Run `python Day_60_Graph_and_Geometric_Learning/solutions.py` to inspect handcrafted GraphSAGE/GAT layers, monitor training metrics on a toy citation-style graph, and export feature embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f67b1",
   "metadata": {},
   "source": [
    "Graph neural network helpers for Day 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96349900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GraphData:\n",
    "    \"\"\"Simple container for toy graph node classification tasks.\"\"\"\n",
    "\n",
    "    features: np.ndarray\n",
    "    adjacency: np.ndarray\n",
    "    labels: np.ndarray\n",
    "\n",
    "\n",
    "def build_toy_graph() -> GraphData:\n",
    "    \"\"\"Create a reproducible toy graph with two communities.\"\"\"\n",
    "\n",
    "    features = np.array(\n",
    "        [\n",
    "            [1.0, 0.2, 0.8],\n",
    "            [0.9, 0.1, 0.7],\n",
    "            [1.1, 0.25, 0.9],\n",
    "            [-0.2, 1.0, 0.1],\n",
    "            [-0.1, 0.9, 0.2],\n",
    "            [-0.3, 1.1, 0.15],\n",
    "        ],\n",
    "        dtype=float,\n",
    "    )\n",
    "    labels = np.array([0, 0, 0, 1, 1, 1], dtype=int)\n",
    "    adjacency = np.array(\n",
    "        [\n",
    "            [1, 1, 1, 0, 0, 0],\n",
    "            [1, 1, 1, 0, 0, 0],\n",
    "            [1, 1, 1, 0, 0, 1],\n",
    "            [0, 0, 0, 1, 1, 1],\n",
    "            [0, 0, 0, 1, 1, 1],\n",
    "            [0, 0, 1, 1, 1, 1],\n",
    "        ],\n",
    "        dtype=float,\n",
    "    )\n",
    "    adjacency = adjacency + np.eye(adjacency.shape[0]) * 0.0  # ensure float copy\n",
    "    return GraphData(features=features, adjacency=adjacency, labels=labels)\n",
    "\n",
    "\n",
    "def _softmax(logits: np.ndarray) -> np.ndarray:\n",
    "    logits = logits - logits.max(axis=1, keepdims=True)\n",
    "    exp = np.exp(logits)\n",
    "    exp /= exp.sum(axis=1, keepdims=True)\n",
    "    return exp\n",
    "\n",
    "\n",
    "class GraphSAGEClassifier:\n",
    "    \"\"\"Mean-aggregator GraphSAGE classifier with manual gradients.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_dim: int = 6, num_classes: int = 2, random_state: int = 60\n",
    "    ) -> None:\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.random_state = random_state\n",
    "        self.W_self: np.ndarray | None = None\n",
    "        self.W_neigh: np.ndarray | None = None\n",
    "        self.b_hidden: np.ndarray | None = None\n",
    "        self.W_out: np.ndarray | None = None\n",
    "        self.b_out: np.ndarray | None = None\n",
    "\n",
    "    def _ensure_params(self, input_dim: int) -> None:\n",
    "        if self.W_self is not None:\n",
    "            return\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        self.W_self = rng.normal(0.0, 0.4, size=(input_dim, self.hidden_dim))\n",
    "        self.W_neigh = rng.normal(0.0, 0.4, size=(input_dim, self.hidden_dim))\n",
    "        self.b_hidden = np.zeros(self.hidden_dim)\n",
    "        self.W_out = rng.normal(0.0, 0.4, size=(self.hidden_dim, self.num_classes))\n",
    "        self.b_out = np.zeros(self.num_classes)\n",
    "\n",
    "    def forward(self, data: GraphData) -> Dict[str, np.ndarray]:\n",
    "        assert self.W_self is not None and self.W_neigh is not None\n",
    "        assert (\n",
    "            self.b_hidden is not None\n",
    "            and self.W_out is not None\n",
    "            and self.b_out is not None\n",
    "        )\n",
    "        features = data.features\n",
    "        adjacency = data.adjacency\n",
    "        degrees = adjacency.sum(axis=1, keepdims=True)\n",
    "        degrees[degrees == 0] = 1.0\n",
    "        neigh_mean = adjacency @ features / degrees\n",
    "        hidden_pre = features @ self.W_self + neigh_mean @ self.W_neigh + self.b_hidden\n",
    "        hidden = np.maximum(0.0, hidden_pre)\n",
    "        logits = hidden @ self.W_out + self.b_out\n",
    "        probs = _softmax(logits)\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"hidden\": hidden,\n",
    "            \"hidden_pre\": hidden_pre,\n",
    "            \"neigh\": neigh_mean,\n",
    "            \"probs\": probs,\n",
    "        }\n",
    "\n",
    "    def train(self, data: GraphData, epochs: int = 200, lr: float = 0.1) -> List[float]:\n",
    "        self._ensure_params(data.features.shape[1])\n",
    "        assert self.W_self is not None and self.W_neigh is not None\n",
    "        assert (\n",
    "            self.b_hidden is not None\n",
    "            and self.W_out is not None\n",
    "            and self.b_out is not None\n",
    "        )\n",
    "        y = data.labels\n",
    "        y_onehot = np.eye(self.num_classes)[y]\n",
    "        losses: List[float] = []\n",
    "        for _ in range(epochs):\n",
    "            forward = self.forward(data)\n",
    "            probs = forward[\"probs\"]\n",
    "            loss = float(-np.sum(y_onehot * np.log(probs + 1e-9)) / y.shape[0])\n",
    "            losses.append(loss)\n",
    "\n",
    "            grad_logits = (probs - y_onehot) / y.shape[0]\n",
    "            grad_W_out = forward[\"hidden\"].T @ grad_logits\n",
    "            grad_b_out = grad_logits.sum(axis=0)\n",
    "            grad_hidden = grad_logits @ self.W_out.T\n",
    "            grad_hidden_pre = grad_hidden * (forward[\"hidden_pre\"] > 0)\n",
    "\n",
    "            grad_W_self = data.features.T @ grad_hidden_pre\n",
    "            grad_W_neigh = forward[\"neigh\"].T @ grad_hidden_pre\n",
    "            grad_b_hidden = grad_hidden_pre.sum(axis=0)\n",
    "\n",
    "            self.W_out -= lr * grad_W_out\n",
    "            self.b_out -= lr * grad_b_out\n",
    "            self.W_self -= lr * grad_W_self\n",
    "            self.W_neigh -= lr * grad_W_neigh\n",
    "            self.b_hidden -= lr * grad_b_hidden\n",
    "        return losses\n",
    "\n",
    "    def predict(self, data: GraphData) -> np.ndarray:\n",
    "        probs = self.forward(data)[\"probs\"]\n",
    "        return probs.argmax(axis=1)\n",
    "\n",
    "    def accuracy(self, data: GraphData) -> float:\n",
    "        preds = self.predict(data)\n",
    "        return float((preds == data.labels).mean())\n",
    "\n",
    "\n",
    "class GraphAttentionClassifier:\n",
    "    \"\"\"Attention-based aggregator with trainable linear head.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, temperature: float = 0.5, num_classes: int = 2, random_state: int = 60\n",
    "    ) -> None:\n",
    "        self.temperature = temperature\n",
    "        self.num_classes = num_classes\n",
    "        self.random_state = random_state\n",
    "        self.W_out: np.ndarray | None = None\n",
    "        self.b_out: np.ndarray | None = None\n",
    "        self._embeddings: np.ndarray | None = None\n",
    "        self._attention: np.ndarray | None = None\n",
    "\n",
    "    def _attention_matrix(\n",
    "        self, features: np.ndarray, adjacency: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        sim = (features @ features.T) / self.temperature\n",
    "        sim -= sim.max(axis=1, keepdims=True)\n",
    "        weights = np.exp(sim)\n",
    "        masked = weights * adjacency\n",
    "        normaliser = masked.sum(axis=1, keepdims=True)\n",
    "        normaliser[normaliser == 0] = 1.0\n",
    "        return masked / normaliser\n",
    "\n",
    "    def encode(self, data: GraphData) -> np.ndarray:\n",
    "        adjacency = data.adjacency.copy()\n",
    "        np.fill_diagonal(adjacency, 1.0)\n",
    "        attn = self._attention_matrix(data.features, adjacency)\n",
    "        self._attention = attn\n",
    "        embeddings = attn @ data.features\n",
    "        self._embeddings = embeddings\n",
    "        return embeddings\n",
    "\n",
    "    def train(self, data: GraphData, epochs: int = 200, lr: float = 0.1) -> List[float]:\n",
    "        embeddings = self.encode(data)\n",
    "        if self.W_out is None or self.b_out is None:\n",
    "            rng = np.random.default_rng(self.random_state)\n",
    "            self.W_out = rng.normal(\n",
    "                0.0, 0.4, size=(embeddings.shape[1], self.num_classes)\n",
    "            )\n",
    "            self.b_out = np.zeros(self.num_classes)\n",
    "        assert self.W_out is not None and self.b_out is not None\n",
    "        y = data.labels\n",
    "        y_onehot = np.eye(self.num_classes)[y]\n",
    "        losses: List[float] = []\n",
    "        for _ in range(epochs):\n",
    "            logits = embeddings @ self.W_out + self.b_out\n",
    "            probs = _softmax(logits)\n",
    "            loss = float(-np.sum(y_onehot * np.log(probs + 1e-9)) / y.shape[0])\n",
    "            losses.append(loss)\n",
    "\n",
    "            grad_logits = (probs - y_onehot) / y.shape[0]\n",
    "            grad_W_out = embeddings.T @ grad_logits\n",
    "            grad_b_out = grad_logits.sum(axis=0)\n",
    "            self.W_out -= lr * grad_W_out\n",
    "            self.b_out -= lr * grad_b_out\n",
    "        return losses\n",
    "\n",
    "    def predict(self, data: GraphData) -> np.ndarray:\n",
    "        embeddings = self.encode(data) if self._embeddings is None else self._embeddings\n",
    "        assert self.W_out is not None and self.b_out is not None\n",
    "        logits = embeddings @ self.W_out + self.b_out\n",
    "        probs = _softmax(logits)\n",
    "        return probs.argmax(axis=1)\n",
    "\n",
    "    def attention_matrix(self, data: GraphData) -> np.ndarray:\n",
    "        if self._attention is None:\n",
    "            self.encode(data)\n",
    "        assert self._attention is not None\n",
    "        return self._attention\n",
    "\n",
    "    def accuracy(self, data: GraphData) -> float:\n",
    "        preds = self.predict(data)\n",
    "        return float((preds == data.labels).mean())\n",
    "\n",
    "\n",
    "def train_node_classifiers(random_state: int = 60) -> Dict[str, object]:\n",
    "    \"\"\"Train both GraphSAGE and graph attention classifiers on the toy graph.\"\"\"\n",
    "\n",
    "    data = build_toy_graph()\n",
    "    sage = GraphSAGEClassifier(random_state=random_state)\n",
    "    gat = GraphAttentionClassifier(random_state=random_state)\n",
    "    sage_losses = sage.train(data, epochs=200, lr=0.1)\n",
    "    gat_losses = gat.train(data, epochs=200, lr=0.1)\n",
    "    results = {\n",
    "        \"graphsage_accuracy\": sage.accuracy(data),\n",
    "        \"gat_accuracy\": gat.accuracy(data),\n",
    "        \"graphsage_losses\": sage_losses,\n",
    "        \"gat_losses\": gat_losses,\n",
    "        \"attention_matrix\": gat.attention_matrix(data),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def _demo() -> None:\n",
    "    results = train_node_classifiers()\n",
    "    print(\n",
    "        f\"GraphSAGE accuracy: {results['graphsage_accuracy']:.3f} | GAT accuracy: {results['gat_accuracy']:.3f}\"\n",
    "    )\n",
    "    print(f\"Attention matrix row sums: {results['attention_matrix'].sum(axis=1)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
